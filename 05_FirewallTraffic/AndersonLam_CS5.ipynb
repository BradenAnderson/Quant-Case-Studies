{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3400bf2c-c799-481b-908c-5192a4845731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from firewall import *\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8dd91-739e-41cd-b74b-f3f237e1f92e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c725afc6-fa63-434b-a855-a3442ae359cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>NAT Source Port</th>\n",
       "      <th>NAT Destination Port</th>\n",
       "      <th>Action</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>Bytes Sent</th>\n",
       "      <th>Bytes Received</th>\n",
       "      <th>Packets</th>\n",
       "      <th>Elapsed Time (sec)</th>\n",
       "      <th>pkts_sent</th>\n",
       "      <th>pkts_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57222</td>\n",
       "      <td>53</td>\n",
       "      <td>54587</td>\n",
       "      <td>53</td>\n",
       "      <td>allow</td>\n",
       "      <td>177</td>\n",
       "      <td>94</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>allow</td>\n",
       "      <td>4768</td>\n",
       "      <td>1600</td>\n",
       "      <td>3168</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6881</td>\n",
       "      <td>50321</td>\n",
       "      <td>43265</td>\n",
       "      <td>50321</td>\n",
       "      <td>allow</td>\n",
       "      <td>238</td>\n",
       "      <td>118</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>1199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>allow</td>\n",
       "      <td>3327</td>\n",
       "      <td>1438</td>\n",
       "      <td>1889</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>443</td>\n",
       "      <td>45848</td>\n",
       "      <td>443</td>\n",
       "      <td>allow</td>\n",
       "      <td>25358</td>\n",
       "      <td>6778</td>\n",
       "      <td>18580</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source Port  Destination Port  NAT Source Port  NAT Destination Port  \\\n",
       "0        57222                53            54587                    53   \n",
       "1        56258              3389            56258                  3389   \n",
       "2         6881             50321            43265                 50321   \n",
       "3        50553              3389            50553                  3389   \n",
       "4        50002               443            45848                   443   \n",
       "\n",
       "  Action  Bytes  Bytes Sent  Bytes Received  Packets  Elapsed Time (sec)  \\\n",
       "0  allow    177          94              83        2                  30   \n",
       "1  allow   4768        1600            3168       19                  17   \n",
       "2  allow    238         118             120        2                1199   \n",
       "3  allow   3327        1438            1889       15                  17   \n",
       "4  allow  25358        6778           18580       31                  16   \n",
       "\n",
       "   pkts_sent  pkts_received  \n",
       "0          1              1  \n",
       "1         10              9  \n",
       "2          1              1  \n",
       "3          8              7  \n",
       "4         13             18  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./log2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b076661-cb71-48d8-a251-ca4b975f7b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65532 entries, 0 to 65531\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Source Port           65532 non-null  int64 \n",
      " 1   Destination Port      65532 non-null  int64 \n",
      " 2   NAT Source Port       65532 non-null  int64 \n",
      " 3   NAT Destination Port  65532 non-null  int64 \n",
      " 4   Action                65532 non-null  object\n",
      " 5   Bytes                 65532 non-null  int64 \n",
      " 6   Bytes Sent            65532 non-null  int64 \n",
      " 7   Bytes Received        65532 non-null  int64 \n",
      " 8   Packets               65532 non-null  int64 \n",
      " 9   Elapsed Time (sec)    65532 non-null  int64 \n",
      " 10  pkts_sent             65532 non-null  int64 \n",
      " 11  pkts_received         65532 non-null  int64 \n",
      "dtypes: int64(11), object(1)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da623b6-46d7-472d-a96b-393e8c03d89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source Port             0\n",
       "Destination Port        0\n",
       "NAT Source Port         0\n",
       "NAT Destination Port    0\n",
       "Action                  0\n",
       "Bytes                   0\n",
       "Bytes Sent              0\n",
       "Bytes Received          0\n",
       "Packets                 0\n",
       "Elapsed Time (sec)      0\n",
       "pkts_sent               0\n",
       "pkts_received           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa5a283-2e75-4ec5-b45f-81ca02a8ee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Port --> Number of Unique Values = 22724\n",
      "Destination Port --> Number of Unique Values = 3273\n",
      "NAT Source Port --> Number of Unique Values = 29152\n",
      "NAT Destination Port --> Number of Unique Values = 2533\n"
     ]
    }
   ],
   "source": [
    "port_columns = [c for c in df.columns if \"Port\" in c]\n",
    "for col in port_columns:\n",
    "    print(f\"{col} --> Number of Unique Values = {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e134fa02-caa6-4201-8f33-05b2b119ae4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allow         0.574376\n",
       "deny          0.228697\n",
       "drop          0.196103\n",
       "reset-both    0.000824\n",
       "Name: Action, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Action'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa818d1-c59d-402b-a217-d6aad55cfeb8",
   "metadata": {},
   "source": [
    "# Create cross validation and final test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2aba9f5-776c-4197-8331-7d9d6a812b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = create_train_and_final_test_sets(df=df)\n",
    "train_df = pd.read_csv(\"./datasets/train_20221018_1118.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f87283-3953-4249-b396-aa6dc770e84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58978, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910eef10-352c-4dfa-86b6-86a202d97f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Port --> Number of Unique Values = 21768\n",
      "Destination Port --> Number of Unique Values = 3098\n",
      "NAT Source Port --> Number of Unique Values = 26848\n",
      "NAT Destination Port --> Number of Unique Values = 2393\n"
     ]
    }
   ],
   "source": [
    "port_columns = [c for c in train_df.columns if \"Port\" in c]\n",
    "for col in port_columns:\n",
    "    print(f\"{col} --> Number of Unique Values = {train_df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8531d-1de0-4709-a60a-154b52b5dd87",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5aa3a87-2216-44d2-a9d6-10b69eeaa4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allow         0.574367\n",
       "deny          0.228695\n",
       "drop          0.196107\n",
       "reset-both    0.000831\n",
       "Name: Action, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Action'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c584bfc-a8b2-4c3c-ae9b-8302e8781063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>columns_encoded</th>\n",
       "      <th>test_f1_weighted_mean</th>\n",
       "      <th>test_f1_weighted_min</th>\n",
       "      <th>test_f1_weighted_max</th>\n",
       "      <th>test_f1_weighted_std</th>\n",
       "      <th>train_f1_weighted_mean</th>\n",
       "      <th>train_f1_weighted_min</th>\n",
       "      <th>train_f1_weighted_max</th>\n",
       "      <th>train_f1_weighted_std</th>\n",
       "      <th>test_f1_micro_mean</th>\n",
       "      <th>test_f1_micro_min</th>\n",
       "      <th>test_f1_micro_max</th>\n",
       "      <th>test_f1_micro_std</th>\n",
       "      <th>train_f1_micro_mean</th>\n",
       "      <th>train_f1_micro_min</th>\n",
       "      <th>train_f1_micro_max</th>\n",
       "      <th>train_f1_micro_std</th>\n",
       "      <th>test_f1_macro_mean</th>\n",
       "      <th>test_f1_macro_min</th>\n",
       "      <th>test_f1_macro_max</th>\n",
       "      <th>test_f1_macro_std</th>\n",
       "      <th>train_f1_macro_mean</th>\n",
       "      <th>train_f1_macro_min</th>\n",
       "      <th>train_f1_macro_max</th>\n",
       "      <th>train_f1_macro_std</th>\n",
       "      <th>test_recall_weighted_mean</th>\n",
       "      <th>test_recall_weighted_min</th>\n",
       "      <th>test_recall_weighted_max</th>\n",
       "      <th>test_recall_weighted_std</th>\n",
       "      <th>train_recall_weighted_mean</th>\n",
       "      <th>train_recall_weighted_min</th>\n",
       "      <th>train_recall_weighted_max</th>\n",
       "      <th>train_recall_weighted_std</th>\n",
       "      <th>test_recall_micro_mean</th>\n",
       "      <th>test_recall_micro_min</th>\n",
       "      <th>test_recall_micro_max</th>\n",
       "      <th>test_recall_micro_std</th>\n",
       "      <th>train_recall_micro_mean</th>\n",
       "      <th>train_recall_micro_min</th>\n",
       "      <th>train_recall_micro_max</th>\n",
       "      <th>train_recall_micro_std</th>\n",
       "      <th>test_recall_macro_mean</th>\n",
       "      <th>test_recall_macro_min</th>\n",
       "      <th>test_recall_macro_max</th>\n",
       "      <th>test_recall_macro_std</th>\n",
       "      <th>train_recall_macro_mean</th>\n",
       "      <th>train_recall_macro_min</th>\n",
       "      <th>train_recall_macro_max</th>\n",
       "      <th>train_recall_macro_std</th>\n",
       "      <th>test_precision_weighted_mean</th>\n",
       "      <th>test_precision_weighted_min</th>\n",
       "      <th>test_precision_weighted_max</th>\n",
       "      <th>test_precision_weighted_std</th>\n",
       "      <th>train_precision_weighted_mean</th>\n",
       "      <th>train_precision_weighted_min</th>\n",
       "      <th>train_precision_weighted_max</th>\n",
       "      <th>train_precision_weighted_std</th>\n",
       "      <th>test_precision_micro_mean</th>\n",
       "      <th>test_precision_micro_min</th>\n",
       "      <th>test_precision_micro_max</th>\n",
       "      <th>test_precision_micro_std</th>\n",
       "      <th>train_precision_micro_mean</th>\n",
       "      <th>train_precision_micro_min</th>\n",
       "      <th>train_precision_micro_max</th>\n",
       "      <th>train_precision_micro_std</th>\n",
       "      <th>test_precision_macro_mean</th>\n",
       "      <th>test_precision_macro_min</th>\n",
       "      <th>test_precision_macro_max</th>\n",
       "      <th>test_precision_macro_std</th>\n",
       "      <th>train_precision_macro_mean</th>\n",
       "      <th>train_precision_macro_min</th>\n",
       "      <th>train_precision_macro_max</th>\n",
       "      <th>train_precision_macro_std</th>\n",
       "      <th>test_accuracy_mean</th>\n",
       "      <th>test_accuracy_min</th>\n",
       "      <th>test_accuracy_max</th>\n",
       "      <th>test_accuracy_std</th>\n",
       "      <th>train_accuracy_mean</th>\n",
       "      <th>train_accuracy_min</th>\n",
       "      <th>train_accuracy_max</th>\n",
       "      <th>train_accuracy_std</th>\n",
       "      <th>test_balanced_accuracy_mean</th>\n",
       "      <th>test_balanced_accuracy_min</th>\n",
       "      <th>test_balanced_accuracy_max</th>\n",
       "      <th>test_balanced_accuracy_std</th>\n",
       "      <th>train_balanced_accuracy_mean</th>\n",
       "      <th>train_balanced_accuracy_min</th>\n",
       "      <th>train_balanced_accuracy_max</th>\n",
       "      <th>train_balanced_accuracy_std</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_min</th>\n",
       "      <th>fit_time_max</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_min</th>\n",
       "      <th>score_time_max</th>\n",
       "      <th>score_time_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC(kernel='linear', random_state=7742)</td>\n",
       "      <td>Source Port, Destination Port, NAT Source Port</td>\n",
       "      <td>0.996877</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.998706</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996948</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.917364</td>\n",
       "      <td>0.890109</td>\n",
       "      <td>0.969532</td>\n",
       "      <td>0.030743</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>0.998775</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.996948</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996948</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.879764</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947903</td>\n",
       "      <td>0.038566</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.996956</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.996948</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.997483</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.998292</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.998928</td>\n",
       "      <td>0.998424</td>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.996948</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.879764</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947903</td>\n",
       "      <td>0.038566</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>29.592915</td>\n",
       "      <td>26.055254</td>\n",
       "      <td>33.352509</td>\n",
       "      <td>2.718526</td>\n",
       "      <td>2.758888</td>\n",
       "      <td>2.690408</td>\n",
       "      <td>2.827435</td>\n",
       "      <td>0.046220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC(kernel='linear', random_state=7742)</td>\n",
       "      <td>Source Port, Destination Port, NAT Source Port...</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.998706</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.917352</td>\n",
       "      <td>0.890109</td>\n",
       "      <td>0.969532</td>\n",
       "      <td>0.030736</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>0.998775</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.879757</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947903</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.996938</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.997464</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.998292</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.998928</td>\n",
       "      <td>0.998424</td>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.879757</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947903</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>34.868756</td>\n",
       "      <td>31.684043</td>\n",
       "      <td>38.076043</td>\n",
       "      <td>2.468839</td>\n",
       "      <td>3.062900</td>\n",
       "      <td>2.895521</td>\n",
       "      <td>3.187483</td>\n",
       "      <td>0.105158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC(kernel='linear', random_state=7742)</td>\n",
       "      <td>Source Port, Destination Port, NAT Destination...</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998686</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.917352</td>\n",
       "      <td>0.890109</td>\n",
       "      <td>0.969532</td>\n",
       "      <td>0.030736</td>\n",
       "      <td>0.997565</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.998923</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998686</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998686</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.879757</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947903</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>0.996250</td>\n",
       "      <td>0.992217</td>\n",
       "      <td>0.999247</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.996938</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.998592</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.998688</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998686</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.997464</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.998292</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>0.999287</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998686</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.879757</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947903</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>0.996250</td>\n",
       "      <td>0.992217</td>\n",
       "      <td>0.999247</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>28.130873</td>\n",
       "      <td>25.686436</td>\n",
       "      <td>29.971153</td>\n",
       "      <td>1.703711</td>\n",
       "      <td>2.740681</td>\n",
       "      <td>2.692439</td>\n",
       "      <td>2.818419</td>\n",
       "      <td>0.047076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC(random_state=7742)</td>\n",
       "      <td>Source Port, Destination Port, NAT Destination...</td>\n",
       "      <td>0.996820</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.998706</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.913163</td>\n",
       "      <td>0.890109</td>\n",
       "      <td>0.969596</td>\n",
       "      <td>0.029517</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>0.998775</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.874749</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947940</td>\n",
       "      <td>0.037650</td>\n",
       "      <td>0.998786</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999257</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.996904</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.998605</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.997426</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.998292</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.998922</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.874749</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947940</td>\n",
       "      <td>0.037650</td>\n",
       "      <td>0.998786</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999257</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>2.029780</td>\n",
       "      <td>0.713588</td>\n",
       "      <td>2.849905</td>\n",
       "      <td>0.787562</td>\n",
       "      <td>0.431883</td>\n",
       "      <td>0.424043</td>\n",
       "      <td>0.447101</td>\n",
       "      <td>0.008292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearSVC(max_iter=20000, random_state=7742)</td>\n",
       "      <td>Source Port, Destination Port, NAT Source Port...</td>\n",
       "      <td>0.996820</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.998706</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.913163</td>\n",
       "      <td>0.890109</td>\n",
       "      <td>0.969596</td>\n",
       "      <td>0.029517</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>0.998775</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.874749</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947940</td>\n",
       "      <td>0.037650</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.996904</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.997426</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.998292</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.998928</td>\n",
       "      <td>0.998424</td>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.996897</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.874749</td>\n",
       "      <td>0.847652</td>\n",
       "      <td>0.947940</td>\n",
       "      <td>0.037650</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998374</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>3.033141</td>\n",
       "      <td>1.782557</td>\n",
       "      <td>4.139890</td>\n",
       "      <td>1.017081</td>\n",
       "      <td>0.420869</td>\n",
       "      <td>0.410060</td>\n",
       "      <td>0.432104</td>\n",
       "      <td>0.008354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  \\\n",
       "0       SVC(kernel='linear', random_state=7742)   \n",
       "1       SVC(kernel='linear', random_state=7742)   \n",
       "2       SVC(kernel='linear', random_state=7742)   \n",
       "3                  LinearSVC(random_state=7742)   \n",
       "4  LinearSVC(max_iter=20000, random_state=7742)   \n",
       "\n",
       "                                     columns_encoded  test_f1_weighted_mean  \\\n",
       "0     Source Port, Destination Port, NAT Source Port               0.996877   \n",
       "1  Source Port, Destination Port, NAT Source Port...               0.996860   \n",
       "2  Source Port, Destination Port, NAT Destination...               0.996860   \n",
       "3  Source Port, Destination Port, NAT Destination...               0.996820   \n",
       "4  Source Port, Destination Port, NAT Source Port...               0.996820   \n",
       "\n",
       "   test_f1_weighted_min  test_f1_weighted_max  test_f1_weighted_std  \\\n",
       "0              0.996503              0.997261              0.000252   \n",
       "1              0.996503              0.997261              0.000264   \n",
       "2              0.996503              0.997261              0.000264   \n",
       "3              0.996503              0.997261              0.000281   \n",
       "4              0.996503              0.997261              0.000281   \n",
       "\n",
       "   train_f1_weighted_mean  train_f1_weighted_min  train_f1_weighted_max  \\\n",
       "0                0.998613               0.998515               0.998706   \n",
       "1                0.998613               0.998515               0.998706   \n",
       "2                0.998588               0.998515               0.998685   \n",
       "3                0.998601               0.998515               0.998706   \n",
       "4                0.998613               0.998515               0.998706   \n",
       "\n",
       "   train_f1_weighted_std  test_f1_micro_mean  test_f1_micro_min  \\\n",
       "0               0.000064            0.996948           0.996609   \n",
       "1               0.000064            0.996931           0.996609   \n",
       "2               0.000056            0.996931           0.996609   \n",
       "3               0.000064            0.996897           0.996609   \n",
       "4               0.000064            0.996897           0.996609   \n",
       "\n",
       "   test_f1_micro_max  test_f1_micro_std  train_f1_micro_mean  \\\n",
       "0           0.997372           0.000257             0.998614   \n",
       "1           0.997372           0.000270             0.998614   \n",
       "2           0.997372           0.000270             0.998588   \n",
       "3           0.997372           0.000277             0.998601   \n",
       "4           0.997372           0.000277             0.998614   \n",
       "\n",
       "   train_f1_micro_min  train_f1_micro_max  train_f1_micro_std  \\\n",
       "0            0.998516            0.998707            0.000064   \n",
       "1            0.998516            0.998707            0.000064   \n",
       "2            0.998516            0.998686            0.000056   \n",
       "3            0.998516            0.998707            0.000064   \n",
       "4            0.998516            0.998707            0.000064   \n",
       "\n",
       "   test_f1_macro_mean  test_f1_macro_min  test_f1_macro_max  \\\n",
       "0            0.917364           0.890109           0.969532   \n",
       "1            0.917352           0.890109           0.969532   \n",
       "2            0.917352           0.890109           0.969532   \n",
       "3            0.913163           0.890109           0.969596   \n",
       "4            0.913163           0.890109           0.969596   \n",
       "\n",
       "   test_f1_macro_std  train_f1_macro_mean  train_f1_macro_min  \\\n",
       "0           0.030743             0.998862            0.998775   \n",
       "1           0.030736             0.998862            0.998775   \n",
       "2           0.030736             0.997565            0.995617   \n",
       "3           0.029517             0.998852            0.998775   \n",
       "4           0.029517             0.998862            0.998775   \n",
       "\n",
       "   train_f1_macro_max  train_f1_macro_std  test_recall_weighted_mean  \\\n",
       "0            0.998939            0.000053                   0.996948   \n",
       "1            0.998939            0.000053                   0.996931   \n",
       "2            0.998923            0.001565                   0.996931   \n",
       "3            0.998939            0.000053                   0.996897   \n",
       "4            0.998939            0.000053                   0.996897   \n",
       "\n",
       "   test_recall_weighted_min  test_recall_weighted_max  \\\n",
       "0                  0.996609                  0.997372   \n",
       "1                  0.996609                  0.997372   \n",
       "2                  0.996609                  0.997372   \n",
       "3                  0.996609                  0.997372   \n",
       "4                  0.996609                  0.997372   \n",
       "\n",
       "   test_recall_weighted_std  train_recall_weighted_mean  \\\n",
       "0                  0.000257                    0.998614   \n",
       "1                  0.000270                    0.998614   \n",
       "2                  0.000270                    0.998588   \n",
       "3                  0.000277                    0.998601   \n",
       "4                  0.000277                    0.998614   \n",
       "\n",
       "   train_recall_weighted_min  train_recall_weighted_max  \\\n",
       "0                   0.998516                   0.998707   \n",
       "1                   0.998516                   0.998707   \n",
       "2                   0.998516                   0.998686   \n",
       "3                   0.998516                   0.998707   \n",
       "4                   0.998516                   0.998707   \n",
       "\n",
       "   train_recall_weighted_std  test_recall_micro_mean  test_recall_micro_min  \\\n",
       "0                   0.000064                0.996948               0.996609   \n",
       "1                   0.000064                0.996931               0.996609   \n",
       "2                   0.000056                0.996931               0.996609   \n",
       "3                   0.000064                0.996897               0.996609   \n",
       "4                   0.000064                0.996897               0.996609   \n",
       "\n",
       "   test_recall_micro_max  test_recall_micro_std  train_recall_micro_mean  \\\n",
       "0               0.997372               0.000257                 0.998614   \n",
       "1               0.997372               0.000270                 0.998614   \n",
       "2               0.997372               0.000270                 0.998588   \n",
       "3               0.997372               0.000277                 0.998601   \n",
       "4               0.997372               0.000277                 0.998614   \n",
       "\n",
       "   train_recall_micro_min  train_recall_micro_max  train_recall_micro_std  \\\n",
       "0                0.998516                0.998707                0.000064   \n",
       "1                0.998516                0.998707                0.000064   \n",
       "2                0.998516                0.998686                0.000056   \n",
       "3                0.998516                0.998707                0.000064   \n",
       "4                0.998516                0.998707                0.000064   \n",
       "\n",
       "   test_recall_macro_mean  test_recall_macro_min  test_recall_macro_max  \\\n",
       "0                0.879764               0.847652               0.947903   \n",
       "1                0.879757               0.847652               0.947903   \n",
       "2                0.879757               0.847652               0.947903   \n",
       "3                0.874749               0.847652               0.947940   \n",
       "4                0.874749               0.847652               0.947940   \n",
       "\n",
       "   test_recall_macro_std  train_recall_macro_mean  train_recall_macro_min  \\\n",
       "0               0.038566                 0.998800                0.998374   \n",
       "1               0.038563                 0.998800                0.998374   \n",
       "2               0.038563                 0.996250                0.992217   \n",
       "3               0.037650                 0.998786                0.998374   \n",
       "4               0.037650                 0.998800                0.998374   \n",
       "\n",
       "   train_recall_macro_max  train_recall_macro_std  \\\n",
       "0                0.999280                0.000394   \n",
       "1                0.999280                0.000394   \n",
       "2                0.999247                0.003056   \n",
       "3                0.999257                0.000386   \n",
       "4                0.999280                0.000394   \n",
       "\n",
       "   test_precision_weighted_mean  test_precision_weighted_min  \\\n",
       "0                      0.996956                     0.996621   \n",
       "1                      0.996938                     0.996621   \n",
       "2                      0.996938                     0.996621   \n",
       "3                      0.996904                     0.996621   \n",
       "4                      0.996904                     0.996621   \n",
       "\n",
       "   test_precision_weighted_max  test_precision_weighted_std  \\\n",
       "0                     0.997375                     0.000255   \n",
       "1                     0.997375                     0.000269   \n",
       "2                     0.997375                     0.000269   \n",
       "3                     0.997375                     0.000276   \n",
       "4                     0.997375                     0.000276   \n",
       "\n",
       "   train_precision_weighted_mean  train_precision_weighted_min  \\\n",
       "0                       0.998618                      0.998519   \n",
       "1                       0.998618                      0.998519   \n",
       "2                       0.998592                      0.998519   \n",
       "3                       0.998605                      0.998519   \n",
       "4                       0.998618                      0.998519   \n",
       "\n",
       "   train_precision_weighted_max  train_precision_weighted_std  \\\n",
       "0                      0.998709                      0.000063   \n",
       "1                      0.998709                      0.000063   \n",
       "2                      0.998688                      0.000056   \n",
       "3                      0.998709                      0.000064   \n",
       "4                      0.998709                      0.000063   \n",
       "\n",
       "   test_precision_micro_mean  test_precision_micro_min  \\\n",
       "0                   0.996948                  0.996609   \n",
       "1                   0.996931                  0.996609   \n",
       "2                   0.996931                  0.996609   \n",
       "3                   0.996897                  0.996609   \n",
       "4                   0.996897                  0.996609   \n",
       "\n",
       "   test_precision_micro_max  test_precision_micro_std  \\\n",
       "0                  0.997372                  0.000257   \n",
       "1                  0.997372                  0.000270   \n",
       "2                  0.997372                  0.000270   \n",
       "3                  0.997372                  0.000277   \n",
       "4                  0.997372                  0.000277   \n",
       "\n",
       "   train_precision_micro_mean  train_precision_micro_min  \\\n",
       "0                    0.998614                   0.998516   \n",
       "1                    0.998614                   0.998516   \n",
       "2                    0.998588                   0.998516   \n",
       "3                    0.998601                   0.998516   \n",
       "4                    0.998614                   0.998516   \n",
       "\n",
       "   train_precision_micro_max  train_precision_micro_std  \\\n",
       "0                   0.998707                   0.000064   \n",
       "1                   0.998707                   0.000064   \n",
       "2                   0.998686                   0.000056   \n",
       "3                   0.998707                   0.000064   \n",
       "4                   0.998707                   0.000064   \n",
       "\n",
       "   test_precision_macro_mean  test_precision_macro_min  \\\n",
       "0                   0.997483                  0.996503   \n",
       "1                   0.997464                  0.996503   \n",
       "2                   0.997464                  0.996503   \n",
       "3                   0.997426                  0.996503   \n",
       "4                   0.997426                  0.996503   \n",
       "\n",
       "   test_precision_macro_max  test_precision_macro_std  \\\n",
       "0                  0.998292                  0.000733   \n",
       "1                  0.998292                  0.000726   \n",
       "2                  0.998292                  0.000726   \n",
       "3                  0.998292                  0.000665   \n",
       "4                  0.998292                  0.000665   \n",
       "\n",
       "   train_precision_macro_mean  train_precision_macro_min  \\\n",
       "0                    0.998928                   0.998424   \n",
       "1                    0.998928                   0.998424   \n",
       "2                    0.998917                   0.998414   \n",
       "3                    0.998922                   0.998414   \n",
       "4                    0.998928                   0.998424   \n",
       "\n",
       "   train_precision_macro_max  train_precision_macro_std  test_accuracy_mean  \\\n",
       "0                   0.999296                   0.000386            0.996948   \n",
       "1                   0.999296                   0.000386            0.996931   \n",
       "2                   0.999287                   0.000388            0.996931   \n",
       "3                   0.999296                   0.000389            0.996897   \n",
       "4                   0.999296                   0.000386            0.996897   \n",
       "\n",
       "   test_accuracy_min  test_accuracy_max  test_accuracy_std  \\\n",
       "0           0.996609           0.997372           0.000257   \n",
       "1           0.996609           0.997372           0.000270   \n",
       "2           0.996609           0.997372           0.000270   \n",
       "3           0.996609           0.997372           0.000277   \n",
       "4           0.996609           0.997372           0.000277   \n",
       "\n",
       "   train_accuracy_mean  train_accuracy_min  train_accuracy_max  \\\n",
       "0             0.998614            0.998516            0.998707   \n",
       "1             0.998614            0.998516            0.998707   \n",
       "2             0.998588            0.998516            0.998686   \n",
       "3             0.998601            0.998516            0.998707   \n",
       "4             0.998614            0.998516            0.998707   \n",
       "\n",
       "   train_accuracy_std  test_balanced_accuracy_mean  \\\n",
       "0            0.000064                     0.879764   \n",
       "1            0.000064                     0.879757   \n",
       "2            0.000056                     0.879757   \n",
       "3            0.000064                     0.874749   \n",
       "4            0.000064                     0.874749   \n",
       "\n",
       "   test_balanced_accuracy_min  test_balanced_accuracy_max  \\\n",
       "0                    0.847652                    0.947903   \n",
       "1                    0.847652                    0.947903   \n",
       "2                    0.847652                    0.947903   \n",
       "3                    0.847652                    0.947940   \n",
       "4                    0.847652                    0.947940   \n",
       "\n",
       "   test_balanced_accuracy_std  train_balanced_accuracy_mean  \\\n",
       "0                    0.038566                      0.998800   \n",
       "1                    0.038563                      0.998800   \n",
       "2                    0.038563                      0.996250   \n",
       "3                    0.037650                      0.998786   \n",
       "4                    0.037650                      0.998800   \n",
       "\n",
       "   train_balanced_accuracy_min  train_balanced_accuracy_max  \\\n",
       "0                     0.998374                     0.999280   \n",
       "1                     0.998374                     0.999280   \n",
       "2                     0.992217                     0.999247   \n",
       "3                     0.998374                     0.999257   \n",
       "4                     0.998374                     0.999280   \n",
       "\n",
       "   train_balanced_accuracy_std  fit_time_mean  fit_time_min  fit_time_max  \\\n",
       "0                     0.000394      29.592915     26.055254     33.352509   \n",
       "1                     0.000394      34.868756     31.684043     38.076043   \n",
       "2                     0.003056      28.130873     25.686436     29.971153   \n",
       "3                     0.000386       2.029780      0.713588      2.849905   \n",
       "4                     0.000394       3.033141      1.782557      4.139890   \n",
       "\n",
       "   fit_time_std  score_time_mean  score_time_min  score_time_max  \\\n",
       "0      2.718526         2.758888        2.690408        2.827435   \n",
       "1      2.468839         3.062900        2.895521        3.187483   \n",
       "2      1.703711         2.740681        2.692439        2.818419   \n",
       "3      0.787562         0.431883        0.424043        0.447101   \n",
       "4      1.017081         0.420869        0.410060        0.432104   \n",
       "\n",
       "   score_time_std  \n",
       "0        0.046220  \n",
       "1        0.105158  \n",
       "2        0.047076  \n",
       "3        0.008292  \n",
       "4        0.008354  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### STANDARD SCALER! \n",
    "\n",
    "all_estimators = [SVC(kernel='rbf', \n",
    "                      random_state=7742), \n",
    "                  SVC(kernel='poly', \n",
    "                      random_state=7742),\n",
    "                  SVC(kernel='sigmoid', \n",
    "                      random_state=7742),\n",
    "                  SVC(kernel='linear', \n",
    "                      random_state=7742),\n",
    "                  LinearSVC(random_state=7742),\n",
    "                  LinearSVC(max_iter=20_000, \n",
    "                            random_state=7742),\n",
    "                  SGDClassifier(random_state=7742),\n",
    "                  SGDClassifier(max_iter=20_000, \n",
    "                                random_state=7742)]\n",
    "\n",
    "\n",
    "metrics=['f1_weighted', 'f1_micro', 'f1_macro',\n",
    "         'recall_weighted', 'recall_micro', 'recall_macro',\n",
    "         'precision_weighted', 'precision_micro', 'precision_macro',\n",
    "         'accuracy', 'balanced_accuracy']\n",
    "\n",
    "# base_df = get_all_baseline_model_performance(df=train_df, \n",
    "#                                              target_column=\"Action\", \n",
    "#                                              estimators=all_estimators, \n",
    "#                                              metrics=metrics,\n",
    "#                                              n_cv_splits=5, \n",
    "#                                              random_state=7742, \n",
    "#                                              shuffle=True, \n",
    "#                                              n_jobs=50, \n",
    "#                                              return_estimator=False, \n",
    "#                                              sort_metric=\"test_f1_weighted\", \n",
    "#                                              smaller_is_better=False, \n",
    "#                                              candidate_ohe_columns=['Source Port', 'Destination Port', \n",
    "#                                                                     'NAT Source Port', 'NAT Destination Port'])\n",
    "\n",
    "base_df = pd.read_csv(\"./models/baseline_models_ohe_10192022.csv\")\n",
    "# base_df.to_csv(\"./models/baseline_models_ohe_10192022.csv\",index=False)\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ecf6d-adc4-4691-bacb-6ec64ee6014f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Baseline Model Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b948298-15b4-4ad5-9159-68e452f31191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e225e6-e5a6-427e-993b-a787c13122c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a48cd-9e0f-4e3f-8c05-52a2ac3f79d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee5f34ec-1031-44ea-ab5a-46307341c4f9",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6af9751-91a3-4e54-999a-a2246bd1966b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "      <th>mean_train_f1_weighted</th>\n",
       "      <th>mean_test_f1_micro</th>\n",
       "      <th>rank_test_f1_micro</th>\n",
       "      <th>mean_train_f1_micro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>rank_test_f1_macro</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <th>mean_train_recall_weighted</th>\n",
       "      <th>mean_test_recall_micro</th>\n",
       "      <th>rank_test_recall_micro</th>\n",
       "      <th>mean_train_recall_micro</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>rank_test_recall_macro</th>\n",
       "      <th>mean_train_recall_macro</th>\n",
       "      <th>mean_test_precision_weighted</th>\n",
       "      <th>rank_test_precision_weighted</th>\n",
       "      <th>mean_train_precision_weighted</th>\n",
       "      <th>mean_test_precision_micro</th>\n",
       "      <th>rank_test_precision_micro</th>\n",
       "      <th>mean_train_precision_micro</th>\n",
       "      <th>mean_test_precision_macro</th>\n",
       "      <th>rank_test_precision_macro</th>\n",
       "      <th>mean_train_precision_macro</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_train_balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.901337</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.9013370389517434, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998323</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.880769</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8807692733975462, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998323</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.860671</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8606708472376163, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998323</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.841031</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8410310505352605, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998103</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.821839</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8218394177456803, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998103</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            C class_weight                                             params  \\\n",
       "390  0.901337     balanced  {'model__C': 0.9013370389517434, 'model__class...   \n",
       "388  0.880769     balanced  {'model__C': 0.8807692733975462, 'model__class...   \n",
       "386  0.860671     balanced  {'model__C': 0.8606708472376163, 'model__class...   \n",
       "384  0.841031     balanced  {'model__C': 0.8410310505352605, 'model__class...   \n",
       "382  0.821839     balanced  {'model__C': 0.8218394177456803, 'model__class...   \n",
       "\n",
       "     mean_test_f1_weighted  rank_test_f1_weighted  mean_train_f1_weighted  \\\n",
       "390               0.997343                      1                0.998102   \n",
       "388               0.997343                      1                0.998102   \n",
       "386               0.997343                      1                0.998102   \n",
       "384               0.997343                      1                0.998098   \n",
       "382               0.997343                      1                0.998098   \n",
       "\n",
       "     mean_test_f1_micro  rank_test_f1_micro  mean_train_f1_micro  \\\n",
       "390            0.997372                   1             0.998101   \n",
       "388            0.997372                   1             0.998101   \n",
       "386            0.997372                   1             0.998101   \n",
       "384            0.997372                   1             0.998097   \n",
       "382            0.997372                   1             0.998097   \n",
       "\n",
       "     mean_test_f1_macro  rank_test_f1_macro  mean_train_f1_macro  \\\n",
       "390            0.944434                   1             0.998323   \n",
       "388            0.944434                   1             0.998323   \n",
       "386            0.944434                   1             0.998323   \n",
       "384            0.944434                   1             0.998320   \n",
       "382            0.944434                   1             0.998320   \n",
       "\n",
       "     mean_test_recall_weighted  rank_test_recall_weighted  \\\n",
       "390                   0.997372                          1   \n",
       "388                   0.997372                          1   \n",
       "386                   0.997372                          1   \n",
       "384                   0.997372                          1   \n",
       "382                   0.997372                          1   \n",
       "\n",
       "     mean_train_recall_weighted  mean_test_recall_micro  \\\n",
       "390                    0.998101                0.997372   \n",
       "388                    0.998101                0.997372   \n",
       "386                    0.998101                0.997372   \n",
       "384                    0.998097                0.997372   \n",
       "382                    0.998097                0.997372   \n",
       "\n",
       "     rank_test_recall_micro  mean_train_recall_micro  mean_test_recall_macro  \\\n",
       "390                       1                 0.998101                 0.91047   \n",
       "388                       1                 0.998101                 0.91047   \n",
       "386                       1                 0.998101                 0.91047   \n",
       "384                       1                 0.998097                 0.91047   \n",
       "382                       1                 0.998097                 0.91047   \n",
       "\n",
       "     rank_test_recall_macro  mean_train_recall_macro  \\\n",
       "390                      92                 0.998814   \n",
       "388                      92                 0.998814   \n",
       "386                      92                 0.998814   \n",
       "384                      92                 0.998812   \n",
       "382                      92                 0.998812   \n",
       "\n",
       "     mean_test_precision_weighted  rank_test_precision_weighted  \\\n",
       "390                      0.997379                             1   \n",
       "388                      0.997379                             1   \n",
       "386                      0.997379                             1   \n",
       "384                      0.997379                             1   \n",
       "382                      0.997379                             1   \n",
       "\n",
       "     mean_train_precision_weighted  mean_test_precision_micro  \\\n",
       "390                       0.998107                   0.997372   \n",
       "388                       0.998107                   0.997372   \n",
       "386                       0.998107                   0.997372   \n",
       "384                       0.998103                   0.997372   \n",
       "382                       0.998103                   0.997372   \n",
       "\n",
       "     rank_test_precision_micro  mean_train_precision_micro  \\\n",
       "390                          1                    0.998101   \n",
       "388                          1                    0.998101   \n",
       "386                          1                    0.998101   \n",
       "384                          1                    0.998097   \n",
       "382                          1                    0.998097   \n",
       "\n",
       "     mean_test_precision_macro  rank_test_precision_macro  \\\n",
       "390                   0.997363                        230   \n",
       "388                   0.997363                        230   \n",
       "386                   0.997363                        230   \n",
       "384                   0.997363                        230   \n",
       "382                   0.997363                        230   \n",
       "\n",
       "     mean_train_precision_macro  mean_test_accuracy  rank_test_accuracy  \\\n",
       "390                    0.997835            0.997372                   1   \n",
       "388                    0.997835            0.997372                   1   \n",
       "386                    0.997835            0.997372                   1   \n",
       "384                    0.997831            0.997372                   1   \n",
       "382                    0.997831            0.997372                   1   \n",
       "\n",
       "     mean_train_accuracy  mean_test_balanced_accuracy  \\\n",
       "390             0.998101                      0.91047   \n",
       "388             0.998101                      0.91047   \n",
       "386             0.998101                      0.91047   \n",
       "384             0.998097                      0.91047   \n",
       "382             0.998097                      0.91047   \n",
       "\n",
       "     rank_test_balanced_accuracy  mean_train_balanced_accuracy  \n",
       "390                           92                      0.998814  \n",
       "388                           92                      0.998814  \n",
       "386                           92                      0.998814  \n",
       "384                           92                      0.998812  \n",
       "382                           92                      0.998812  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"Action\"\n",
    "drop_cols = [\"NAT Destination Port\"]\n",
    "\n",
    "X = train_df.drop(columns=drop_cols+[target])\n",
    "y = train_df[target].to_numpy()\n",
    "\n",
    "ohe_cols = [\"Source Port\", \"Destination Port\", \"NAT Source Port\"]\n",
    "scale_cols = [c for c in X.columns if c not in ohe_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[(\"ohe\", OneHotEncoder(handle_unknown='infrequent_if_exist'), ohe_cols), \n",
    "                                             (\"scale\", StandardScaler(), scale_cols)], \n",
    "                               remainder=\"passthrough\",  \n",
    "                               n_jobs=10)\n",
    "\n",
    "model = Pipeline(steps=[(\"preprocess\", preprocess), \n",
    "                        (\"model\", SVC(kernel='linear', \n",
    "                                      random_state=7742, \n",
    "                                      tol=5e-4, \n",
    "                                      cache_size=10_000, \n",
    "                                      break_ties=True))])\n",
    "\n",
    "\n",
    "parameter_grid = {\"model__C\": np.logspace(-2, 2, 400), \n",
    "                  \"model__class_weight\": [\"balanced\", None]}\n",
    "\n",
    "metrics=['f1_weighted', 'f1_micro', 'f1_macro',\n",
    "         'recall_weighted', 'recall_micro', 'recall_macro',\n",
    "         'precision_weighted', 'precision_micro', 'precision_macro',\n",
    "         'accuracy', 'balanced_accuracy']\n",
    "\n",
    "# gs1 = run_gridsearch(X=X, \n",
    "#                      y=y, \n",
    "#                      folds=5,\n",
    "#                      estimator=model, \n",
    "#                      param_grid=parameter_grid, \n",
    "#                      n_jobs=50,\n",
    "#                      scoring= metrics, \n",
    "#                      random_state=7742,\n",
    "#                      save_name=get_gs_save_name(model_name=f\"SVC_Linear\"))\n",
    "\n",
    "PATH=f\"./models/SVC_Linear_20221019_0951.pkl\"\n",
    "gs1 = load_gs_from_pickle(pickle_filepath=PATH)\n",
    "gs1_df = gs_to_clean_df(gs1.cv_results_, sort_metric=\"mean_test_f1_weighted\")\n",
    "gs1_df.loc[:,[c for c in gs1_df.columns if \"std\" not in c]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87791f56-4ba2-43e1-87fb-84bc521c6339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997342884010411"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf906ef-00ab-4c43-b953-8a6761df9d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 0.8218394177456803, 'model__class_weight': 'balanced'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f8df6df-2b21-4469-9403-1b815915e360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "      <th>mean_train_f1_weighted</th>\n",
       "      <th>mean_test_f1_micro</th>\n",
       "      <th>rank_test_f1_micro</th>\n",
       "      <th>mean_train_f1_micro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>rank_test_f1_macro</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <th>mean_train_recall_weighted</th>\n",
       "      <th>mean_test_recall_micro</th>\n",
       "      <th>rank_test_recall_micro</th>\n",
       "      <th>mean_train_recall_micro</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>rank_test_recall_macro</th>\n",
       "      <th>mean_train_recall_macro</th>\n",
       "      <th>mean_test_precision_weighted</th>\n",
       "      <th>rank_test_precision_weighted</th>\n",
       "      <th>mean_train_precision_weighted</th>\n",
       "      <th>mean_test_precision_micro</th>\n",
       "      <th>rank_test_precision_micro</th>\n",
       "      <th>mean_train_precision_micro</th>\n",
       "      <th>mean_test_precision_macro</th>\n",
       "      <th>rank_test_precision_macro</th>\n",
       "      <th>mean_train_precision_macro</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_train_balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.901337</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.9013370389517434, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998323</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.880769</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8807692733975462, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998323</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.860671</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8606708472376163, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998323</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998101</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.841031</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8410310505352605, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998103</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.821839</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{'model__C': 0.8218394177456803, 'model__class...</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998103</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>230</td>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998097</td>\n",
       "      <td>0.91047</td>\n",
       "      <td>92</td>\n",
       "      <td>0.998812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            C class_weight                                             params  \\\n",
       "390  0.901337     balanced  {'model__C': 0.9013370389517434, 'model__class...   \n",
       "388  0.880769     balanced  {'model__C': 0.8807692733975462, 'model__class...   \n",
       "386  0.860671     balanced  {'model__C': 0.8606708472376163, 'model__class...   \n",
       "384  0.841031     balanced  {'model__C': 0.8410310505352605, 'model__class...   \n",
       "382  0.821839     balanced  {'model__C': 0.8218394177456803, 'model__class...   \n",
       "\n",
       "     mean_test_f1_weighted  rank_test_f1_weighted  mean_train_f1_weighted  \\\n",
       "390               0.997343                      1                0.998102   \n",
       "388               0.997343                      1                0.998102   \n",
       "386               0.997343                      1                0.998102   \n",
       "384               0.997343                      1                0.998098   \n",
       "382               0.997343                      1                0.998098   \n",
       "\n",
       "     mean_test_f1_micro  rank_test_f1_micro  mean_train_f1_micro  \\\n",
       "390            0.997372                   1             0.998101   \n",
       "388            0.997372                   1             0.998101   \n",
       "386            0.997372                   1             0.998101   \n",
       "384            0.997372                   1             0.998097   \n",
       "382            0.997372                   1             0.998097   \n",
       "\n",
       "     mean_test_f1_macro  rank_test_f1_macro  mean_train_f1_macro  \\\n",
       "390            0.944434                   1             0.998323   \n",
       "388            0.944434                   1             0.998323   \n",
       "386            0.944434                   1             0.998323   \n",
       "384            0.944434                   1             0.998320   \n",
       "382            0.944434                   1             0.998320   \n",
       "\n",
       "     mean_test_recall_weighted  rank_test_recall_weighted  \\\n",
       "390                   0.997372                          1   \n",
       "388                   0.997372                          1   \n",
       "386                   0.997372                          1   \n",
       "384                   0.997372                          1   \n",
       "382                   0.997372                          1   \n",
       "\n",
       "     mean_train_recall_weighted  mean_test_recall_micro  \\\n",
       "390                    0.998101                0.997372   \n",
       "388                    0.998101                0.997372   \n",
       "386                    0.998101                0.997372   \n",
       "384                    0.998097                0.997372   \n",
       "382                    0.998097                0.997372   \n",
       "\n",
       "     rank_test_recall_micro  mean_train_recall_micro  mean_test_recall_macro  \\\n",
       "390                       1                 0.998101                 0.91047   \n",
       "388                       1                 0.998101                 0.91047   \n",
       "386                       1                 0.998101                 0.91047   \n",
       "384                       1                 0.998097                 0.91047   \n",
       "382                       1                 0.998097                 0.91047   \n",
       "\n",
       "     rank_test_recall_macro  mean_train_recall_macro  \\\n",
       "390                      92                 0.998814   \n",
       "388                      92                 0.998814   \n",
       "386                      92                 0.998814   \n",
       "384                      92                 0.998812   \n",
       "382                      92                 0.998812   \n",
       "\n",
       "     mean_test_precision_weighted  rank_test_precision_weighted  \\\n",
       "390                      0.997379                             1   \n",
       "388                      0.997379                             1   \n",
       "386                      0.997379                             1   \n",
       "384                      0.997379                             1   \n",
       "382                      0.997379                             1   \n",
       "\n",
       "     mean_train_precision_weighted  mean_test_precision_micro  \\\n",
       "390                       0.998107                   0.997372   \n",
       "388                       0.998107                   0.997372   \n",
       "386                       0.998107                   0.997372   \n",
       "384                       0.998103                   0.997372   \n",
       "382                       0.998103                   0.997372   \n",
       "\n",
       "     rank_test_precision_micro  mean_train_precision_micro  \\\n",
       "390                          1                    0.998101   \n",
       "388                          1                    0.998101   \n",
       "386                          1                    0.998101   \n",
       "384                          1                    0.998097   \n",
       "382                          1                    0.998097   \n",
       "\n",
       "     mean_test_precision_macro  rank_test_precision_macro  \\\n",
       "390                   0.997363                        230   \n",
       "388                   0.997363                        230   \n",
       "386                   0.997363                        230   \n",
       "384                   0.997363                        230   \n",
       "382                   0.997363                        230   \n",
       "\n",
       "     mean_train_precision_macro  mean_test_accuracy  rank_test_accuracy  \\\n",
       "390                    0.997835            0.997372                   1   \n",
       "388                    0.997835            0.997372                   1   \n",
       "386                    0.997835            0.997372                   1   \n",
       "384                    0.997831            0.997372                   1   \n",
       "382                    0.997831            0.997372                   1   \n",
       "\n",
       "     mean_train_accuracy  mean_test_balanced_accuracy  \\\n",
       "390             0.998101                      0.91047   \n",
       "388             0.998101                      0.91047   \n",
       "386             0.998101                      0.91047   \n",
       "384             0.998097                      0.91047   \n",
       "382             0.998097                      0.91047   \n",
       "\n",
       "     rank_test_balanced_accuracy  mean_train_balanced_accuracy  \n",
       "390                           92                      0.998814  \n",
       "388                           92                      0.998814  \n",
       "386                           92                      0.998814  \n",
       "384                           92                      0.998812  \n",
       "382                           92                      0.998812  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1_df = gs_to_clean_df(gs1.cv_results_, sort_metric=\"mean_test_f1_weighted\")\n",
    "gs1_df.loc[:,[c for c in gs1_df.columns if \"std\" not in c]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5339e55f-adc3-4f7e-b0a8-3fbb538dcd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>penalty</th>\n",
       "      <th>eta0</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "      <th>mean_train_f1_weighted</th>\n",
       "      <th>mean_test_f1_micro</th>\n",
       "      <th>rank_test_f1_micro</th>\n",
       "      <th>mean_train_f1_micro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>rank_test_f1_macro</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <th>mean_train_recall_weighted</th>\n",
       "      <th>mean_test_recall_micro</th>\n",
       "      <th>rank_test_recall_micro</th>\n",
       "      <th>mean_train_recall_micro</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>rank_test_recall_macro</th>\n",
       "      <th>mean_train_recall_macro</th>\n",
       "      <th>mean_test_precision_weighted</th>\n",
       "      <th>rank_test_precision_weighted</th>\n",
       "      <th>mean_train_precision_weighted</th>\n",
       "      <th>mean_test_precision_micro</th>\n",
       "      <th>rank_test_precision_micro</th>\n",
       "      <th>mean_train_precision_micro</th>\n",
       "      <th>mean_test_precision_macro</th>\n",
       "      <th>rank_test_precision_macro</th>\n",
       "      <th>mean_train_precision_macro</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_train_balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21368</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00010791110315313644, 'mode...</td>\n",
       "      <td>0.998207</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.936886</td>\n",
       "      <td>38</td>\n",
       "      <td>0.999658</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.900641</td>\n",
       "      <td>18192</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.998255</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.998458</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.900641</td>\n",
       "      <td>18192</td>\n",
       "      <td>0.999672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25688</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00014632908647345993, 'mode...</td>\n",
       "      <td>0.998164</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.936086</td>\n",
       "      <td>47</td>\n",
       "      <td>0.999636</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.902197</td>\n",
       "      <td>17909</td>\n",
       "      <td>0.999616</td>\n",
       "      <td>0.998198</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.990120</td>\n",
       "      <td>652</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.902197</td>\n",
       "      <td>17909</td>\n",
       "      <td>0.999616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21278</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00010791110315313644, 'mode...</td>\n",
       "      <td>0.998158</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.929695</td>\n",
       "      <td>249</td>\n",
       "      <td>0.994340</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.894708</td>\n",
       "      <td>19439</td>\n",
       "      <td>0.989408</td>\n",
       "      <td>0.998193</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.988453</td>\n",
       "      <td>869</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.894708</td>\n",
       "      <td>19439</td>\n",
       "      <td>0.989408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>0.000143</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00014266210152607368, 'mode...</td>\n",
       "      <td>0.998132</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.925010</td>\n",
       "      <td>462</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.886770</td>\n",
       "      <td>21159</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>0.998187</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.998450</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999574</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.886770</td>\n",
       "      <td>21159</td>\n",
       "      <td>0.999542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36868</th>\n",
       "      <td>0.000321</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'model__alpha': 0.00032137643441002787, 'mode...</td>\n",
       "      <td>0.998129</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.933425</td>\n",
       "      <td>109</td>\n",
       "      <td>0.999545</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.899229</td>\n",
       "      <td>18401</td>\n",
       "      <td>0.999502</td>\n",
       "      <td>0.998164</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.990105</td>\n",
       "      <td>655</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.899229</td>\n",
       "      <td>18401</td>\n",
       "      <td>0.999502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alpha class_weight early_stopping learning_rate        loss penalty  \\\n",
       "21368  0.000108         None          False      adaptive  perceptron      l2   \n",
       "25688  0.000146         None          False      adaptive  perceptron      l2   \n",
       "21278  0.000108         None           True      adaptive  perceptron      l2   \n",
       "25328  0.000143         None          False      adaptive  perceptron      l2   \n",
       "36868  0.000321         None          False      adaptive  perceptron      l2   \n",
       "\n",
       "       eta0                                             params  \\\n",
       "21368   0.1  {'model__alpha': 0.00010791110315313644, 'mode...   \n",
       "25688   0.1  {'model__alpha': 0.00014632908647345993, 'mode...   \n",
       "21278   0.1  {'model__alpha': 0.00010791110315313644, 'mode...   \n",
       "25328   0.1  {'model__alpha': 0.00014266210152607368, 'mode...   \n",
       "36868  0.03  {'model__alpha': 0.00032137643441002787, 'mode...   \n",
       "\n",
       "       mean_test_f1_weighted  rank_test_f1_weighted  mean_train_f1_weighted  \\\n",
       "21368               0.998207                      1                0.999648   \n",
       "25688               0.998164                      2                0.999618   \n",
       "21278               0.998158                      3                0.999453   \n",
       "25328               0.998132                      4                0.999517   \n",
       "36868               0.998129                      5                0.999496   \n",
       "\n",
       "       mean_test_f1_micro  rank_test_f1_micro  mean_train_f1_micro  \\\n",
       "21368            0.998254                   1             0.999648   \n",
       "25688            0.998203                   3             0.999619   \n",
       "21278            0.998203                   2             0.999453   \n",
       "25328            0.998186                   5             0.999517   \n",
       "36868            0.998169                   7             0.999496   \n",
       "\n",
       "       mean_test_f1_macro  rank_test_f1_macro  mean_train_f1_macro  \\\n",
       "21368            0.936886                  38             0.999658   \n",
       "25688            0.936086                  47             0.999636   \n",
       "21278            0.929695                 249             0.994340   \n",
       "25328            0.925010                 462             0.999558   \n",
       "36868            0.933425                 109             0.999545   \n",
       "\n",
       "       mean_test_recall_weighted  rank_test_recall_weighted  \\\n",
       "21368                   0.998254                          1   \n",
       "25688                   0.998203                          3   \n",
       "21278                   0.998203                          2   \n",
       "25328                   0.998186                          5   \n",
       "36868                   0.998169                          7   \n",
       "\n",
       "       mean_train_recall_weighted  mean_test_recall_micro  \\\n",
       "21368                    0.999648                0.998254   \n",
       "25688                    0.999619                0.998203   \n",
       "21278                    0.999453                0.998203   \n",
       "25328                    0.999517                0.998186   \n",
       "36868                    0.999496                0.998169   \n",
       "\n",
       "       rank_test_recall_micro  mean_train_recall_micro  \\\n",
       "21368                       1                 0.999648   \n",
       "25688                       3                 0.999619   \n",
       "21278                       2                 0.999453   \n",
       "25328                       5                 0.999517   \n",
       "36868                       7                 0.999496   \n",
       "\n",
       "       mean_test_recall_macro  rank_test_recall_macro  \\\n",
       "21368                0.900641                   18192   \n",
       "25688                0.902197                   17909   \n",
       "21278                0.894708                   19439   \n",
       "25328                0.886770                   21159   \n",
       "36868                0.899229                   18401   \n",
       "\n",
       "       mean_train_recall_macro  mean_test_precision_weighted  \\\n",
       "21368                 0.999672                      0.998255   \n",
       "25688                 0.999616                      0.998198   \n",
       "21278                 0.989408                      0.998193   \n",
       "25328                 0.999542                      0.998187   \n",
       "36868                 0.999502                      0.998164   \n",
       "\n",
       "       rank_test_precision_weighted  mean_train_precision_weighted  \\\n",
       "21368                             1                       0.999648   \n",
       "25688                             3                       0.999619   \n",
       "21278                             4                       0.999453   \n",
       "25328                             5                       0.999517   \n",
       "36868                             7                       0.999496   \n",
       "\n",
       "       mean_test_precision_micro  rank_test_precision_micro  \\\n",
       "21368                   0.998254                          1   \n",
       "25688                   0.998203                          3   \n",
       "21278                   0.998203                          2   \n",
       "25328                   0.998186                          5   \n",
       "36868                   0.998169                          7   \n",
       "\n",
       "       mean_train_precision_micro  mean_test_precision_macro  \\\n",
       "21368                    0.999648                   0.998458   \n",
       "25688                    0.999619                   0.990120   \n",
       "21278                    0.999453                   0.988453   \n",
       "25328                    0.999517                   0.998450   \n",
       "36868                    0.999496                   0.990105   \n",
       "\n",
       "       rank_test_precision_macro  mean_train_precision_macro  \\\n",
       "21368                          1                    0.999645   \n",
       "25688                        652                    0.999655   \n",
       "21278                        869                    0.999526   \n",
       "25328                          4                    0.999574   \n",
       "36868                        655                    0.999588   \n",
       "\n",
       "       mean_test_accuracy  rank_test_accuracy  mean_train_accuracy  \\\n",
       "21368            0.998254                   1             0.999648   \n",
       "25688            0.998203                   3             0.999619   \n",
       "21278            0.998203                   2             0.999453   \n",
       "25328            0.998186                   5             0.999517   \n",
       "36868            0.998169                   7             0.999496   \n",
       "\n",
       "       mean_test_balanced_accuracy  rank_test_balanced_accuracy  \\\n",
       "21368                     0.900641                        18192   \n",
       "25688                     0.902197                        17909   \n",
       "21278                     0.894708                        19439   \n",
       "25328                     0.886770                        21159   \n",
       "36868                     0.899229                        18401   \n",
       "\n",
       "       mean_train_balanced_accuracy  \n",
       "21368                      0.999672  \n",
       "25688                      0.999616  \n",
       "21278                      0.989408  \n",
       "25328                      0.999542  \n",
       "36868                      0.999502  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"Action\"\n",
    "drop_cols = [\"NAT Destination Port\"]\n",
    "\n",
    "X = train_df.drop(columns=drop_cols+[target])\n",
    "y = train_df[target].to_numpy()\n",
    "\n",
    "ohe_cols = [\"Source Port\", \"Destination Port\", \"NAT Source Port\"]\n",
    "scale_cols = [c for c in X.columns if c not in ohe_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[(\"ohe\", OneHotEncoder(handle_unknown='infrequent_if_exist'), \n",
    "                                              ohe_cols), \n",
    "                                             (\"scale\", StandardScaler(), scale_cols)], \n",
    "                               remainder=\"passthrough\",  \n",
    "                               n_jobs=10)\n",
    "\n",
    "model_sgd = Pipeline(steps=[(\"preprocess\", preprocess), \n",
    "                        (\"model\", SGDClassifier(random_state=7742, \n",
    "                                                tol=5e-4, \n",
    "                                                n_jobs=15, \n",
    "                                                max_iter=50_000,\n",
    "                                                shuffle=True))])\n",
    "\n",
    "\n",
    "alphas = np.logspace(start=-4, stop=1.5, num=500)\n",
    "\n",
    "parameter_grid = [{\"model__penalty\": [\"l2\", \"l1\"], \n",
    "                  \"model__loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \n",
    "                                  \"squared_hinge\", \"perceptron\"], \n",
    "                  \"model__alpha\":alphas, \n",
    "                  \"model__learning_rate\":[\"optimal\"], \n",
    "                  \"model__early_stopping\":[True, False], \n",
    "                  \"model__class_weight\":[\"balanced\",None]}, \n",
    "                  {\"model__penalty\": [\"l2\", \"l1\"], \n",
    "                  \"model__loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \n",
    "                                  \"squared_hinge\", \"perceptron\"], \n",
    "                  \"model__alpha\":alphas, \n",
    "                  \"model__learning_rate\":[\"adaptive\"], \n",
    "                   \"model__eta0\":[5e-1, 1e-1, 3e-1, 3e-2, 1e-2, \n",
    "                           3e-3, 1e-3, 3e-4, 1e-4],\n",
    "                  \"model__early_stopping\":[True, False], \n",
    "                  \"model__class_weight\":[\"balanced\",None]}]\n",
    "\n",
    "metrics=['f1_weighted', 'f1_micro', 'f1_macro',\n",
    "         'recall_weighted', 'recall_micro', 'recall_macro',\n",
    "         'precision_weighted', 'precision_micro', 'precision_macro',\n",
    "         'accuracy', 'balanced_accuracy']\n",
    "\n",
    "# gs2 = run_gridsearch(X=X, \n",
    "#                      y=y, \n",
    "#                      folds=5,\n",
    "#                      estimator=model_sgd, \n",
    "#                      param_grid=parameter_grid, \n",
    "#                      n_jobs=50,\n",
    "#                      scoring= metrics, \n",
    "#                      random_state=7742,\n",
    "#                      save_name=get_gs_save_name(model_name=f\"SGD\"))\n",
    "\n",
    "PATH=f\"./models/SGD_20221027_2341.pkl\"\n",
    "gs2 = load_gs_from_pickle(pickle_filepath=PATH)\n",
    "gs2_df = gs_to_clean_df(gs2.cv_results_, sort_metric=\"mean_test_f1_weighted\")\n",
    "gs2_df.loc[:,[c for c in gs2_df.columns if \"std\" not in c]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8fd2e4-08cd-494b-859e-56dadfd78b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 0.00010791110315313644,\n",
       " 'model__class_weight': None,\n",
       " 'model__early_stopping': False,\n",
       " 'model__eta0': 0.1,\n",
       " 'model__learning_rate': 'adaptive',\n",
       " 'model__loss': 'perceptron',\n",
       " 'model__penalty': 'l2'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3c8356a-014f-4d33-aa5f-6fd6dcff6e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982066173672688"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a9e5967-8359-4ce3-976c-8603f5c28f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>param_model__class_weight</th>\n",
       "      <th>param_model__early_stopping</th>\n",
       "      <th>param_model__learning_rate</th>\n",
       "      <th>param_model__loss</th>\n",
       "      <th>param_model__penalty</th>\n",
       "      <th>param_model__eta0</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_f1_weighted</th>\n",
       "      <th>split1_test_f1_weighted</th>\n",
       "      <th>split2_test_f1_weighted</th>\n",
       "      <th>split3_test_f1_weighted</th>\n",
       "      <th>split4_test_f1_weighted</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>std_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "      <th>split0_train_f1_weighted</th>\n",
       "      <th>split1_train_f1_weighted</th>\n",
       "      <th>split2_train_f1_weighted</th>\n",
       "      <th>split3_train_f1_weighted</th>\n",
       "      <th>split4_train_f1_weighted</th>\n",
       "      <th>mean_train_f1_weighted</th>\n",
       "      <th>std_train_f1_weighted</th>\n",
       "      <th>split0_test_f1_micro</th>\n",
       "      <th>split1_test_f1_micro</th>\n",
       "      <th>split2_test_f1_micro</th>\n",
       "      <th>split3_test_f1_micro</th>\n",
       "      <th>split4_test_f1_micro</th>\n",
       "      <th>mean_test_f1_micro</th>\n",
       "      <th>std_test_f1_micro</th>\n",
       "      <th>rank_test_f1_micro</th>\n",
       "      <th>split0_train_f1_micro</th>\n",
       "      <th>split1_train_f1_micro</th>\n",
       "      <th>split2_train_f1_micro</th>\n",
       "      <th>split3_train_f1_micro</th>\n",
       "      <th>split4_train_f1_micro</th>\n",
       "      <th>mean_train_f1_micro</th>\n",
       "      <th>std_train_f1_micro</th>\n",
       "      <th>split0_test_f1_macro</th>\n",
       "      <th>split1_test_f1_macro</th>\n",
       "      <th>split2_test_f1_macro</th>\n",
       "      <th>split3_test_f1_macro</th>\n",
       "      <th>split4_test_f1_macro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>std_test_f1_macro</th>\n",
       "      <th>rank_test_f1_macro</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_precision_micro</th>\n",
       "      <th>split3_train_precision_micro</th>\n",
       "      <th>split4_train_precision_micro</th>\n",
       "      <th>mean_train_precision_micro</th>\n",
       "      <th>std_train_precision_micro</th>\n",
       "      <th>split0_test_precision_macro</th>\n",
       "      <th>split1_test_precision_macro</th>\n",
       "      <th>split2_test_precision_macro</th>\n",
       "      <th>split3_test_precision_macro</th>\n",
       "      <th>split4_test_precision_macro</th>\n",
       "      <th>mean_test_precision_macro</th>\n",
       "      <th>std_test_precision_macro</th>\n",
       "      <th>rank_test_precision_macro</th>\n",
       "      <th>split0_train_precision_macro</th>\n",
       "      <th>split1_train_precision_macro</th>\n",
       "      <th>split2_train_precision_macro</th>\n",
       "      <th>split3_train_precision_macro</th>\n",
       "      <th>split4_train_precision_macro</th>\n",
       "      <th>mean_train_precision_macro</th>\n",
       "      <th>std_train_precision_macro</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>split4_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>split0_train_accuracy</th>\n",
       "      <th>split1_train_accuracy</th>\n",
       "      <th>split2_train_accuracy</th>\n",
       "      <th>split3_train_accuracy</th>\n",
       "      <th>split4_train_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>split0_test_balanced_accuracy</th>\n",
       "      <th>split1_test_balanced_accuracy</th>\n",
       "      <th>split2_test_balanced_accuracy</th>\n",
       "      <th>split3_test_balanced_accuracy</th>\n",
       "      <th>split4_test_balanced_accuracy</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>std_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>split0_train_balanced_accuracy</th>\n",
       "      <th>split1_train_balanced_accuracy</th>\n",
       "      <th>split2_train_balanced_accuracy</th>\n",
       "      <th>split3_train_balanced_accuracy</th>\n",
       "      <th>split4_train_balanced_accuracy</th>\n",
       "      <th>mean_train_balanced_accuracy</th>\n",
       "      <th>std_train_balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21368</th>\n",
       "      <td>0.988638</td>\n",
       "      <td>0.123793</td>\n",
       "      <td>0.648347</td>\n",
       "      <td>0.049381</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00010791110315313644, 'mode...</td>\n",
       "      <td>0.998106</td>\n",
       "      <td>0.997502</td>\n",
       "      <td>0.998275</td>\n",
       "      <td>0.998698</td>\n",
       "      <td>0.998452</td>\n",
       "      <td>0.998207</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.998135</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.998728</td>\n",
       "      <td>0.998474</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.907198</td>\n",
       "      <td>0.940804</td>\n",
       "      <td>0.941322</td>\n",
       "      <td>0.954484</td>\n",
       "      <td>0.936886</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.998074</td>\n",
       "      <td>0.998119</td>\n",
       "      <td>0.998576</td>\n",
       "      <td>0.999093</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.998458</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.999562</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.998135</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.998728</td>\n",
       "      <td>0.998474</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.904807</td>\n",
       "      <td>0.864766</td>\n",
       "      <td>0.904668</td>\n",
       "      <td>0.905188</td>\n",
       "      <td>0.923775</td>\n",
       "      <td>0.900641</td>\n",
       "      <td>0.019372</td>\n",
       "      <td>18192</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.999502</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25688</th>\n",
       "      <td>0.603336</td>\n",
       "      <td>0.097788</td>\n",
       "      <td>0.666953</td>\n",
       "      <td>0.057021</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00014632908647345993, 'mode...</td>\n",
       "      <td>0.998360</td>\n",
       "      <td>0.997646</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.998703</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>0.998164</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.997796</td>\n",
       "      <td>0.998728</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.940816</td>\n",
       "      <td>0.935551</td>\n",
       "      <td>0.940377</td>\n",
       "      <td>0.927604</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>0.936086</td>\n",
       "      <td>0.004754</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.998349</td>\n",
       "      <td>0.998099</td>\n",
       "      <td>0.998297</td>\n",
       "      <td>0.957462</td>\n",
       "      <td>0.998391</td>\n",
       "      <td>0.990120</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>652</td>\n",
       "      <td>0.999632</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.999566</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.997796</td>\n",
       "      <td>0.998728</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.904919</td>\n",
       "      <td>0.898007</td>\n",
       "      <td>0.904094</td>\n",
       "      <td>0.905188</td>\n",
       "      <td>0.898775</td>\n",
       "      <td>0.902197</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>17909</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.999502</td>\n",
       "      <td>0.999502</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.999717</td>\n",
       "      <td>0.999616</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21278</th>\n",
       "      <td>1.280090</td>\n",
       "      <td>0.071650</td>\n",
       "      <td>0.550490</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00010791110315313644, 'mode...</td>\n",
       "      <td>0.998105</td>\n",
       "      <td>0.997646</td>\n",
       "      <td>0.997908</td>\n",
       "      <td>0.998509</td>\n",
       "      <td>0.998621</td>\n",
       "      <td>0.998158</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999534</td>\n",
       "      <td>0.999343</td>\n",
       "      <td>0.999575</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.998135</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.997965</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999534</td>\n",
       "      <td>0.999343</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.940622</td>\n",
       "      <td>0.935587</td>\n",
       "      <td>0.914886</td>\n",
       "      <td>0.902767</td>\n",
       "      <td>0.954613</td>\n",
       "      <td>0.929695</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.998182</td>\n",
       "      <td>0.998099</td>\n",
       "      <td>0.998315</td>\n",
       "      <td>0.949057</td>\n",
       "      <td>0.998612</td>\n",
       "      <td>0.988453</td>\n",
       "      <td>0.019699</td>\n",
       "      <td>869</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>0.999516</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>0.999440</td>\n",
       "      <td>0.999547</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.998135</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.997965</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.998203</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999534</td>\n",
       "      <td>0.999343</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.904699</td>\n",
       "      <td>0.898078</td>\n",
       "      <td>0.873125</td>\n",
       "      <td>0.873787</td>\n",
       "      <td>0.923849</td>\n",
       "      <td>0.894708</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>19439</td>\n",
       "      <td>0.993465</td>\n",
       "      <td>0.991937</td>\n",
       "      <td>0.981409</td>\n",
       "      <td>0.987007</td>\n",
       "      <td>0.993221</td>\n",
       "      <td>0.989408</td>\n",
       "      <td>0.004629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>0.844990</td>\n",
       "      <td>0.213842</td>\n",
       "      <td>0.704759</td>\n",
       "      <td>0.085071</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'model__alpha': 0.00014266210152607368, 'mode...</td>\n",
       "      <td>0.998248</td>\n",
       "      <td>0.997667</td>\n",
       "      <td>0.997784</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>0.998132</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999004</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.997881</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999004</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.915129</td>\n",
       "      <td>0.947997</td>\n",
       "      <td>0.884593</td>\n",
       "      <td>0.941257</td>\n",
       "      <td>0.936075</td>\n",
       "      <td>0.925010</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999004</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.998257</td>\n",
       "      <td>0.998099</td>\n",
       "      <td>0.998334</td>\n",
       "      <td>0.999114</td>\n",
       "      <td>0.998446</td>\n",
       "      <td>0.998450</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.999227</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>0.999574</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.997881</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.999004</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.873669</td>\n",
       "      <td>0.914566</td>\n",
       "      <td>0.841875</td>\n",
       "      <td>0.905037</td>\n",
       "      <td>0.898704</td>\n",
       "      <td>0.886770</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>21159</td>\n",
       "      <td>0.999710</td>\n",
       "      <td>0.999498</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.999058</td>\n",
       "      <td>0.999686</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36868</th>\n",
       "      <td>0.592534</td>\n",
       "      <td>0.078234</td>\n",
       "      <td>0.559726</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'model__alpha': 0.00032137643441002787, 'mode...</td>\n",
       "      <td>0.998275</td>\n",
       "      <td>0.997752</td>\n",
       "      <td>0.997653</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>0.998129</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.997796</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999513</td>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.940717</td>\n",
       "      <td>0.948070</td>\n",
       "      <td>0.914718</td>\n",
       "      <td>0.927538</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>0.933425</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999513</td>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.998257</td>\n",
       "      <td>0.998136</td>\n",
       "      <td>0.998260</td>\n",
       "      <td>0.957484</td>\n",
       "      <td>0.998391</td>\n",
       "      <td>0.990105</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>655</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.999234</td>\n",
       "      <td>0.999668</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.998305</td>\n",
       "      <td>0.997796</td>\n",
       "      <td>0.997711</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.999576</td>\n",
       "      <td>0.999513</td>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.904813</td>\n",
       "      <td>0.914674</td>\n",
       "      <td>0.872844</td>\n",
       "      <td>0.905037</td>\n",
       "      <td>0.898775</td>\n",
       "      <td>0.899229</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>18401</td>\n",
       "      <td>0.999642</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.999460</td>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.999736</td>\n",
       "      <td>0.999502</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "21368       0.988638      0.123793         0.648347        0.049381   \n",
       "25688       0.603336      0.097788         0.666953        0.057021   \n",
       "21278       1.280090      0.071650         0.550490        0.033447   \n",
       "25328       0.844990      0.213842         0.704759        0.085071   \n",
       "36868       0.592534      0.078234         0.559726        0.049486   \n",
       "\n",
       "      param_model__alpha param_model__class_weight  \\\n",
       "21368           0.000108                      None   \n",
       "25688           0.000146                      None   \n",
       "21278           0.000108                      None   \n",
       "25328           0.000143                      None   \n",
       "36868           0.000321                      None   \n",
       "\n",
       "      param_model__early_stopping param_model__learning_rate  \\\n",
       "21368                       False                   adaptive   \n",
       "25688                       False                   adaptive   \n",
       "21278                        True                   adaptive   \n",
       "25328                       False                   adaptive   \n",
       "36868                       False                   adaptive   \n",
       "\n",
       "      param_model__loss param_model__penalty param_model__eta0  \\\n",
       "21368        perceptron                   l2               0.1   \n",
       "25688        perceptron                   l2               0.1   \n",
       "21278        perceptron                   l2               0.1   \n",
       "25328        perceptron                   l2               0.1   \n",
       "36868        perceptron                   l2              0.03   \n",
       "\n",
       "                                                  params  \\\n",
       "21368  {'model__alpha': 0.00010791110315313644, 'mode...   \n",
       "25688  {'model__alpha': 0.00014632908647345993, 'mode...   \n",
       "21278  {'model__alpha': 0.00010791110315313644, 'mode...   \n",
       "25328  {'model__alpha': 0.00014266210152607368, 'mode...   \n",
       "36868  {'model__alpha': 0.00032137643441002787, 'mode...   \n",
       "\n",
       "       split0_test_f1_weighted  split1_test_f1_weighted  \\\n",
       "21368                 0.998106                 0.997502   \n",
       "25688                 0.998360                 0.997646   \n",
       "21278                 0.998105                 0.997646   \n",
       "25328                 0.998248                 0.997667   \n",
       "36868                 0.998275                 0.997752   \n",
       "\n",
       "       split2_test_f1_weighted  split3_test_f1_weighted  \\\n",
       "21368                 0.998275                 0.998698   \n",
       "25688                 0.997765                 0.998703   \n",
       "21278                 0.997908                 0.998509   \n",
       "25328                 0.997784                 0.998613   \n",
       "36868                 0.997653                 0.998618   \n",
       "\n",
       "       split4_test_f1_weighted  mean_test_f1_weighted  std_test_f1_weighted  \\\n",
       "21368                 0.998452               0.998207              0.000403   \n",
       "25688                 0.998347               0.998164              0.000398   \n",
       "21278                 0.998621               0.998158              0.000365   \n",
       "25328                 0.998347               0.998132              0.000355   \n",
       "36868                 0.998347               0.998129              0.000368   \n",
       "\n",
       "       rank_test_f1_weighted  split0_train_f1_weighted  \\\n",
       "21368                      1                  0.999682   \n",
       "25688                      2                  0.999682   \n",
       "21278                      3                  0.999534   \n",
       "25328                      4                  0.999682   \n",
       "36868                      5                  0.999619   \n",
       "\n",
       "       split1_train_f1_weighted  split2_train_f1_weighted  \\\n",
       "21368                  0.999555                  0.999703   \n",
       "25688                  0.999555                  0.999555   \n",
       "21278                  0.999343                  0.999575   \n",
       "25328                  0.999555                  0.999682   \n",
       "36868                  0.999576                  0.999512   \n",
       "\n",
       "       split3_train_f1_weighted  split4_train_f1_weighted  \\\n",
       "21368                  0.999619                  0.999682   \n",
       "25688                  0.999619                  0.999682   \n",
       "21278                  0.999236                  0.999576   \n",
       "25328                  0.999004                  0.999661   \n",
       "36868                  0.999067                  0.999703   \n",
       "\n",
       "       mean_train_f1_weighted  std_train_f1_weighted  split0_test_f1_micro  \\\n",
       "21368                0.999648               0.000055              0.998135   \n",
       "25688                0.999618               0.000057              0.998389   \n",
       "21278                0.999453               0.000138              0.998135   \n",
       "25328                0.999517               0.000261              0.998305   \n",
       "36868                0.999496               0.000223              0.998305   \n",
       "\n",
       "       split1_test_f1_micro  split2_test_f1_micro  split3_test_f1_micro  \\\n",
       "21368              0.997626              0.998305              0.998728   \n",
       "25688              0.997711              0.997796              0.998728   \n",
       "21278              0.997711              0.997965              0.998559   \n",
       "25328              0.997711              0.997881              0.998643   \n",
       "36868              0.997796              0.997711              0.998643   \n",
       "\n",
       "       split4_test_f1_micro  mean_test_f1_micro  std_test_f1_micro  \\\n",
       "21368              0.998474            0.998254           0.000370   \n",
       "25688              0.998389            0.998203           0.000388   \n",
       "21278              0.998643            0.998203           0.000353   \n",
       "25328              0.998389            0.998186           0.000342   \n",
       "36868              0.998389            0.998169           0.000358   \n",
       "\n",
       "       rank_test_f1_micro  split0_train_f1_micro  split1_train_f1_micro  \\\n",
       "21368                   1               0.999682               0.999555   \n",
       "25688                   3               0.999682               0.999555   \n",
       "21278                   2               0.999534               0.999343   \n",
       "25328                   5               0.999682               0.999555   \n",
       "36868                   7               0.999618               0.999576   \n",
       "\n",
       "       split2_train_f1_micro  split3_train_f1_micro  split4_train_f1_micro  \\\n",
       "21368               0.999703               0.999619               0.999682   \n",
       "25688               0.999555               0.999619               0.999682   \n",
       "21278               0.999576               0.999237               0.999576   \n",
       "25328               0.999682               0.999004               0.999661   \n",
       "36868               0.999513               0.999067               0.999703   \n",
       "\n",
       "       mean_train_f1_micro  std_train_f1_micro  split0_test_f1_macro  \\\n",
       "21368             0.999648            0.000055              0.940623   \n",
       "25688             0.999619            0.000057              0.940816   \n",
       "21278             0.999453            0.000138              0.940622   \n",
       "25328             0.999517            0.000261              0.915129   \n",
       "36868             0.999496            0.000223              0.940717   \n",
       "\n",
       "       split1_test_f1_macro  split2_test_f1_macro  split3_test_f1_macro  \\\n",
       "21368              0.907198              0.940804              0.941322   \n",
       "25688              0.935551              0.940377              0.927604   \n",
       "21278              0.935587              0.914886              0.902767   \n",
       "25328              0.947997              0.884593              0.941257   \n",
       "36868              0.948070              0.914718              0.927538   \n",
       "\n",
       "       split4_test_f1_macro  mean_test_f1_macro  std_test_f1_macro  \\\n",
       "21368              0.954484            0.936886           0.015748   \n",
       "25688              0.936083            0.936086           0.004754   \n",
       "21278              0.954613            0.929695           0.018544   \n",
       "25328              0.936075            0.925010           0.023004   \n",
       "36868              0.936083            0.933425           0.011483   \n",
       "\n",
       "       rank_test_f1_macro  ...  split2_train_precision_micro  \\\n",
       "21368                  38  ...                      0.999703   \n",
       "25688                  47  ...                      0.999555   \n",
       "21278                 249  ...                      0.999576   \n",
       "25328                 462  ...                      0.999682   \n",
       "36868                 109  ...                      0.999513   \n",
       "\n",
       "       split3_train_precision_micro  split4_train_precision_micro  \\\n",
       "21368                      0.999619                      0.999682   \n",
       "25688                      0.999619                      0.999682   \n",
       "21278                      0.999237                      0.999576   \n",
       "25328                      0.999004                      0.999661   \n",
       "36868                      0.999067                      0.999703   \n",
       "\n",
       "       mean_train_precision_micro  std_train_precision_micro  \\\n",
       "21368                    0.999648                   0.000055   \n",
       "25688                    0.999619                   0.000057   \n",
       "21278                    0.999453                   0.000138   \n",
       "25328                    0.999517                   0.000261   \n",
       "36868                    0.999496                   0.000223   \n",
       "\n",
       "       split0_test_precision_macro  split1_test_precision_macro  \\\n",
       "21368                     0.998074                     0.998119   \n",
       "25688                     0.998349                     0.998099   \n",
       "21278                     0.998182                     0.998099   \n",
       "25328                     0.998257                     0.998099   \n",
       "36868                     0.998257                     0.998136   \n",
       "\n",
       "       split2_test_precision_macro  split3_test_precision_macro  \\\n",
       "21368                     0.998576                     0.999093   \n",
       "25688                     0.998297                     0.957462   \n",
       "21278                     0.998315                     0.949057   \n",
       "25328                     0.998334                     0.999114   \n",
       "36868                     0.998260                     0.957484   \n",
       "\n",
       "       split4_test_precision_macro  mean_test_precision_macro  \\\n",
       "21368                     0.998428                   0.998458   \n",
       "25688                     0.998391                   0.990120   \n",
       "21278                     0.998612                   0.988453   \n",
       "25328                     0.998446                   0.998450   \n",
       "36868                     0.998391                   0.990105   \n",
       "\n",
       "       std_test_precision_macro  rank_test_precision_macro  \\\n",
       "21368                  0.000369                          1   \n",
       "25688                  0.016329                        652   \n",
       "21278                  0.019699                        869   \n",
       "25328                  0.000351                          4   \n",
       "36868                  0.016311                        655   \n",
       "\n",
       "       split0_train_precision_macro  split1_train_precision_macro  \\\n",
       "21368                      0.999628                      0.999711   \n",
       "25688                      0.999632                      0.999711   \n",
       "21278                      0.999532                      0.999516   \n",
       "25328                      0.999624                      0.999715   \n",
       "36868                      0.999594                      0.999738   \n",
       "\n",
       "       split2_train_precision_macro  split3_train_precision_macro  \\\n",
       "21368                      0.999664                      0.999562   \n",
       "25688                      0.999711                      0.999566   \n",
       "21278                      0.999595                      0.999440   \n",
       "25328                      0.999651                      0.999227   \n",
       "36868                      0.999706                      0.999234   \n",
       "\n",
       "       split4_train_precision_macro  mean_train_precision_macro  \\\n",
       "21368                      0.999659                    0.999645   \n",
       "25688                      0.999655                    0.999655   \n",
       "21278                      0.999547                    0.999526   \n",
       "25328                      0.999653                    0.999574   \n",
       "36868                      0.999668                    0.999588   \n",
       "\n",
       "       std_train_precision_macro  split0_test_accuracy  split1_test_accuracy  \\\n",
       "21368                   0.000049              0.998135              0.997626   \n",
       "25688                   0.000054              0.998389              0.997711   \n",
       "21278                   0.000050              0.998135              0.997711   \n",
       "25328                   0.000176              0.998305              0.997711   \n",
       "36868                   0.000183              0.998305              0.997796   \n",
       "\n",
       "       split2_test_accuracy  split3_test_accuracy  split4_test_accuracy  \\\n",
       "21368              0.998305              0.998728              0.998474   \n",
       "25688              0.997796              0.998728              0.998389   \n",
       "21278              0.997965              0.998559              0.998643   \n",
       "25328              0.997881              0.998643              0.998389   \n",
       "36868              0.997711              0.998643              0.998389   \n",
       "\n",
       "       mean_test_accuracy  std_test_accuracy  rank_test_accuracy  \\\n",
       "21368            0.998254           0.000370                   1   \n",
       "25688            0.998203           0.000388                   3   \n",
       "21278            0.998203           0.000353                   2   \n",
       "25328            0.998186           0.000342                   5   \n",
       "36868            0.998169           0.000358                   7   \n",
       "\n",
       "       split0_train_accuracy  split1_train_accuracy  split2_train_accuracy  \\\n",
       "21368               0.999682               0.999555               0.999703   \n",
       "25688               0.999682               0.999555               0.999555   \n",
       "21278               0.999534               0.999343               0.999576   \n",
       "25328               0.999682               0.999555               0.999682   \n",
       "36868               0.999618               0.999576               0.999513   \n",
       "\n",
       "       split3_train_accuracy  split4_train_accuracy  mean_train_accuracy  \\\n",
       "21368               0.999619               0.999682             0.999648   \n",
       "25688               0.999619               0.999682             0.999619   \n",
       "21278               0.999237               0.999576             0.999453   \n",
       "25328               0.999004               0.999661             0.999517   \n",
       "36868               0.999067               0.999703             0.999496   \n",
       "\n",
       "       std_train_accuracy  split0_test_balanced_accuracy  \\\n",
       "21368            0.000055                       0.904807   \n",
       "25688            0.000057                       0.904919   \n",
       "21278            0.000138                       0.904699   \n",
       "25328            0.000261                       0.873669   \n",
       "36868            0.000223                       0.904813   \n",
       "\n",
       "       split1_test_balanced_accuracy  split2_test_balanced_accuracy  \\\n",
       "21368                       0.864766                       0.904668   \n",
       "25688                       0.898007                       0.904094   \n",
       "21278                       0.898078                       0.873125   \n",
       "25328                       0.914566                       0.841875   \n",
       "36868                       0.914674                       0.872844   \n",
       "\n",
       "       split3_test_balanced_accuracy  split4_test_balanced_accuracy  \\\n",
       "21368                       0.905188                       0.923775   \n",
       "25688                       0.905188                       0.898775   \n",
       "21278                       0.873787                       0.923849   \n",
       "25328                       0.905037                       0.898704   \n",
       "36868                       0.905037                       0.898775   \n",
       "\n",
       "       mean_test_balanced_accuracy  std_test_balanced_accuracy  \\\n",
       "21368                     0.900641                    0.019372   \n",
       "25688                     0.902197                    0.003138   \n",
       "21278                     0.894708                    0.019307   \n",
       "25328                     0.886770                    0.026214   \n",
       "36868                     0.899229                    0.014141   \n",
       "\n",
       "       rank_test_balanced_accuracy  split0_train_balanced_accuracy  \\\n",
       "21368                        18192                        0.999706   \n",
       "25688                        17909                        0.999702   \n",
       "21278                        19439                        0.993465   \n",
       "25328                        21159                        0.999710   \n",
       "36868                        18401                        0.999642   \n",
       "\n",
       "       split1_train_balanced_accuracy  split2_train_balanced_accuracy  \\\n",
       "21368                        0.999502                        0.999775   \n",
       "25688                        0.999502                        0.999502   \n",
       "21278                        0.991937                        0.981409   \n",
       "25328                        0.999498                        0.999756   \n",
       "36868                        0.999525                        0.999460   \n",
       "\n",
       "       split3_train_balanced_accuracy  split4_train_balanced_accuracy  \\\n",
       "21368                        0.999663                        0.999713   \n",
       "25688                        0.999659                        0.999717   \n",
       "21278                        0.987007                        0.993221   \n",
       "25328                        0.999058                        0.999686   \n",
       "36868                        0.999148                        0.999736   \n",
       "\n",
       "       mean_train_balanced_accuracy  std_train_balanced_accuracy  \n",
       "21368                      0.999672                     0.000092  \n",
       "25688                      0.999616                     0.000095  \n",
       "21278                      0.989408                     0.004629  \n",
       "25328                      0.999542                     0.000257  \n",
       "36868                      0.999502                     0.000201  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs2.cv_results_).sort_values(by=\"mean_test_f1_weighted\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc283634-2d25-486b-b4a2-d355ddf5ff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>eta0</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "      <th>mean_train_f1_weighted</th>\n",
       "      <th>mean_test_f1_micro</th>\n",
       "      <th>rank_test_f1_micro</th>\n",
       "      <th>mean_train_f1_micro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>rank_test_f1_macro</th>\n",
       "      <th>mean_train_f1_macro</th>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <th>mean_train_recall_weighted</th>\n",
       "      <th>mean_test_recall_micro</th>\n",
       "      <th>rank_test_recall_micro</th>\n",
       "      <th>mean_train_recall_micro</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>rank_test_recall_macro</th>\n",
       "      <th>mean_train_recall_macro</th>\n",
       "      <th>mean_test_precision_weighted</th>\n",
       "      <th>rank_test_precision_weighted</th>\n",
       "      <th>mean_train_precision_weighted</th>\n",
       "      <th>mean_test_precision_micro</th>\n",
       "      <th>rank_test_precision_micro</th>\n",
       "      <th>mean_train_precision_micro</th>\n",
       "      <th>mean_test_precision_macro</th>\n",
       "      <th>rank_test_precision_macro</th>\n",
       "      <th>mean_train_precision_macro</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_train_balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0.244205</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'model__alpha': 3.3908066818940525e-05, 'mode...</td>\n",
       "      <td>0.998320</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.998355</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.937236</td>\n",
       "      <td>164</td>\n",
       "      <td>0.999670</td>\n",
       "      <td>0.998355</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.998355</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.904451</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999704</td>\n",
       "      <td>0.998350</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.998355</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.990150</td>\n",
       "      <td>987</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.998355</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999661</td>\n",
       "      <td>0.904451</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0.126486</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'model__alpha': 6.207679593624768e-05, 'model...</td>\n",
       "      <td>0.998309</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.942354</td>\n",
       "      <td>15</td>\n",
       "      <td>0.999637</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.910653</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.998334</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.990179</td>\n",
       "      <td>978</td>\n",
       "      <td>0.999629</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.910653</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "      <td>0.00015</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0.071969</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'model__alpha': 0.00015023380843318595, 'mode...</td>\n",
       "      <td>0.998307</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.945110</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.910644</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999546</td>\n",
       "      <td>0.998339</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.998538</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999572</td>\n",
       "      <td>0.910644</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>0.000059</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0.202359</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'model__alpha': 5.9255309755456745e-05, 'mode...</td>\n",
       "      <td>0.998304</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.939852</td>\n",
       "      <td>59</td>\n",
       "      <td>0.998376</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.907293</td>\n",
       "      <td>28</td>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.998334</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.990201</td>\n",
       "      <td>973</td>\n",
       "      <td>0.997253</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.907293</td>\n",
       "      <td>28</td>\n",
       "      <td>0.999554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0.323746</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>perceptron</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'model__alpha': 3.3908066818940525e-05, 'mode...</td>\n",
       "      <td>0.998303</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.942601</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999610</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.907315</td>\n",
       "      <td>27</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.998339</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.907315</td>\n",
       "      <td>27</td>\n",
       "      <td>0.999580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha class_weight early_stopping      eta0 learning_rate  \\\n",
       "187   0.000034         None          False  0.244205      adaptive   \n",
       "1506  0.000062         None          False  0.126486      adaptive   \n",
       "3438   0.00015         None          False  0.071969      adaptive   \n",
       "1409  0.000059         None          False  0.202359      adaptive   \n",
       "190   0.000034         None          False  0.323746      adaptive   \n",
       "\n",
       "            loss penalty                                             params  \\\n",
       "187   perceptron      l2  {'model__alpha': 3.3908066818940525e-05, 'mode...   \n",
       "1506  perceptron      l2  {'model__alpha': 6.207679593624768e-05, 'model...   \n",
       "3438  perceptron      l2  {'model__alpha': 0.00015023380843318595, 'mode...   \n",
       "1409  perceptron      l2  {'model__alpha': 5.9255309755456745e-05, 'mode...   \n",
       "190   perceptron      l2  {'model__alpha': 3.3908066818940525e-05, 'mode...   \n",
       "\n",
       "      mean_test_f1_weighted  rank_test_f1_weighted  mean_train_f1_weighted  \\\n",
       "187                0.998320                      1                0.999661   \n",
       "1506               0.998309                      2                0.999623   \n",
       "3438               0.998307                      3                0.999572   \n",
       "1409               0.998304                      4                0.999525   \n",
       "190                0.998303                      5                0.999585   \n",
       "\n",
       "      mean_test_f1_micro  rank_test_f1_micro  mean_train_f1_micro  \\\n",
       "187             0.998355                   1             0.999661   \n",
       "1506            0.998338                   4             0.999623   \n",
       "3438            0.998338                   3             0.999572   \n",
       "1409            0.998338                   5             0.999525   \n",
       "190             0.998338                   2             0.999585   \n",
       "\n",
       "      mean_test_f1_macro  rank_test_f1_macro  mean_train_f1_macro  \\\n",
       "187             0.937236                 164             0.999670   \n",
       "1506            0.942354                  15             0.999637   \n",
       "3438            0.945110                   1             0.999600   \n",
       "1409            0.939852                  59             0.998376   \n",
       "190             0.942601                  10             0.999610   \n",
       "\n",
       "      mean_test_recall_weighted  rank_test_recall_weighted  \\\n",
       "187                    0.998355                          1   \n",
       "1506                   0.998338                          4   \n",
       "3438                   0.998338                          3   \n",
       "1409                   0.998338                          5   \n",
       "190                    0.998338                          2   \n",
       "\n",
       "      mean_train_recall_weighted  mean_test_recall_micro  \\\n",
       "187                     0.999661                0.998355   \n",
       "1506                    0.999623                0.998338   \n",
       "3438                    0.999572                0.998338   \n",
       "1409                    0.999525                0.998338   \n",
       "190                     0.999585                0.998338   \n",
       "\n",
       "      rank_test_recall_micro  mean_train_recall_micro  mean_test_recall_macro  \\\n",
       "187                        1                 0.999661                0.904451   \n",
       "1506                       4                 0.999623                0.910653   \n",
       "3438                       3                 0.999572                0.910644   \n",
       "1409                       5                 0.999525                0.907293   \n",
       "190                        2                 0.999585                0.907315   \n",
       "\n",
       "      rank_test_recall_macro  mean_train_recall_macro  \\\n",
       "187                      128                 0.999704   \n",
       "1506                       1                 0.999644   \n",
       "3438                       2                 0.999546   \n",
       "1409                      28                 0.999554   \n",
       "190                       27                 0.999580   \n",
       "\n",
       "      mean_test_precision_weighted  rank_test_precision_weighted  \\\n",
       "187                       0.998350                             1   \n",
       "1506                      0.998334                             5   \n",
       "3438                      0.998339                             2   \n",
       "1409                      0.998334                             4   \n",
       "190                       0.998339                             3   \n",
       "\n",
       "      mean_train_precision_weighted  mean_test_precision_micro  \\\n",
       "187                        0.999661                   0.998355   \n",
       "1506                       0.999623                   0.998338   \n",
       "3438                       0.999572                   0.998338   \n",
       "1409                       0.999526                   0.998338   \n",
       "190                        0.999585                   0.998338   \n",
       "\n",
       "      rank_test_precision_micro  mean_train_precision_micro  \\\n",
       "187                           1                    0.999661   \n",
       "1506                          4                    0.999623   \n",
       "3438                          3                    0.999572   \n",
       "1409                          5                    0.999525   \n",
       "190                           2                    0.999585   \n",
       "\n",
       "      mean_test_precision_macro  rank_test_precision_macro  \\\n",
       "187                    0.990150                        987   \n",
       "1506                   0.990179                        978   \n",
       "3438                   0.998538                          1   \n",
       "1409                   0.990201                        973   \n",
       "190                    0.998516                          2   \n",
       "\n",
       "      mean_train_precision_macro  mean_test_accuracy  rank_test_accuracy  \\\n",
       "187                     0.999635            0.998355                   1   \n",
       "1506                    0.999629            0.998338                   4   \n",
       "3438                    0.999654            0.998338                   3   \n",
       "1409                    0.997253            0.998338                   5   \n",
       "190                     0.999639            0.998338                   2   \n",
       "\n",
       "      mean_train_accuracy  mean_test_balanced_accuracy  \\\n",
       "187              0.999661                     0.904451   \n",
       "1506             0.999623                     0.910653   \n",
       "3438             0.999572                     0.910644   \n",
       "1409             0.999525                     0.907293   \n",
       "190              0.999585                     0.907315   \n",
       "\n",
       "      rank_test_balanced_accuracy  mean_train_balanced_accuracy  \n",
       "187                           128                      0.999704  \n",
       "1506                            1                      0.999644  \n",
       "3438                            2                      0.999546  \n",
       "1409                           28                      0.999554  \n",
       "190                            27                      0.999580  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"Action\"\n",
    "drop_cols = [\"NAT Destination Port\"]\n",
    "\n",
    "X = train_df.drop(columns=drop_cols+[target])\n",
    "y = train_df[target].to_numpy()\n",
    "\n",
    "ohe_cols = [\"Source Port\", \"Destination Port\", \"NAT Source Port\"]\n",
    "scale_cols = [c for c in X.columns if c not in ohe_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[(\"ohe\", OneHotEncoder(handle_unknown='infrequent_if_exist'), \n",
    "                                              ohe_cols), \n",
    "                                             (\"scale\", StandardScaler(), scale_cols)], \n",
    "                               remainder=\"passthrough\",  \n",
    "                               n_jobs=10)\n",
    "\n",
    "model_sgd = Pipeline(steps=[(\"preprocess\", preprocess), \n",
    "                        (\"model\", SGDClassifier(random_state=7742, \n",
    "                                                tol=5e-4, \n",
    "                                                n_jobs=15, \n",
    "                                                max_iter=50_000,\n",
    "                                                shuffle=True))])\n",
    "\n",
    "\n",
    "alphas = list(np.logspace(start=-4.5, stop=-3.5, num=100)) + [gs2.best_params_['model__alpha']]\n",
    "\n",
    "parameter_grid = {\"model__penalty\": [\"l2\"], \n",
    "                  \"model__loss\": [\"perceptron\"], \n",
    "                  \"model__alpha\":alphas, \n",
    "                  \"model__learning_rate\":[\"adaptive\"], \n",
    "                  \"model__eta0\":list(np.logspace(start=-2, stop=0, num=50)) + [gs2.best_params_['model__eta0']],\n",
    "                  \"model__early_stopping\":[False], \n",
    "                  \"model__class_weight\":[None]}\n",
    "\n",
    "metrics=['f1_weighted', 'f1_micro', 'f1_macro',\n",
    "         'recall_weighted', 'recall_micro', 'recall_macro',\n",
    "         'precision_weighted', 'precision_micro', 'precision_macro',\n",
    "         'accuracy', 'balanced_accuracy']\n",
    "\n",
    "# gs3 = run_gridsearch(X=X, \n",
    "#                      y=y, \n",
    "#                      folds=5,\n",
    "#                      estimator=model_sgd, \n",
    "#                      param_grid=parameter_grid, \n",
    "#                      n_jobs=50,\n",
    "#                      scoring= metrics, \n",
    "#                      random_state=7742,\n",
    "#                      save_name=get_gs_save_name(model_name=f\"SGD_Perceptron_Tune2\"))\n",
    "PATH=f\"./models/SGD_Perceptron_Tune2_20221029_1713.pkl\"\n",
    "gs3 = load_gs_from_pickle(pickle_filepath=PATH)\n",
    "gs3_df = gs_to_clean_df(gs3.cv_results_, sort_metric=\"mean_test_f1_weighted\")\n",
    "gs3_df.loc[:,[c for c in gs3_df.columns if \"std\" not in c]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "572c4369-9374-46a4-83e4-c4ae8d87ba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795e-05"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e401c47-d902-45c0-87e7-2afecdf3cc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 3.3908066818940525e-05,\n",
       " 'model__class_weight': None,\n",
       " 'model__early_stopping': False,\n",
       " 'model__eta0': 0.2442053094548651,\n",
       " 'model__learning_rate': 'adaptive',\n",
       " 'model__loss': 'perceptron',\n",
       " 'model__penalty': 'l2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "456cccf4-dfd5-4b99-a38b-1b45e4054906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983202152244438"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aea41c-7a5a-4d58-89ec-5e68dfd87067",
   "metadata": {},
   "source": [
    "# Final Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc319ef0-274d-4beb-b5a3-3ab65cbbf267",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./datasets/test_20221018_1118.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d53c30c-228d-4438-b6f3-46fd96ae8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Action\"\n",
    "drop_cols = [\"NAT Destination Port\"]\n",
    "\n",
    "X_test = test_df.drop(columns=drop_cols+[target])\n",
    "y_test = test_df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8a837b2-e141-4415-8a37-b57ddb98480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "299f24c5-ba0e-4ae5-94a0-bdbccc4204a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allow         3769\n",
       "deny          1493\n",
       "drop          1289\n",
       "reset-both       3\n",
       "Name: predicted, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame({\"predicted\":y_pred, \"true\":y_test})\n",
    "predictions_df[\"predicted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d37b308-fa61-43b1-a6c6-ac149dcee5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allow         3765\n",
       "deny          1499\n",
       "drop          1285\n",
       "reset-both       5\n",
       "Name: true, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[\"true\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd754ca5-8dc3-4bfd-a57f-b01134ec8bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>reset-both</td>\n",
       "      <td>reset-both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>reset-both</td>\n",
       "      <td>reset-both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4614</th>\n",
       "      <td>allow</td>\n",
       "      <td>reset-both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4940</th>\n",
       "      <td>reset-both</td>\n",
       "      <td>reset-both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383</th>\n",
       "      <td>allow</td>\n",
       "      <td>reset-both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted        true\n",
       "2840  reset-both  reset-both\n",
       "2882  reset-both  reset-both\n",
       "4614       allow  reset-both\n",
       "4940  reset-both  reset-both\n",
       "6383       allow  reset-both"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.loc[predictions_df[\"true\"]==\"reset-both\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1845137c-101e-4a91-9ecf-e8de0734d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "926ed48a-c9fc-4612-9479-70ad7cc7182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.9975197287578468\n",
      "Micro F1: 0.9975587427525175\n",
      "Macro F1: 0.9352877498934004\n"
     ]
    }
   ],
   "source": [
    "test_weighted_f1 = f1_score(y_true=y_test, y_pred=y_pred, average=\"weighted\")\n",
    "test_micro_f1 = f1_score(y_true=y_test, y_pred=y_pred, average=\"micro\")\n",
    "test_macro_f1 = f1_score(y_true=y_test, y_pred=y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Weighted F1: {test_weighted_f1}\")\n",
    "print(f\"Micro F1: {test_micro_f1}\")\n",
    "print(f\"Macro F1: {test_macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf272268-9990-456b-8327-60d9d991c884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Label')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAJGCAYAAAAeQiKCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACRZElEQVR4nOzdd1gUV9sG8HthWXoHUYqgCBYEUTS2KCjWxN5bRGNNYkzsrzEqaoxJ1DcxJibR5JVo7BpjSWxRQUWNFXuPYEWk97ac7w8+RhaWKsoo9++69mL3zJnZM4XZZ09bhRBCgIiIiIhkQaeyC0BEREREzzA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMqo1OnTqFLly6wsrKCjo4OFAoFFArFS3v/4OBg6T0VCgXCw8Nf2ntXRUFBQRrHm6iyFXcPGDFihJTu5+f3QssRHh6uUY7g4OAX+n5VCYMzkp3Hjx9j3rx5aNu2Lezs7KBSqWBnZwcfHx989NFHOHHiRKWVLTIyEl27dsW+ffsQFxcH/vqZdvk/IPIee/bs0Zq3TZs2hfJWRMCZf3tBQUHPvb0XqeD+l+bxooPyiviQv3fvHiZOnIj69evD2NgYBgYGqFGjBry8vDBkyBAsXrwYaWlpz13W5/3CUjAAz/8wNTWFt7c3Zs6ciaioqOcu66uCgVflUlZ2AYjyW758OaZNm4aMjAyN9KioKERFReHcuXP49ttvERcXBwsLi5devr179yI2NhZA7gfqhAkTULNmzZdaBldXVyxevFh6bWVl9VLfv7yWL1+Orl27aqRduHABx44dq6QSlU6zZs00jjeVztmzZ9G+fXskJiZqpEdGRiIyMhKXLl3Chg0bMHjwYDg6OlZSKUuWnJyMCxcu4MKFC/j5559x8OBBeHl5VXaxijRo0CA0bNgQAODk5PRC38vKykrjf8PV1fWFvl9VwuCMZGPRokX45JNPpNdKpRLdunVD48aNAQC3bt3C3r17ER0dXVlFxL1796TnDg4O+Pbbb196GZycnDB16tSX/r7Pa+/evbh16xbc3NyktOXLl1diiYqXlJQEU1NTeHh4wMPD44W+V8Hg786dO/jxxx+l1wMHDkTTpk018sg9KH///felwMzU1BSDBg2Cs7MzEhISEBERgRMnTuD+/fuVXErtxo8fD1dXV6SlpeHvv//GkSNHAADR0dEICAjA+fPnS7WdvGvoZerSpQu6dOnyUt7LzMzslbwXvRIEkQxcvnxZ6OrqCgACgKhWrZo4f/58oXzp6eni22+/FcnJyRrp169fF+PGjRN16tQRBgYGwsjISNStW1d8+OGH4u7du4W24+vrK71XQECAuH79uujXr5+wtLQUBgYGokWLFuLw4cNS/sOHD0v5tT18fX2FEEIEBAQUSsuzevVqjXXye/r0qZgyZYpo0KCBMDIyEnp6esLOzk40a9ZMfPDBB+LEiRNFlqXg/mVlZYlVq1aJdu3aCSsrK6FUKoWNjY3o2LGj+O2330ROTo5G/oLbu337tli2bJnw8PAQKpVK1KhRQ3z00UciLS1N+8nTIv9x0NHRkZ5/9NFHUp6YmBhhaGgoAGic+4L7dOvWLTFx4kTRunVr4ejoKIyMjIRKpRIODg6ie/fuYteuXRrvnf/cans4OztLefOnr169WmzcuFE0a9ZMGBkZSfmKOm/Tp0+X0iwtLcXDhw+lZX///bdQKBTS8i1btpT62AlR+JysXr26UJ60tDSxbNky8eabbwpLS0uhp6cn7O3txeDBg8W5c+cK5c/KyhJff/21aNGihTA3Nxe6urrCyspKNGjQQLzzzjtiw4YNWvdX2yP//4Y28fHxGvnXrFmjNd+xY8dEUlJSofS4uDixYMEC0bRpU2FmZiZUKpVwdnYWo0ePFrdu3dLIW1JZAwICii2rtn0uuH9vvvmmxvI7d+5oXS8pKUlMmTJF1KxZU+jq6oq5c+dK2yjr+RJCiOjoaDFu3DhRrVo1YWBgIHx8fMTGjRuLvQcUdw8SIvdeM3fuXNGsWTNhbm4uVCqVcHR0FG+99ZbYsWOHEEIIZ2fnUt3v7t69W+J1sWXLFtG1a1dRrVo1oVQqhaWlpWjTpo34/vvvRUZGhkZebdtbu3at8PHxEQYGBsLa2loEBASImJgYrefQ19dXWFtbC6VSKSwsLIS7u7sYMGCA+P7777UeXzljcEayMG7cOI1/ym3btpV63U2bNgkDA4MibySmpqZi3759Guvk/wD38vISJiYmhdZTqVTi8uXLQogXG5ylpaWJunXrFrv9GTNmSPmLuzEnJyeLtm3bFrutbt26iczMzCK317p1a63rDRkypNTnJP9xsLa2lrZpZmYmfRh/+eWXUp5evXoVuU9btmwp8QN43rx5Ws+ttkdRwVnB/S4pOMvMzBRNmzaV0t9++20hhBBJSUkaH26jRo0q9XEr6pwUDM6ePHkiPD09i9xHpVIpfv311yLPibZH8+bNte6vtkdJwVlMTIxG/kmTJomsrKxS7fv169dFzZo1i3xvY2Njjf/nkspaEcHZ1KlTNZaHhoZqXa/gNZQXnJXnfMXFxYl69eppzf/2228X+f9S3D3o5MmTolq1aiUeq4oIzrKzs8WAAQOK3c4bb7wh4uPjpXUKbq+oe1Hr1q019mvu3LnFvo+dnV2J14DcsFmTZOHQoUPSc0tLS/Tq1atU6926dQvDhw+X+qjZ2toiICAA2dnZ+N///ofExEQkJSWhf//+uHnzJuzs7Apt4+LFi7CxscH48ePx5MkTrF27FgCQmZmJb7/9Fj/99JPUz2v//v04cOCAVM68Ztjn6dtx+PBh3LhxAwBgYGCAUaNGwcHBAZGRkbh9+zZCQkJKva0PP/xQaoIBgK5du6JZs2Y4cuSI1KF39+7dmD17Nr744gut2wgNDUXnzp3RrFkzrF+/Hv/++y8AYMOGDfjqq6/g4OBQ5n2cMGECQkNDkZiYiLVr12LcuHH44YcfAAAuLi7o3r07/vjjD63r6unpoUmTJvDx8YGtrS3MzMyQnJyM0NBQHD58GACwYMEC6bi999576NatG6ZNmyZtI3+zoLm5eZH7bWdnh4EDB8LKygp3794tdp/09PSwfv16NG7cGCkpKfjzzz8RFBSEEydOICIiAgDg7u6OZcuWlelYlcawYcNw6dIlALn7M3ToUFSvXh0hISE4ePAgsrOzMXr0aPj4+MDDwwPJycn47bffpPX79u2LJk2aSE2M+a+xvD52mzZtwpkzZwAAtWvXxnvvvSflKalvkZWVFZycnKRmy6+//hpBQUFo1aoVmjRpgtatW6Ndu3ZQqVQa66nVavTu3VvqPmBnZ4ehQ4fC3Nwcu3fvxunTp5GSkoIBAwbg1q1bsLW1xeLFiws1A3/yySewtLQEAKn/1fM4efKkxuvq1atrzRcaGorWrVvD398fSUlJUl+6sp4vAPj0009x/fp1adu+vr7w9fVFaGgo/vzzzzLvQ2JiInr06KExqKFjx45o0aIF4uPjNTr8z5o1C+Hh4fj888+ltLymXqB097uFCxdi8+bN0uu84xIWFoadO3cCyB35Pm7cOGzcuFHrNkJDQ9GyZUv4+/tj9+7dCAsLk9JPnDiBli1bAoB0LwEAf39/tGvXDikpKbh//z6OHTtWIYNOXrrKjg6JhBDCyMio0Df40vjoo4+k9XR0dMTVq1elZUeOHNH49vTZZ59Jy/LXrujo6IgLFy5Iy/LX4jRp0kTj/fJ/Q8tfA5OnPDVnv//+u5TWuXPnQttMT08XDx48kF4XVXMWHR2t0Tw4ePBgaZ2cnBzh7++vUfuQnp6udXv9+vWT1gsLC9NYtnPnTm2nodjjYG1tLTIzM0WNGjUEANGgQQOxfft2aflXX31V6Nhoa4q+ceOG2Lhxo1i+fLlYsmSJWLx4scZ1U7DpLP/2tDULFsxjYWGh0TSZp7jmaCGE+OWXXzSOa95zPT09cebMmVIdr4KKqzm7cOGCxrLjx49Ly3JyckTLli2lZWPGjBFCCBEbGyulmZmZFWpOysnJEf/++69GWknNYyXZtGlTsbUZlpaW4quvvtJoZt+xY4e0XKVSifDwcGlZRkaGRo3awoULizxe2q6f4hQ8x+PHjxeLFy8WCxYsKFQT26hRoyLXGzRoUKFuA+U5X1lZWRq1+W3bthVqtVpap1OnTkXub1HnbdmyZRrrfPHFF4WOQ/5roDRNlkXlyc7OFlZWVlL6m2++KZVfCCHeffddaZlCoRD379/Xur0WLVpINa4xMTEa97dvv/1W2p6ZmZmU/vjx40LlzGuGfpWw5oxeacePH5eeN23aFPXr15det2nTBrVq1ZJqQPLnza9ly5Yao6/q1q0rPY+Li6voIhfSrFkz6OvrIyMjA/v27YOHhwe8vLzg7u6Oxo0bw9/fv1S1Vf/88w/UarX0+p133pGeKxQKDB8+HAcPHgQApKSk4OLFi2jWrFmh7YwbN056nv9YAOU/Hnp6ehg/fjzmzp2Lq1evYsKECQAAQ0NDjB49Gjt27Chy3fDwcAwdOrTI85fnwYMH5SpbnoCAANjb25d5vXfffRf79u3D5s2bkZKSIqUvXLgQPj4+z1UmbUJDQzVet2rVqsi8ecfM0tISHh4euHLlChITE1GrVi00a9YMbm5u8PT0hL+/P2rVqlWh5RwwYAAsLCzw2Wef4dixY4WmnYmLi8P06dOhUqnw0UcfFdq3zMxMuLi4lLhvL0L+Wrj8rKysip2WZcaMGYXmwivP+bp+/TqSk5Ol9MGDB0NHJ3fmK4VCgaFDh2L//v3F7kNB+cthamqqtSN/RV0DN27ckEa1A8CQIUOk8gO5/2v/+9//AABCCJw4cQL9+/cvtJ1Ro0ZBqcwNU6ysrGBjY4MnT54A0LwXtWnTRqpNbNiwIZo3bw43Nzd4eHigXbt2qFOnToXs18vEec5IFvIHHzdv3iz1/GH5/0GrVatWaHn+ZsyiAgtnZ2eN1/r6+tLznJycUpVDm4L7UHB6kDyOjo4ICgqCjY0NAODq1avYuHEj5s+fj969e8Pe3h6bNm0q8f0K7l/B41GwSbc0xyP/sQCe73iMGzdOasZ6+PAhgNzmnrzmp6L06tWrVB/ERR3f0nJ3dy/3unnBZp685ukXIf+HXkmePn0qPV+/fj0aNGgAAHj06BF27NiBJUuWICAgADVr1sTkyZMrvKydOnXCkSNHEBMTgz///BOffvopPD09NfJ888030vPy7tuLZGxsDE9PT0yfPh1XrlyBt7d3kXm1XUPl2af4+HiN9JL+l0sjfzmcnJygq6tb5m2U1ou4FwFF35t/+OEHtGjRAgAQExODv/76C8uWLcPYsWPh5uaGgQMHPte9qzKw5oxkoX379rh16xaA3H/UHTt2lKrfWf4Pdm0TROZ9yyqYNz89PT2N188zC3z+b4cF+znk7Z82gwYNQt++fXHq1ClcunQJt27dwuHDh3H+/HkkJydj1KhR6NatG4yNjYvcRsH9K3g88h8Lbfnz5D8eFTkjvp2dHfr164f169dLaQWDmoJu3LiBCxcuSK8nTZqE//znP7C1tYVCoUC1atUq7EPayMioXOulp6dr9MfKS3v//feL7EvzPAqet88//7zQNZwn/z55eXnhypUruHTpEs6dO4dbt27h3Llz2LNnD3JycvD111+jR48eL2RWeUtLS7z11lt46623sGDBAvTq1UuqLY2IiEB2djaUSqXGvpmYmGDu3LlFbrOofl8V4fDhw+U6DtquofKcr4JzOJb0v1wa+adeuX//PtRq9QsL0F7EvQgo+n7k5OSEEydO4Pbt2zh16hRu3bqFixcvYufOncjOzsbmzZvRtWtXjBgxoox7UnkYnJEsTJgwAatWrZK+3bz33nuoXbt2ockeMzMzsXLlSowcORLGxsZo1aoVTp8+DQA4c+YMrl27JjVtHj16VKNTd3HNCRUl/031xo0bSEhIgLm5OSIjI7FmzRqt68TGxiIpKQnOzs5o3bo1WrduDSA3SM27oaakpOD69evFNpO98cYb0NXVlZo2165dK036KoSQBjoAubUBlTGR5ocffigFZ76+viWWISYmRuP1sGHDpG/hhw4dKjYwUyqVyM7OBgCkpqY+T7GLNXXqVFy5cgVA7jf9qKgopKWlYdOmTejatSsCAgIq9P0KXsfVq1fHyJEjC+U7deqURk1DWFgYvL294enpqVF71ahRI1y8eBFA7sSxeUFJ/g/G8hy/gIAAfPTRR2jSpEmhZfm/ZBgbG0tNV/n3LTk5GU2aNEH79u011hVC4NChQ6hdu7aUVvBD/EWe77Iqz/mqV68eTExMpKbNjRs3Yty4cVAoFBBCYN26dWUuR+vWraUO+klJSfj6668LNW1GRERItVXPc0zr1q0LKysrqbZu/fr1GDdunPTl9ddff5XyKhQKqdarvC5cuABPT0/UqVNHowmzZ8+e0uCDs2fPMjgjKquGDRti3rx5mD17NoDcWcR9fHzQo0cPqRnh5s2b0iS0w4YNA5A70eUPP/yAzMxM5OTkwNfXV2O0Zh5TU1OMHj36he9H/olCExMT4ePjg2bNmiE4OLjIQOLmzZto2bIlmjVrhkaNGsHe3h5KpRJ79+7VyFfSLyLY2NjgnXfekfrEbNiwAfHx8XjjjTcQEhKiMRrr/fffL9Rk+TK0aNECu3fvRlZWVqkmdq1Tpw50dHSkoH3YsGEYNGgQHj9+XOJPMjk4OEijJpcuXYqYmBgYGhpK/fgqwp9//onvv/8eQG6t6fr163H69Gl8/PHHAHKD0TfffLNCZ0739vaGv7+/1H9wzJgx2LVrl/R/cvfuXYSEhODu3btYvXo1GjVqBCD32Nvb26NNmzawt7eHmZkZLly4IAVmgOY1lr+rwdmzZ/HRRx/ByckJKpUKEydOLLGca9aswZo1a1CnTh20adNG6j926tQpjdGG+SdM7datG+rWrSuNXn777bfRt29f1KtXD9nZ2bh58yaCg4Px+PFjHD58WOojVbBP5vvvv48uXbpAqVSiR48ez9Vk/bzKc76USiWGDx+OFStWAABCQkLQrl07abRm3rbKYsSIEVi4cKFUizVt2jQcOHAALVq0QHJyMo4ePYoGDRpI/1e2trbQ09NDVlYWgNwRnGFhYVCpVPDz8ys0KXJ+urq6mDhxIgIDAwEAx44dQ9u2bdGhQweEhYVp9DHt16/fc/+SwcCBA5GQkIB27drBwcEBVlZWuHPnDv766y8pT2X8osxzqcTBCESF/Pe//xUqlarYUV4ARFxcnLTOhg0bhL6+fpF5jY2NxV9//aXxPgUnoc2vuBGZJY3WTE1NFa6uroXKoFAoRIcOHbSO+jtx4kSJ+9unTx8pf3Ej0xITE4ucGyjv0bVrV43ReiWNdMu/rKhRjwUVHK1ZkuJGa44fP17rfvj7+wsHBwfpdf4JP4UQYtKkSVrX++CDD8q0b0WN1oyMjBS2trZS+vTp04UQuaPp2rVrJ6U3b9681HN85SlpnrPIyMhi583Stl5x/yMARK1atTTmnDp//rzGBML5/59Ko6SyARA1atQoNJLu2rVrxc5zlvcoOHqwSZMmWvOVZgLgkuY5K+16RSnP+YqNjRXu7u5a8/n5+RX5/1IR85zl6d27t9Z8ixcvFkIUP6IzKytL9OnTp9j99fHxEbGxsdI6JY0QzT//Wv7/95LmibSysirzCN7KxgEBJCuTJk3Cv//+i7lz56J169bStzdbW1s0adIEH374IUJDQzW+BQ0aNAjnz5/HmDFj4OrqCgMDAxgYGMDd3R0ffPABLl68WOg3HV8UQ0NDHDx4EH369IGZmRmMjIzQtm1b/P333xg6dKjWderWrYulS5eiT58+cHd3h7m5OXR1dWFpaYnWrVtj2bJlpe67ZGpqiuDgYPz000/w9fWFpaUllEolrK2t4e/vj19//RW7d+8uNL+UnC1fvhzz58+Hs7Mz9PT0ULNmTUybNg27du2SmsO0WbhwISZOnAgHB4cK71sjhMCIESOk2lAPDw/Mnz8fAKQfWjczMwOQO4o2rwahotjZ2eHUqVNYvnw5fH19YWVlBaVSierVq8PHxwfvvfce9u3bp3HN/fDDDxg5ciS8vLxga2sLpVIJExMTeHl5Yfr06fjnn3805oDz9vbGhg0b0KRJExgYGJS5jOfOncPixYvx9ttvo379+rC2toauri7MzMzg4+ODTz/9FJcuXdJongRym/QuXryIzz//HM2bN4e5uTn09PTg4OCA5s2bY8qUKTh69Cjatm2rsd62bdvQu3dvWFlZVWhfyYpQnvNlaWmJY8eOYcyYMbC1tYW+vj4aNWqE1atXF9sXrzjNmzfH5cuXMWfOHPj4+MDMzAx6enqoXr06OnfujN69e2vkX7VqFQICAmBnZ6fRn7Y0lEoltm7dio0bN6Jz586wsbGBUqmEhYUFWrdujW+//RahoaElDggqjUWLFmH8+PHw8fFB9erVoaenByMjI9SrVw/vv/8+zp49W+zIXzlSCFHKYXFERERE9MKx5oyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmf0Uvz6669QKBS4deuWRvqKFSugUCgwa9YsjfTk5GQolUosWLCg1O/h5+eHbt26lblspVkvPj4egYGBuHr1apm3X5To6GgoFAoEBQUVm8/FxQUKhaLQ44svvpDyLFiwAB07doS5uTkUCgXOnDnzXGWLjIzEwIEDYWZmBgsLCwwfPhyxsbElrieEwFdffYVatWpBX18fDRs2xKZNmwrlS0hIwNixY2FjYwMjIyP4+fkhLCxMI09gYKDW/VYoFBg/fryULygoSGue//znP+Xa97z1f/zxx0LLTp06JS3Pf4zLe+1VpBEjRmg9Dl26dJHybNq0CX379oWDgwMUCgWWLFnyXO+ZlZWFmTNnokaNGjAyMkK7du1w8eLFUq27e/duNGnSBPr6+nBycsLcuXOhVqs18pTmegoODi7yOqlXr16h9z127Bjat28PExMTmJub480338TNmzfLvO9+fn5QKBQYNGhQoWWZmZmwsrKqkGP8MhT1P2RgYCDlOXPmDEaOHIn69etDR0enQq73X3/9FfXq1YOBgQEaNmyILVu2lGq9a9eu4a233oKxsTEsLS3xzjvvIDo6WiPPvn374OvrC1tbW+jr66N27dqYPHkyEhISNPIV9X+zd+9ejXwBAQFwc3OT3rNt27bYv3//8x2AYihf2JaJ8nnzzTcBAKGhoXBzc5PSjx8/DiMjI4SGhmrkP3nyJNRqNVq3bl3q91ixYgV0dXUrpsAFxMfHY968eWjYsCEaNGjwQt6jOP369cOUKVM00mrWrCk9/+mnn+Dq6oqOHTti27Ztz/Ve2dnZ6NKlCzIzM7F27VpkZWVh+vTp6NmzJ44cOQKFQlHkuosXL8asWbPw6aefolWrVtixYwcGDx4MIyMjdO/eXco3ZMgQnD59Gl999RXs7Ozw9ddfo3379rhw4QKcnJwAAKNHj9YILADgyJEjmDFjBrp27Vrovffu3Qtzc3PptYODQ7mPgYmJCdavX68RBALA+vXrYWJiguTkZI30F3ntlUXt2rWxbt06jTQLCwvp+datW/Hvv/+ie/fu+Omnn577/SZNmoQ1a9Zg6dKlcHFxwVdffQV/f39cunQJ1atXL3K9kydPomfPnhg0aBAWLVqEq1evYtasWUhJSdEIZkpzPTVp0gQnTpzQ2H5iYiK6du1a6Do5cOAAunXrhjFjxuCTTz5BVlYWTp48ibS0tHLtv4mJCXbt2oXk5GSYmJhI6X/99ReysrLKtc3KVPB/SEfnWf1NaGgojh49iubNm5f7eOW3detWjBgxAv/5z3/QqVMn/PHHHxg4cCDMzc3RqVOnItdLTExE+/bt4eDggPXr1yM1NRUzZ87E22+/jRMnTkhljo2NRatWrfDxxx/D0tISly9fRmBgIC5fvlwoqNL2f1O/fn2N11lZWZg2bRpcXV2RlpaGX375BW+99RYOHz6MNm3aPPfxKEQQvSTVq1cXo0eP1khzcXER77//vjA0NBSZmZlSemBgoFAqlSI5OfmFl8vX11e8/fbbxea5e/euACC2bNlSYe/79OlTAUCsXr262HzOzs7igw8+KDaPWq0WQghx+PBhAUCcPn263OXauHGjACAuX74spYWGhgoAYs+ePUWul5GRIUxNTcXkyZM10t9++23h5eUlvT5x4oQAIHbu3CmlpaSkiGrVqomJEycWW7aAgABhaWkpMjIypLTVq1cLAOLp06el3sfiABBDhw4VCoVC3Lt3T0pXq9WiRo0aYtiwYc99jLVJTU19rvUDAgKEh4dHsXnyrhMhcvdz8eLF5X6/Bw8eCF1dXfH9999LaYmJicLa2lrMmDGj2HU7d+4smjRpopG2ePFioaenJyIjI4UQpb+etMm7Jk6dOiWlZWVliZo1a4qZM2eWav9K4uvrKzp37ixsbGzE2rVrNZb1799fuk6e5xiXVnZ2tsb9s6xK8z+U/9opzT2zJPXq1RP9+/fXSOvUqZNo3rx5sestWrRIGBoaiidPnkhpp0+fFgDE77//Xuy6K1euFADEw4cPpbTS/N9ok52dLZycnMSYMWPKvG5psFmTXppWrVpp1JA9evQI4eHhmDhxItRqNc6fPy8tCw0NRePGjWFsbAwAePDgAYYNGwYbGxsYGhqibdu2OHv2rMb2tTUtbd++HXXr1oWBgQHeeOMNnDlzBiYmJggMDCxUvi1btqBu3bowMTFB+/btcefOHQBAeHg4atWqBQDo37+/VO0dHh4OAMjIyMAnn3wCZ2dn6Ovro379+li/fn2h7a9atQouLi4wMjKCv78/bt++XfaDWIT833Cf119//QUvLy94eHhIaa1atYKLiwv+/PPPIte7c+cOkpKS0LlzZ430Ll264OLFi7h37x4A4Pz581AoFBrfjo2MjNCmTRvs2rWryO2np6dj+/bt6NevH1QqVXl3r1S8vb1Rv359bNy4UUo7dOgQYmNj0bdv30L5tV17165dQ58+fWBlZQUjIyM0atQIGzZskJbnNU3PmDED1atXh62tLYDc/ZwyZQocHBygr68PT09PrddTeVTkdbJ//36o1WqNZj1TU1N079692OsEyL0GtF0nWVlZ2LdvH4DSX0/arF+/Hm5ubmjWrJmUduDAAdy7dw8TJkwo9T6WRKlUon///hrnNSkpCbt378aQIUMK5T9x4gR69OgBe3t7GBsbw9vbG2vXri2ULz4+Hh9++CEcHR2hr6+PWrVqYebMmdLyvOvt119/Rd26daGvry91C1i5ciXq168PfX191KxZE59++imys7Ofe18r8tq5e/curl+/jsGDB2ukDxkyBKdOnSrURJnf+fPn4e3tjWrVqklpTZs2hbW1dbH3DwCwtrYGgAqp1dTV1YWFhcULqyFlcEYvzZtvvonr169LfZdCQ0Nhb2+PunXrokmTJlLglpOTg5MnT0pNmnFxcXjzzTcRFhaG5cuXY9u2bTA2Nkb79u0RFRVV5PudP38e/fv3R4MGDfD7779j5MiRGDRokNZ/prCwMCxZsgRffPEFgoKCcPPmTQwbNgwAUKNGDfz+++8AgM8//xwnTpzAiRMnUKNGDQDAgAED8NNPP2HKlCnYvXs3unTpgmHDhmHPnj3S9nfv3o2xY8eiXbt22L59O9q3b6+1r0pRhBDIzs6WHgX75pRGeHg4FAqF1sA0v2vXrhWq0geABg0a4Nq1a0Wul56eDgCFAid9fX1pu3n5dHR0CjUD6uvrIzw8vMgmk927dyMxMVHrhx4AeHh4QFdXF7Vr18aiRYvKdYzyGzx4sEZQtH79enTt2lWjmbAot27dQsuWLXHr1i18++232LlzJ0aOHFkooFi2bBlu376N//3vf/jtt98AAEOHDsWKFSswefJk7Ny5E02bNsXQoUO1fohrk/86Ke+HsouLC/z8/IrNc+3aNdjZ2cHKykojvUGDBrhx4wZycnKKXDc9Pb1U1wlQ8vVU0JMnT3Do0KFC18nJkydhbW2NU6dOwd3dHUqlEvXr19faJ7IshgwZgv3790sBxfbt22FiYoKOHTsWyhsREYHWrVvj559/xq5du9C3b1+MGjUKa9askfJkZGSgffv2WLduHaZNm4Y9e/YgMDCwUMBy5swZLF26FAsWLMBff/0FJycnLF++HOPGjUP79u2xc+dOjB8/Hl999RXGjRtXqn1Rq9Ua144QoszHI68PV3Hyzl3B+0yDBg0ghMD169eLXFfbtQPkXhfargm1Wo309HScO3cO8+fPR/fu3eHs7KyR586dO7CwsIBKpYKPjw/++OMPre+ddx+OiYnBkiVLcOvWLYwdO7bYfS23F1IfR6TFP//8o9Gc9fHHH4t+/foJIYSYPHmy6NOnjxBCiLCwMAFAbN26VQghxJw5c4S5ublGNXZ6erpwdHQU06ZNk9IKVrX3799f1KlTR6M6Pq/6fu7cuRrrGRsbi6ioKClt1apVAoC4f/++EKLoZs1Dhw4JAGLfvn0a6f379xfNmjWTXjdv3ly0adNGI8/MmTNL3awJQOOhq6urNW9xzZp5+5B/37WpU6eOGDduXKH0oUOHisaNGxe5XmJiotDR0RFffvmlRvq7774rAIj169cLIYTYvXu3ACD++ecfKY9arRZubm4CgHj06JHW7ffu3Vs4ODhonE8hhNi7d6+YN2+e2Lt3r9i3b5/44IMPhI6OTolNwUXB/zdF3blzRwAQV69eFenp6cLc3Fxs3rxZ6zEueO0NGTJE2NraioSEhGLfx8PDQ+Tk5EhpFy5cEAA0mgqFyG3ucXZ2LrbcAQEBha4TAOLo0aPF7qc2zs7OwtfXt9j3Gz16tKhbt26h9Lz/neL2vWnTpqJr164aaWvWrBEAxNixY4UQpb+eClq2bJkAIG7cuKGRPnbsWGFgYCCsrKzEihUrxN9//y01PRZ1jIqTd85zcnKEs7OzWLFihRAi91y9//77Qojij3FOTo7IysoSY8eOFS1btpTS85rejh8/Xux7q1Qq6f4kRG4zm42NTaGmws8//1woFApx586dIreXd18s+FiwYEGx+65N3nVYnN9++00AEI8fP9ZIv3XrlgAgduzYUeS6U6dOFVZWVhrdACIiIoRCoRDu7u6F8js4OEj706VLl0JdZb755hvx3XfficOHD4vt27eLTp06FdmFJe/aBiBMTEyKLefzYs0ZvTSNGzfW6PwfGhoq1Y7lb/LM+5u3bP/+/WjXrh2srKykb3S6urpo06YNTp8+XeT7nT59Gt26ddOoju/Zs6fWvN7e3lKzEgCp0/+DBw+K3af9+/fDysoK7du31/jG6e/vj/Pnz0OtVkOtVuPs2bPo3bu3xrr9+vUrdtv5DRgwAKdPn5Ye//zzT6nXzePi4gIhRIk1ZwC0fvMVQhT7jdjU1BTvvPMOvvzyS+zZswdxcXFYs2aN1OSTdx46deoENzc3jB8/HpcuXUJUVBSmTp2Kf//9VyNffgkJCfjrr78waNCgQss7d+6MOXPmoHPnzujUqRO+++47TJ48GT/++CMeP35c4r4WpXbt2mjRogXWr1+P3bt3QwihMaihOAcPHkS/fv1gZmZWbL6uXbtqHNOjR48CAAYOHKiRb/DgwYiIiMD9+/eL3Z6rq6vGdXL69Gl4e3uXqsz5hYeHIzg4uMR8RV0nRS3L88EHH2DPnj1YtmwZYmNjcezYMcyaNQu6urrS+S3t9VTQunXr4OPjA3d3d430nJwcpKenY8GCBXjvvffg7++PNWvWwMvLC59//nmJ+1oUhUIh1bJGRUXh4MGDGDp0qNa8cXFxmDhxIpydnaGnpwc9PT2sXLlSY7TowYMHUb9+fbRs2bLY9/Xy8oKjo6P0+vr164iOjtZ67QghCg260ubvv//WuHZGjRpV4joFBQUFlbrGreA1UpprZ+zYsUhKSsK4cePw8OFD3LlzByNGjICOjo7Wa+Kvv/5CaGgoVq5ciStXrqB79+4ateofffQRPvjgA/j5+aFXr17Ys2cPmjdvjjlz5hTaVq9evXD69Gns2bMH/fr1w4ABAzRaSCoSR2vSS6Onp4c33ngDoaGhSE1NRVhYGL777jsAucHZkydPcOfOHYSGhsLV1VUa7RUdHY2TJ09CT0+v0DZdXV2LfL/Hjx9rBFwAYGlpqXU7BZuq8qrN85pWihIdHY3Y2Fit28wrg1KpRHZ2tkYfCQCws7Mrdtv52draomnTpqXO/zwsLS0RFxdXKD0+Ph6WlpbFrvvf//4XkZGReOuttwAANjY2WLBgAaZOnSqdTz09PWzevBkDBw6El5cXAMDT0xMff/wxvv3220LNZEDuyK6MjIwiP/QKGjBgAJYsWYKwsDCp+bk8hgwZgmXLluHq1avo3bu3xtQCxYmJiYG9vX2J+QpeE3FxcVAqlVLfmDx5xy42NlYazaqNgYGBLK4TPT09qb+oNgEBAbh06RKmTp2Kjz/+GCqVCnPnzsU333yjMcqzNNdTfnfu3MGpU6fw3//+t9CyvOuqffv2UppCoZC6GjyPIUOG4Msvv8TixYvh5ORUZGA1YsQIHD9+HHPmzIGHhwfMzMzwww8/aDStPs+1A6DQccl/7ZSkUaNGsLGxKTHf88q7j8TFxWncB+Pj4zWWa+Pm5obVq1fjww8/lJr6+/Tpg7feegtJSUmF8ufdY1q1aoUmTZqgadOmUt9VbXR0dNC3b19Mnz4daWlpMDQ0lJbZ2NhIx6dLly6Ijo7GtGnTtI4ef14Mzuilat26NZYuXYpjx45BqVSicePGAHL7dbm4uCA0NBShoaEa/V2srKzQpUsXrXOe5fU/0aZGjRp4+vSpRlpcXFyFduC0srKCra0t/vrrL63Lq1WrBl1dXSiVykL94548eVJh5ahI9evXLzTnGABcvXq1xLmNrKyssHfvXjx69AixsbFwc3PDzp07oVKppHMN5NZUXr9+Hbdv34YQAm5ubpgwYQJ8fHy0Brrr169HvXr1NLZRnNJ+cy/JwIEDMWnSJNy9e7dM35Ctra3x6NGjEvMVrCHIqx2OjY3VCFIjIyOl5XJRv359REVFFSrr1atXUbdu3WI7kCsUCixduhRz5sxBREQEatasiaysLMyaNQstWrSQ8pX2esqzfv166OjoFKo9yiuvNkKI5+7s7unpCQ8PD/z3v//FjBkztNb8pKen488//8TSpUvx4YcfSukF++ZZW1uXaq44bdcOUPi+ItdrB8jte5Z/LrqrV68WOT9dfkOHDkX//v1x8+ZNWFpawsHBAR4eHujRo0ex63l7e0NXV7fEwVilvX/4+PjgwIEDpcpbVmzWpJeqdevWSE9Px7fffotmzZppfBC3atUKW7ZsQUREhDQvGgB06NABV69eRf369dG0aVONh6enZ5Hv1axZM+zevVvj5ldUR8+SFFWT1qFDBzx9+hQqlapQ2Zo2bQqVSgVdXV00adKk0LfzrVu3lqssL9pbb72FS5cuaXSuPXnyJMLDw/H222+Xahv29vZo2LAhlEolfvjhB2lC2/wUCgXc3Nzg7u6O6OhobNq0CWPGjCm0rcePHyM4OLjIgQDabNq0Cbq6uqUO5opSrVo1TJ06Ff3794e/v3+p1+vQoQO2bt2q9Zt8cfKu+82bN2ukb9q0Cc7OzsXWmr1snTp1go6OjkZZk5OTsWvXrlJfJ+bm5vDy8oKFhQWWL18OFxcXdOjQoVC+0lxPALBhwwb4+flprXnq3LkzlEol/v77bylNCIHDhw+jUaNGpSpvcWbMmIHu3btj+PDhWpdnZGRArVZrdGZPSkrCzp07NfJ16NAB165dw8mTJ8v0/nXr1oWtra3Wa0ehUGjcUytbrVq1UK9evUKDMTZs2IA33nijVLV3KpUKDRs2hIODAw4dOoSbN29ixIgRxa5z4sQJqNVq1K5du8g8OTk52Lp1Kzw8PDRqzbQJDQ0tdlvPgzVn9FK1atUKOjo6+OuvvzBjxoxCy/K+UeaffHby5MlYt24dfH198dFHH6FmzZp4+vQp/vnnH9jb22PSpEla32vmzJlo1qwZ+vbti7FjxyI8PBxLly6FSqUq8zfl6tWrw8LCAhs2bJBmK/fy8kLHjh3RvXt3dOnSBdOnT4eXlxdSUlJw5coV3L59Gz///DMAYNasWejZs6c0YvTMmTMVNj0CAISEhODp06e4cuUKgNxpH8LDw+Hi4iI1c0VERMDV1RVz5szR2p8iT9++feHl5YV+/fph0aJFyM7OxrRp0/Dmm29qTGswf/58zJ8/H3fu3JFGP61btw5paWmoU6cOHj16hJ9++gl3794tNMHjwoULUadOHdjZ2eHGjRv4/PPP4ePjo/XmunHjRuTk5BQZnHXu3Bn+/v5o2LAhAGDnzp1YuXIlPvroI40mnsDAQMybNw93796Fi4tLyQf1/+X/JYbSmjt3Lnbv3o0333wT06dPR40aNXD16lWkpqZi+vTpRa7n5eWFvn37YvLkyUhNTYWHhwc2b96MvXv3aozoK6+rV69q/MrFpUuXsHXrVhgbG2s0zdSpUwfOzs44ePBgkdtycHDA+PHjMWPGDCiVSjg7O0sTyH788cdSvpCQEPj7++N///ufFLicOnUKISEh8Pb2RlpaGnbu3Im1a9diz549GqN4S3s9Abmjs69du1ZosuY8NWrUwAcffICZM2dCCAF3d3cEBQXhypUrGr/SERwcjHbt2mH16tUlftjnN2zYMGmEtzbm5uZo1qwZvvjiC9ja2kKpVOKLL76Aubm5Rq36O++8gxUrVqBbt26YO3cuGjZsiIcPH+LIkSNYuXJlkdvX1dXFnDlz8OGHH8LW1hbdu3fHuXPnMHfuXIwcOVKaDqi8nj59ipCQEOl5cnKy9AXzrbfegpGREQBg1KhR+PXXX0scKTx//nwMHDhQmjx7x44d2L9/f6GZ+ZVKJQICAvDLL78AAFJSUhAYGIi2bdvCwMAAJ0+exKJFixAYGIi6detK6/Xp0wdNmzaFl5cXDA0NceHCBXz11Vfw8vJCr169AOTeE0eMGIHBgwfD1dUVcXFx+OGHH3DmzBmNybz//PNPrFmzBt26dYOTkxNiY2Px22+/4e+//9aYRqVCvbChBkRF8PT0LDQJqRBCnDt3TgAQ1tbWGiPYhBDi8ePHYtSoUaJGjRpCpVIJR0dH0a9fPxEaGirl0TaCaNu2bcLd3V3o6+sLHx8fcfToUaGrqyu++eabYtfLm9Tw8OHDUtrvv/8u6tevL/T19QUAcffuXSFE7mSZ8+bNE25ubkKlUglbW1vRrl07sWbNGo1t/vjjj8LJyUkYGBgIX19fcfz48QqbhNbX11fraKuAgAApT2lHawohxKNHj0T//v2FqampMDMzE8OGDRPR0dEaeebOnatxHIQQYu3ataJevXpCX19fWFtbi3feeUdjRFmeKVOmCEdHR6FSqYSzs7OYNWuWSEtL01qWpk2bijfeeKPIsk6cOFG4ubkJQ0NDoa+vLzw9PcWyZcsKXUNTp04V+vr6Ii4urth9RwkTh5ZmtKYQQly5ckX06NFDmJmZCSMjI+Ht7S02btxY4vukpaWJyZMnixo1agg9PT3h4eEhfvvtt2LLLETpJtPMO2cFHwVHgpZmtKYQudf+jBkzhJ2dnXRdh4WFaeTJO175r/Pz58+L5s2bCxMTE2FiYiL8/f21jk4s7fUkROnOb1ZWlvj000+Fvb29UKlUonHjxuKvv/7SyJM3mri4CZeFKN1ErAXP8a1bt0S7du2EkZGRcHJyEosXLxZz584VxsbGGuvFxsaK9957T1SvXl2oVCpRu3ZtMWvWrFK9948//ijq1q0r9PT0hKOjo5g1a5bIysoqtpylmYQ27zxqe+S/B5RmtGaeoKAg4e7uLlQqlWjQoIHYvHlzoTwF72Opqamic+fOwtraWujr64tGjRppvYcuWrRIeHt7C1NTU2FsbCw8PDzE7NmzNUYRx8TEiB49eggHBwehUqmEiYmJ8PPzE3v37tXY1rVr10TPnj2l68be3l506dJFBAcHl2o/y0MhRAV1ziB6Bfz999/o2LEjgoOD4evrW9nFoZeoTZs28PT0xIoVKyq7KCRjs2fPxvbt23Hp0qUS5+sielHYrEmvtffffx/+/v6wtrbGlStXsGDBAjRu3PjF/BYayVZmZiYuXLggTfRKVJTQ0FB88sknDMyoUrHmjF5rQ4YMQXBwMKKjo2Fubo4uXbpgyZIlZZrGgoiI6GVicEZEREQkI5xKg4iIiEhGGJwRERERyQiDMyIiIiIZ4WhNKpecnBw8evQIpqamHNVERERUAiEEkpKSYG9vX+JE6AzOqFwePXokq5+SISIiehXcv38fjo6OxeZhcEblYmpqCgCIOOcCMxO2jr8OersX/TulRET0fLKRhWP4S/r8LA6DMyqXvKZMMxMdmJkyOHsdKBV6JWciIqLy+f+Jy0rTFYifqkREREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhFlZReAqKLcumiIDcvs8O81QyTEKJGeqgNjMzVc6qajfd9YdB0SC4UCuHDcBNP71Sl2W3aOmVhz6qpGWmKsLrb+WA0n9pnjyX0VFDoCVnbZqO+TggmfP4CRSQ4A4GyICf742RYRNwyREKuLrEwdmFpko3aDNLw1LAZtuiW8sGNAmjzeSEa7XvGo55MKm+pZMDZTIzZKibvXDLH5+2q4etq4sotIZWRgpMbACVFo0z0Bdg6ZyEjTwbVzRti0vBounzKp7OJRGfBcFo01Z5UsKCgICoUCCoUCgYGBUrqfn5+UHh4eXmnle5Xcu2WA0D0WeByuj9QkXeSoFUiKU+LSSRMsm1YTK+fbl3pbhiZqjdf3b+vjvY51sek7O9y7ZYCMdB2kp+ri0V19HNxqhZREXSnvjfPGOHXQHE8eqJCeqgt1tgLx0Xo4d8QMn42thR2/2FTYPlPx/PvEo/uIGLh5psHSNhsqfYHqTllo2SkRX++4ja5DYiq7iFQG+oZqLNl+B0M+joKTawZUBgKmlmq84Z+Er7bdgW/PuMouIpUSz2XxGJzRa6OGcwYmfnkf/wu9il3/XsC6s1fQcUCstHzfBmsAQKNWydj3KKzQw6/Xs5tBl3wf2mo18NlYF0Q/VgEABk2MxNpTV7DzzgX8cvQa3pv/AAZGOVJ+14ZpmL48Ar/+cxW77l7A6uNX0bRdorR87/+Xg168nBzgyC5zzBxUGz1cG2JIkwY4sstcWj7yk8fQ0RGVWEIqi6GTnsDNMw0AELLTHAMaemDGgNpIT9WBri7w0ZcPYGqRXcmlpNLguSwegzN6bTRomoq334mBQ61MqAwEbGpkoc/YKGm5Ui+nyHWfPtLD0d0WAAAT82x0GfwsqPtnvznCrxsCANr3icXI/0SimmMW9A0FHF0z0Gt0NEwtntW0Ne+QCP++cajulAmVvoC9Sya6DY9+Vg5V0eWgivW/z2tg4TgXnDtiiow0XcRE6uG7mY7ScnMrNcytq+4HwKtFoPOgZ1+gfl5gj4RYJcKOmUoBt7FZDnx7xFdS+aj0eC5LwuDsBTly5Ah69uwJV1dXmJubQ6lUwsbGBh07dsQff/xR7u0KIfDzzz+jdevWMDc3h0qlgrOzM959913cvn1byrdv3z6pWXTatGlS+rBhw6BQKKBUKpGYmFubc/v2bSlv3759y102OcnJAaIe6uH3ldWktL5jnxaZf8cvNlBnKwAAXYfGwND4WQB17sizvg9KPYGPu7uhl5sn+tZviMCRtRB+3aDI7arVwMO7Kuxek9uUqaMj0Ht00eWgipWarFsoTd/w2blNT9VBYhy73r4KqtfMhIVNbiCdkqSDqIcqadnda4bS83pNUl962ahseC5LxrvSC3Lu3Dns3LlTIy0mJgZ///03/v77b6xfvx6DBw8u0zaFEBg4cCC2bNmikX7v3j2sXr0aW7Zswd9//43mzZujbdu20NfXR0ZGBkJCQqS8ec/VajWOHj2Kt99+G8HBwdLyDh06aH3vjIwMZGRkSK/zAjs5+qibG66fe9bRW1cpMPrTR+hTRHCWlqKDPetzmxqVejnoNSpaY/mTB89uHPs3aTZJnthnjrBjJvhm1y241EvXWDaokQfinupJrw2M1Ji05D78esWXa7+oIgiMmfNIevXnWispKCd5s7R9VsOZv48nkPsBry0fyRPPZclYc/aC+Pn54eDBg4iMjERGRgZSUlKwa9cuafmSJUvKvM2tW7dKgZmzszPOnj2L+Ph4zJgxAwCQnJyMUaNGAQAMDQ3RsmVLALmBYlJSEv799188ePAAOjq5pz0vKCtNcLZo0SKYm5tLDycnpzKXv7KosxX4KdABm7+vpnX53g1WSE7I/Z7i2zMeNjWyNJZnZz378DYwUmPZ7pv4/cZFdByQ2y8tLUUXa5dWL7Ec6am6+GqiMw79blHOPaHnodTLwfTl99C2e+5o2fNHTbB6UY1KLhWVh0JR9GvBLoSvFJ5L7RicvSCOjo7YtWsX/Pz8YGFhAWNjY3Tv3l1afvXq1WLW1m7Hjh3S88mTJ6NJkyYwNzfHZ599Bmvr3BqdK1eu4M6dOwCeBVpqtRqhoaFSrVm/fv0APAvK8tJr1qwJNzc3re89c+ZMJCQkSI/79++Xufwvy7Ldt/DX/TCsO3sF70x9LKX/+lV1xMdofktTq4E/fraVXvcdF4WC8vdJatI2CfWapMLYNAe9xzyribt10bDQehsvXMGfEWEIOnEV3QJya+PU2Qp8/6kjctjt7KUyMlHjs9/uwr9vPADgxD4zzAmohaxM3gJfFXFPnzX0GJtpjqY2NlVrzUfyxHNZMt6ZXoCcnBz4+/vjm2++wfXr15GWllYoT3p6upY1i/fkyRPpubOzs/RcqVTC0dGxUL78tWAhISFSEDZy5EjUrFkT58+fx7lz5/DgwYNC+QvS19eHmZmZxkPOdHUBmxpZGDb5ifTPn52lg8gIfY18x/eYI/JeblrjNklw9Sh8Xty9C58/AIB49hUvfz+m/JR6QA3nTIyY8SxITI5XIiGm6t50Xjbr6llYuv02GrdJBgDsXG2N+aNckJnO29+rJPKevvRhbWSSg2oOmdKy/F0Kbpw3eullo7LhuSwZ704vwKVLl3Dx4kUAgJ2dHS5duoTs7Ozn7qdlZ2cnPY+IiJCeq9VqKcDKn69p06awsLAAkFtLFhwcDF1dXbRu3Rq+vr5Qq9VYsGCBtF5xwdmr4Ic5Djj6pzki76uQlalAbJQS65fZSX0adHQFqtfM1Fhn20/5BgyML1xrBgB+PeOgb5AbfJ0/aorr542QkqSD7T8/m6/MxzdJer7k45o4ddAUTx/pIStTgagHevj1q2fNnibm2TCzrLp9KV4m57ppWLb7Fmp7pCMnB1i1oAa+n+WInBz2M3sV7dtoJT0fPfsRzKyy4f1mktRUnZKog5CdFpVUOioLnsvi8ev7C6BUPjusurq6MDExQUJCgtQ3rLx69OiBdevWAQC+/vprtG3bFrVr18YXX3yBmJjc/k8NGjSAq6ur9N5+fn74448/cOrUKeTk5KBZs2YwNTWFr68v1q5dKzWVKhQK+Pv7P1f5KtuJveYaTZQFDfrwiTRCCACunTXCtbO5Awec66ahWbskretZVcvGBwsf4OupTkhL0cVHb7trLLevlYHBE5/Vah7YbIUDm60KbgYAoFDkDk7Q5X/eS9Fv/FPY2uf2IdTRAcbMfowxsx9r5JnW1xUXT1Tt2chfFeu/qQYfvyS4eabBt0cCfHs8+7UNtRpYNsMRSfH853oV8FwWr+ru+QtUr149NGzYEJcvX8ajR49Qq1YtAIC7u3sJaxavf//+2LRpE37//XeEh4ejcePGGsuNjIywatUqjbQOHTrgjz/+QM7/d3Ly8/PT+Cv+v8elp6cnqlXT3mH+VfH2O9E4E2KKB3cMkBSXW1tmYZMN90ap6Dw4Fs07aNZcatSajSt+eovOg2Nh65CJLSuq4UaYMTLSFLB1yELLzgkY8tETjXnO+oyLwrUzxngcoUJygi50dAFruyzU90lBt+Ex8HgjpQL3mqjqyEjTxbQ+rhjwQRTado9HNccsZKTp4Po5I2xcXg2X/2GQ/arguSweg7MXQFdXF7t27cKkSZMQEhICtVqNjh07YtmyZRp9w8pKoVBgy5YtWLVqFYKCgnDlyhWkpaWhRo0aaN++PT755JNCAWDHjh01Xvv6+gIAXF1d4ejoWKr+Zq+KgR9GYeCH2psmtfl0ZXiZtt+kbTKatE0uMd+4uY9KzEMvx9JJNbF0Us3KLgZVoLQUXfz6VQ38+hVH2r7qeC6LphCiKg9WpfJKTEyEubk54m7Whpkpuy6+Djrbe1d2EYiIXlvZIgvB2IGEhIQSB9XxU5WIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREcmIsrILQK+23u6eUCr0KrsYVAEeTW9V2UWgCmL/1fHKLgIRPQfWnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlRVnYBiORu4bp/0bRdkvR6dNu6uH/boBJLVDUoddT4sNUpNLSLQoNqT2GinwUAOP3AHu9u7Vnkeo5midj2ziYY6WUDAC48roZhm/pq5HGxjMPoZufR1PERbI1SAABRKcY4cc8JP/3jgyfJJhr5G1SLwpg3zqGJ/WOY6mciOsUIIXed8eM/TRGTalSRu00lqOmWjv7vR8H7zWRY2GQjI00HUQ/1cGKfOdYuqV7ZxaMyMDBSY+CEKLTpngA7h0xkpOng2jkjbFpeDZdPmZS8gdcYgzOiYnQaGKsRmNHLY6iXjXebhpVxLYF5HQ9LgZk29maJWD/od5jqZ2qkO5onob/nVfjVDkeftQMQn24IAGhbKxzfdNsHPd0cKW8Ns2QManQFvrUi8M7m3oWCOXoxWneNx3++vweVgZDSVPpqmFqoYWiUw+DsFaJvqMaS7Xfg5pkmpakM1HjDPwk+fkn4ckJNhOywrMQSVi42a1aioKAgKBQKKBQKBAYGVnZxqAAruyyMnfsIajWQkaao7OJUOdlqHWy84IHZ+9th0eE3S7XOQK8reMPpEVIzi/7e2cX9jhSY3XhqDf9V76DDz+/gTkzuB4GtcSo6uv0LANBR5GCO/xEpMPtoV2e88f1o/PSPD4DcIG26b2i595FKz84pA9OX34fKQCA5QQeLP3LCgIYe6OXWEBO6uOH3VbaVXUQqg6GTnkiBWchOcwxo6IEZA2ojPVUHurrAR18+gKlF0V+yXncMzoiKMPHLBzC1UOP3lbaIi2Yl88uWlq2HhYfb4o+r9RARb15i/hqmSZj05klkqXWw/MQbReZT5zwLtEMjnBCVYoInySY4ec9RSjdU5n4o1LGOg51JbrPnv7EWOHSnNtKy9BB0tpGUt73rXZgbpJd5/6hseo+OhoFRbpC8aoE9/t5ihYRYJdJSdHHrohF2BdlUcgmp9AQ6D4qTXv28wB4JsUqEHTPFkV25/+vGZjnw7RFfSeWrfAzOiLRo1zsOLTsl4v4dfaxZzKaSV0Fgh2AYq7Lwy+nGuP606A/qP6+7ITolt8mytfN9VDNOhp1JMlo63weQG7yd+P9AzUCZVeL7KnUE6ttGV8AeUHGatH3WvcDeJQMrg69j178Xse7sFUz4vGrXsrxqqtfMhIVN7vlKSdJB1EOVtOzuNUPpeb0mqS+9bHLB4Owl2b59O7y9vWFgYAAXFxd89tlnUKvVWvNeuHABQ4cOhaOjI1QqFaysrNClSxccPHhQI1/+ZtG5c+di2bJlqFu3LgwNDeHh4YF169ZJed99910p7549ezS2M3HiRGnZjh07Kn7nXzEWNll4b8FDqNXAfyc7ITOd/yZy18fjKlo5P8DNp1b46ZRPsXmjU40xaEM/XH1ig7q2MTg4Zi3+Hr0Wta3i8SDBFJP/7IxbMdYAgDuxVsjI1gUA1LaKR3vXf2GozMIInwsa27Q0Siv0PlSx7JyeBcoDJzyFs3sGVAYCNjWy0X1EDJZuvw19w5xitkByYWn7LJBOSdTVWJaSpKM1X1XDtpqXYNu2bejfvz+EyO3EGhERgdmzZ8PBwaFQ3p07d6Jfv37Iynp2I4qLi8O+ffuwf/9+rFixAuPHjy+03vLlyxEX96ya+OrVqxg2bBhq1aqFVq1aYerUqQgKCoIQAsuXL0fXrl0BANnZ2di0aRMAwNHREd26ddO6DxkZGcjIyJBeJyYmluNIvBomfP4Q5lZq/L7SBldPG1d2cagE1YyTMaXtCWSpdTD7QHtk5+gWm9/aKBXf9/wLdW1jCi2zMEhHU8eHCA13QoZaiZRMFVadaoIJrU4DAJZ136d1m1lqBvAvmq7y2SCA25cNMDegFgBg/pq7cPVIh3PdDHQeFIudq9m8+SpRKIp+LQSqLN5RXjAhBKZMmSIFZoGBgUhISMDx48c1gh0ASEtLw+jRo5GVlQUXFxecPn0aGRkZuHHjBurWrQshBCZPnozo6MJNKImJidiwYQMSEhIwffp0KX3NmjUAgAYNGuCtt94CAOzduxd37twBABw4cABRUVEAgNGjR0NXV/sH26JFi2Bubi49nJycnvPIyJObVyradEtAUrwuQveYw80rFW5eqdDTe3aXcHZPR0039jGSi9HNzsNMPxOH/3UBINCgWhRqWcZLy430stGgWhSsDHObSEb6hEmB2eE7zvBbGYAWK0ZhfVhDmOhn4Z3GlzC5zQlp/Z9ONcXs/X64/tQamdk6eJpihF3X3HHjqbWU51Gi6cvY1SotIebZvenAJitEP1Yh+rEK+zdaSeluXlW3GexVEvf0Wb2QsZlmC5KxqVprvqqGwdkLdvPmTURERAAAbG1tMXv2bJiZmaFly5YYM2aMRt7Q0FA8ffoUABAeHo5mzZpBX18fdevWxY0bNwDkBnAhISGF3qdHjx4YNGgQzMzM8M4770jp4eHh0vNp06YByA0Yv//+ewDAb7/9BgDQ1dXF6NGji9yPmTNnIiEhQXrcv3+/rIfilWBonNssYmqhxtLtd/Dd3lv4bu8tWFd/Vr0+++cI/GdFRGUVkQowUuXWMndy+xebhmzDpiHbMMf/iLTczSYWm4Zsw1v1bgEAXK1jpWU7r9VFTKoRUjJV2HKpgZTe2lnz+v7jan30XzcAPt+NQ/tVAfgqpBXszXL7QMWkGuBGMX3cqGLcvKB9Prn8NS3pafxIexVE3tOXAi8jkxxUc3g2rY1LvWdffG+cr7pzCPJKfsHy13I5ODhAR+fZIXd2dtbI++TJkzJvM0/9+vWl58bGz5ri0tOfXei+vr5o1qwZAGD16tWIioqS+ph1795dazNrHn19fZiZmWk8iF40C4M0WBikwUT17Oat1MmR0kvTYb+gyKRnc5L1qH8D1kapMFZlor/nVSk9MUNfeu5d4zHauETA0jAN+rrZ8Kz+BMt77JGm4wg66w214K30RduXr4as48BY2NTIhE2NTHQY8CzYPhfCGsxXRf7zOXr2I5hZZcP7zSS07Z4AAEhJ1EHITotKKl3lq7p1hi+Jjc2zb9QPHz5ETk6OFKDl1ajlsbOzk5537twZe/fuLbQ9IQQUBRvpAejp6UnPtS3PM3XqVAwcOBDx8fEYNGgQUlJypwl47733SrlHr7eLJ0zQ2b5RofRf/7mK6v/fIZm/EPDyHB0fVCitsX2klL7iZFN8ur89Pt3fXiNPU8eHWN1vJ4DCvxDw23kvdHG/DRP9LLRzjUA711811s0RwK/5pspoWD0KM3yPay3fnht18OtZ73LsGZXVyf3m2L/JEp0GxqFOw3SsO3tNY/mxv8xxYh+/NL4q1n9TDT5+SXDzTINvjwT49kiQlqnVwLIZjkiKr7ohCr/uvWDu7u5SDdnTp0+xYMECJCYm4uTJk1i1apVG3tatW8PWNncixf3792PJkiWIiYlBRkYGrl+/ji+//BJ16tR5rvL07dsXtWrldqQ9fPgwAMDV1RUdO3Z8ru0SvSruxFph8Ma+2H6lHh4kmCIzWwdZah1EJRvh8B0XjNnWHftuPfs/ux5li5P3HBCdYogstQ4S01U4dd8eM/f6Y/qeDhDgBMUvy38nO2H5fxxw66Ih0tMUSE9T4NYlQ6yYbY+FY50BnotXRkaaLqb1ccX6b6rhwR0VMjMUSIrXxelDppjez7VK/zoAwJqzF06hUGDJkiUYMGAAhBAIDAyUfg0gLxDLY2hoiF9++QX9+vVDZmYmpk2bJvUTqyi6urqYPHkyPvzwQylt7Nixxda2ERDQvEHJmajCeX5TvhrdMw8cil03PM4Scw60K922HtrjzO89ylUOqlhCKLB7jQ12r2Efv9dBWooufv2qBn79qkZlF0V2WHP2EvTr1w/btm2Dl5cXVCoVnJycMHv2bCxcuLBQ3u7du+Ps2bMYPnw4atasCT09PZibm6N+/foYPny4NO3F83j33XdhZZXb3q9SqTBy5Mjn3iYRERFVDIUQVXkmkarpwYMHqFevHlJSUjB8+HD8+uuvJa9UQGJiIszNzeGHnlAq9EpegWTv0fRWlV0EqiD2X2nvI0dElSdbZCEYO5CQkFDioDrWnFUh27dvh7u7O9zc3JCSkgJDQ0N8+umnlV0sIiIiyofBWRWSkJCAW7duQQiBJk2aYNeuXXBzc6vsYhEREVE+HBBQhYwYMQIjRoyo7GIQERFRMUoVnB05cqTkTEVo27ZtudclIiIiqmpKFZz5+fmVe6oFtVpdciYiIiIiAlDK4GzOnDmcB4uIiIjoJShVcJY3aSoRERERvVgcrUlEREQkI881WjMyMhK///47rl+/jpSUFPzyyy8Acn9D8u7du/D09IShoWGFFJSIiIioKih3cLZixQpMmTIFGRkZAHJ/QzIvOIuKikLLli3x448/YsyYMRVTUiIiIqIqoFzNmrt27cKECRPg6emJnTt34r33NH9g2MPDA15eXvjjjz8qooxEREREVUa5as4WL16MmjVr4vDhwzA2NsbZs2cL5fH09MTRo0efu4BEREREVUm5as7CwsLw9ttvw9jYuMg8Dg4OePLkSbkLRkRERFQVlSs4y8nJgZ6eXrF5nj59Cn19/XIVioiIiKiqKldwVrduXRw7dqzI5dnZ2QgJCYGnp2e5C0ZERERUFZUrOBs6dCjOnTuHzz77rNAytVqNqVOn4t9//8Xw4cOfu4BEREREVUm5BgR8+OGH2LVrF+bOnYu1a9dKzZcDBgzAmTNnEB4ejk6dOmHUqFEVWlgiIiKi1125as709PSwb98+/Oc//0F0dDQuX74MIQS2bt2K2NhYzJgxAzt37uTvcRIRERGVUbknoVWpVFi4cCE+++wz3LhxA7GxsTAzM0P9+vWhq6tbkWUkIiIiqjKe6+ebgNxfBqhXr15FlIWIiIioynvu4Oz48eMICwtDQkICzM3N4e3tjVatWlVE2YiIiIiqnHIHZ0eOHMGYMWNw+/ZtAIAQQupj5ubmhlWrVqFNmzYVU0oiIiKiKqJcwdmJEyfQqVMnZGVl4a233kKbNm1gZ2eHJ0+e4MiRI9izZw86deqEw4cPo0WLFhVdZiIiIqLXVrmCs08++QQKhQLBwcGFasemT5+OkJAQdO7cGZ988gkOHTpUIQUlIiIiqgrKNZXG6dOnMXDgwCKbLX19fTFw4ECcOnXquQpHREREVNWUKzgzMDCAg4NDsXkcHBxgYGBQrkIRERERVVXlCs78/f1LbK48dOgQOnToUK5CEREREVVV5QrOli5dikePHmHkyJF4+PChxrKHDx9ixIgRiIyMxJIlSyqkkERERERVRakGBLRv375QmpWVFdasWYN169bB2dkZ1apVQ1RUFCIiIqBWq+Hl5YWAgAAcPHiwwgtNRERE9LoqVXAWHBxc5LLs7GzcuXMHd+7c0Ui/cOECf1uTiIiIqIxKFZzl5OS86HIQEREREcrZ54yIiIiIXgwGZ0REREQy8lw/fP7gwQMcPnwYjx49QkZGRqHlCoUCs2fPfp63ICIiIqpSyh2cTZs2DcuWLYNarZbS8v/4ed5zBmdEREREpVeuZs1Vq1Zh6dKlaNeuHbZu3QohBAICArBhwwaMHz8eSqUS/fr14+9qEhEREZVRuWrOVq5cCRcXF+zZswc6OrnxnYuLCwYOHIiBAwdiwIAB6NixIwYMGFChhSUiIiJ63ZWr5uz69evo0qWLFJgBufOd5fH19cXbb7/NXwggIiIiKqNyj9a0sLCQnhsbGyMmJkZjed26dXHlypVyF4yIiIioKipXcObg4IAHDx5Ir11dXfHPP/9o5Ll8+TKMjY2fr3REREREVUy5grPWrVvj5MmT0uuePXvi/PnzGD9+PP7880/MnDkTe/bsQdu2bSusoERERERVQbkGBLzzzjt49OgRIiIi4OzsjGnTpmH37t1YuXIlVq1aBSEEXFxcsHjx4oouLxEREdFrrVzBmZ+fH/z8/KTXJiYmOHnyJHbs2IE7d+7A2dkZ3bt3Z7MmERERURk91y8E5Kenp4d+/fpJr8PCwnDv3j306NGjot6CiIiI6LX3wn5bc9myZejdu/eL2jwRERHRa4k/fE5EREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpKRChutSUSvNvuvjld2EaiCPB3fsrKLQBXI9scTlV0EeslKHZxt3ry5TBu+e/dumQtDREREVNWVOjgbNGgQFApFqTcshChTfiIiIiIqQ3A2Z84cBltEREREL1ipg7PAwMAXWAwiIiIiAjhak4iIiEhWGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSkef6bc3MzEz8/fffuH79OlJSUjB79mwAQHp6OhITE2FjYwMdHcZ/RERERKVV7shp586dqFmzJrp3746pU6dqTFJ78eJF1KhRAxs3bqyIMhIRERFVGeUKzkJDQ9GvXz/o6+tj2bJlGDJkiMbyN954A3Xq1MG2bdsqpJBEREREVUW5mjU/++wzWFhY4MyZM7C1tUVMTEyhPD4+Pjh16tRzF5CIiIioKilXzdnJkyfRs2dP2NraFpnHyckJkZGR5S4YERERUVVUruAsIyMD5ubmxeZJSEjgYAAiIiKiMipX9FS7dm2cOXOm2DwnTpxAvXr1ylUoIiIioqqqXMFZ3759cfToUaxZs0br8iVLluDy5csYOHDgcxWOiIiIqKop14CAadOmYdu2bRg5ciR+++03pKenAwCmT5+OEydO4Pjx4/D29saECRMqtLBEREREr7tyBWcmJiY4evQoJkyYgM2bN0OtVgPIrTFTKBQYMGAAVqxYAX19/QotLBEREdHrrty/EGBpaYl169bh22+/xenTpxEbGwszMzM0a9YMdnZ2FVlGIiIioirjuX6+CQCsra3RpUuXiigLERERUZXHuS6IiIiIZKRcNWft27cvVT6FQoGDBw+W5y2IiIiIqqRyBWfBwcHFLlcoFBBCQKFQlGfzRERERFVWuZo1c3JytD7i4+Nx6NAhNG/eHH379kVmZmZFl5eIiIjotVahfc7MzMzg5+eHffv24fTp01i4cGFFbp6IiIjotfdCBgSYmpqia9euWL169YvYPBEREdFr64WN1tTR0cHjx49f1OaJiIiIXksvJDj7999/sWXLFjg7O7+IzRMRERG9tso1WvPdd9/Vmp6dnY2HDx/i2LFjyMrKQmBg4POUjYiIiKjKKVdwFhQUVOxyd3d3TJ48GWPHji3P5omIiIiqrHIFZ3fv3tWarqOjAwsLC5iamj5XoYiIiIiqqnIFZwqFAiqVCtWrV6/o8hARERFVaeUaEFCrVi3MmjWrostCREREVOWVKzizsrKClZVVRZeFiIiIqMorV3DWpk0bnDx5sqLLQkRERFTllSs4W7RoES5fvox58+YhOzu7ostEREREVGWVa0DAl19+iYYNG2L+/PlYuXIlGjVqBDs7OygUCo18CoUCv/zyS4UUlIiIiKgqKHVwpquri8DAQMyePVtjnrPHjx8X+TNNDM6IiIiIyqbUwZkQAkIIAEXPc0ZEREREz6dczZr8zUwiIiKiF+OF/PA5EREREZVPmYKzgh3+iYiIiKhilalZ8+uvv8bq1atLnV+hUODOnTtlLhQRERFRVVWm4Cw+Ph7x8fEvqChEREREVKZmzcDAQOTk5JTpQURERESlxwEBRERERDLC4IyIiIhIRhicEREREclIuSahJXrdGRipMXBCFNp0T4CdQyYy0nRw7ZwRNi2vhsunTCq7eFRKHm8ko12veNTzSYVN9SwYm6kRG6XE3WuG2Px9NVw9bVzZRawylDpqvN/uNDwcolC/xlOY6GcBAM6E18DYNT018g5sdglNXR7BrVoMLI3ToaMQeJxgguN3auLXUG/EpRpq5He2jsfI1ufQ1OURbExSAQBRScb4519H/HzUB08Sn/3PNq99H0OaX4KrbSwsjNKhp5uDxDR93Hxijd/P1cfBa64v+EhQHt5ni1bq4Iyd+1+soKAgjBw5EgAwd+5cBAYGVm6BqjB9QzWWbL8DN880KU1loMYb/knw8UvClxNqImSHZSWWkErLv0883h4eo5FW3SkL1Z2y0LJTIr6Z6og9660rqXRVi4FeNka0DitV3o87noS+Uq2RVqdaHOpUi0PXhrfwzs99EJWU++FdwzwRa0b9DlODTI38jpZJcPS5hrbuERj4Y3/Ep+UGdA0dotDG7Z5GXmuTNLQ0eYCWrg/w5Z5UbDrtWc69pNLifbZ4bNYkKmDopCfSDSNkpzkGNPTAjAG1kZ6qA11d4KMvH8DUIruSS0mlkZMDHNlljpmDaqOHa0MMadIAR3aZS8tHfvIYOjqiEktYdWTn6GDz6QYI3OGHL/e0LjZvQqo+fgrxQd8VA9Fi4WiMCuqJp0lGAABb01QMb3lBytvJ444UmN2MtEbnr4eh6zfD8O9TSym/f4N/pfw3Im3w6fb2eHvZEDRfOAY9lg9G6G0naXmvxtcrapepGLzPFo/B2SsoLS2t5ExUTgKdB8VJr35eYI+EWCXCjplKH+rGZjnw7RFfSeWjsvjf5zWwcJwLzh0xRUaaLmIi9fDdTEdpubmVGubWVfcD4GVKz9LDF3vaYueFergfa15s3r4rBuGnkGa4G22JTLUS5+/VwLqTXtJyF5t46XmOePbLNcfvOOFpkgmeJJrgn38dpHQDvWfn+NgtZ/x1yR2PE8yQpdbFgzhzbDnjIS3PUvNj8cXjfbYkvAorwfbt2+Ht7Q0DAwO4uLjgs88+g1qtLpTPz88PCoUCCoUCoaGhGDx4MKysrGBkZCTliYyMxMcffww3NzcYGBjAxMQETZo0weLFi5GZqVnNn7ctFxcXnDlzBv7+/jA2NoaVlRVGjhyJmJiYgkWocqrXzISFTe6NPCVJB1EPVdKyu9ee9XOp1yT1pZeNyi41WbdQmr7hsy4a6ak6SIxj11u5SclUFUrLH2BFJjzrj7Tnkhuik3P/N1u53oetaTKqmSajRe0HAAB1jgL//OsIbXQUOXCyTED/plekvOv/8dKalyoO77Ml413pJdu2bRv69+8PIXKbUiIiIjB79mw4ODgUu16vXr0QHR2tkXbnzh20bt0aT548kdIyMjJw/vx5nD9/Hjt37sSBAwdgYGCgsV50dDR8fX2Rmpp74aempiIoKAhhYWE4efIk9PX1C71/RkYGMjIypNeJiYll2/FXhKXtsw+AlETND/aUJB2t+ehVIjBmziPp1Z9rraDO5m8Gy10N80QMeuMSACA7R4GtZxtIy6KTjfHOz33x3wF7Ud8+Gvsm/SYtexBniv/ub4XbUYX7Fe6f/CtsTJ61QqRmKjF/lx/2X6nzAveEAN5nS4M1Zy+REAJTpkyRArPAwEAkJCTg+PHjGoGPNoaGhggJCUFqairOnz8PAJg4caIUmA0fPhzR0dG4efMmGjVqBAA4duwYli9fXmhbKSkpGDZsGGJiYnD58mW4ubkBAMLCwhAUFKT1/RctWgRzc3Pp4eTkpDXf60ShKPq1YDelV45SLwfTl99D2+4JAIDzR02welGNSi4VlaSWTRxWBeyEhVEGcgSw6M82uB5pKy23Mk7FskF7UN8+utC6FkbpaOr8CPrKkj/kjVTZWNDrELo0vFWh5afi8T6rHYOzl+jmzZuIiIgAANja2mL27NkwMzNDy5YtMWbMmGLXXbhwIdq2bQtDQ0N4e3sjLS0N+/fvB5DbXLls2TJYW1vDzc1NY6Tnzp07C21LqVTiv//9L6ysrODh4YGpU6dKy/K2WdDMmTORkJAgPe7fv1/W3X8lxD19VplsbKbZ1Gxsqtaaj+TPyESNz367C/++8QCAE/vMMCegFrIyeQuUs8Y1H+N/I/+AvUUystQ6CNzRDtvPN9DIE9AqDO7Vc7tkhNxwRselw9Hmi3ex8VRDmOhnYUiLS/iow8lC2+703wC88dkYdP92CDafzt2mnm4OZnQ9BgWqcFTwEvA+WzLemV6i/M2SDg4O0NF5dvidnZ2LXdfHx0fjdWxsLLKzc78Nmpubw8LCQlrm4uIiPc/f5JnHxsYGxsbP5nfK/95RUVFa319fXx9mZmYaj9dR5D196YZgZJKDag7P+u251EuXnt84b1RoXZIn6+pZWLr9Nhq3SQYA7FxtjfmjXJCZztufnHWofwcrhu2GuWEGktJVmLihK3ZfrFson6vts47luy7URUyKEVIyVdiWr+mzleu9QusBQHaOLh7Gm+H7w82lNHPDDFgac9DVi8T7bMl4d3qJbGxspOcPHz7UmDsur0atKPkHAQCAlZUVlMrcizuvNitPeHi49NzOzq7QtqKjo5GSkqL1vatVq1bCXrz+9m20kp6Pnv0IZlbZ8H4zSWoOS0nUQchOi0oqHZWFc900LNt9C7U90pGTA6xaUAPfz3JETg77mVUGC8M0WBimwVj/2YexUjdHSjdQ5k5MO7T5BXzR7wD0lWo8STTG6KCe+Odf7V0pIhOffdHs3ugGrI1TYazKRF+fq1J6YvqzfrSBPQ6hdZ0IVDNNhlJHjepmSXjf79SzvGkqJKRq9tOlisf7bPGqbp1hJXB3d4ezszMiIiLw9OlTLFiwAJMmTcLVq1exatWqMm3L0NAQHTt2xJ49eyCEwKRJk7B48WLEx8dj/vz5Ur4ePXoUWjc7OxtTp07F559/jsePH2PJkiXSsk6dOpV/B18T67+pBh+/JLh5psG3RwJ8ezwLfNVqYNkMRyTF81/nVdBv/FPY2ud+4OvoAGNmP8aY2Y818kzr64qLJ6r2bOQvy6FpvxZK83Z6IqX/FOKDn0KaYUrnE9JyO7MUbBq/RWOdR/Em6PbtMADA+n+80MnjDkz0s+BbNwK+dddo5M0RwNoTjaTXPbxvoof3Ta3lyxHAN3+3hFqw3uJF4322eLwCXyKFQoElS5ZA8f89HgMDA2Fubo6WLVtKaWWxbNky2NrmdoxdvXo1bGxsUKdOHWnAQMuWLfHhhx8WWs/ExATr16+X+pzdupXbAdbb2xsjRowo5969PjLSdDGtjyvWf1MND+6okJmhQFK8Lk4fMsX0fq5VetZqIrn596kV3vm5L3acr4sHcabIzNZBlloHT5OMEHzDBe+t7Y4DV5+NwFx7wgsXH9ghJtkQWWodpGfp4n6sGf666IZRQT3xx/n6lbg3VQfvs8VTCFGVx0NUju3btyMwMBDXr1+HnZ0dRowYAScnJ4wdOxbAs59v8vPzQ0hICADg7t27Gn3J8jx69AiLFi3Cnj17cP/+fSiVSri7u2PgwIGYNGmSxrQYeQGgs7Mzdu7ciSlTpuD48eNQqVTo2bMnlixZotH0WpzExESYm5vDDz2hVOg95xEhoor0dHzLyi4CVSDbH0+UnIlkL1tkIRg7kJCQUGK/bQZnVUj+4Cx/v7TyYHBGJF8Mzl4vDM5eD2UJztisSURERCQjDM6IiIiIZKTqDoWogtiCTUREJH+sOSMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkRFnZBSAioopl++OJyi4CET0H1pwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZUVZ2AYjkxOONZLTrFY96PqmwqZ4FYzM1YqOUuHvNEJu/r4arp40ru4hURgZGagycEIU23RNg55CJjDQdXDtnhE3Lq+HyKZPKLh6VEc/n66GOZyoGfxSF2vXTYG6dDQOjHKQk6iL8hgEObbPEnvVWABSVXcxKoxBCiMouBL16EhMTYW5uDj/0hFKhV9nFqTATv3iAt4fHFLn8m6mO2LPe+iWWiJ6HvqEaS/+4AzfPtELL1Grgywk1EbLDshJKRuXB8/n6aN8nDjO+u1fk8m0/2mLlfPuXWKIXL1tkIRg7kJCQADMzs2LzslmzjMLCwhAYGIjAwEAEBweXaV0XFxcoFAooFBX/bSA+Pl4qV1BQUKHlgYGB0ntrW065cnKAI7vMMXNQbfRwbYghTRrgyC5zafnITx5DR4ffZ14VQyc9kT7IQ3aaY0BDD8wYUBvpqTrQ1QU++vIBTC2yK7mUVFo8n6+PxxEqLJvuiJGt6qFbLU8MadIA+zc9C6w7Dy76S3JVwGbNMgoLC8O8efOk135+fpVXmHzi4+Olcvn6+mLEiBGVW6BX1P8+r4HUZF3pdUaaLr6b6Yi23RMAAOZWaphbZyPu6etTW/j6Eug8KE569fMCeyTEKhF2zBRHdpmj08A4GJvlwLdHPHavsanEclLp8Hy+Tq6dNca1s8+6icRE6uD3lbboNDD3HGdnVe26o1dm77OyspCdzW9E9GLlD8zy6BvmSM/TU3WQGMfvNK+C6jUzYWGTe89ISdJB1EOVtOzuNUPpeb0mqS+9bFR2PJ+vL4VCwNY+E33GPpXStq2s2gG27IKzESNGSM1vf/zxB8aPHw87Ozvo6+vjwYMHSE5Oxrx58+Dl5QVjY2MYGhrC09MTX3zxBTIzMzW2tXv3bvj6+sLS0hJKpRLW1tbw9vbGqFGjEBf37BuYEAJBQUFo27YtLCwsoFKp4OLigg8++ACRkZFSPhcXF4wcOVJ6PW/ePKmsgYGBZdrPiIgI9OvXD+bm5jA1NUWfPn0QHh5eKN+2bdvQoUMHWFlZQaVSwd7eHgMHDsS5c+c0jlmtWrWk1yEhIVK5tNXsqdVqfPHFF6hduzaMjIzg4+ODAwcOlKn8VYfAmDmPpFd/rrWCOrvqdlJ9lVjaPvsyl5KoGXSnJOlozUfyxfP5evpm1y3sfXgRv525hk4D45CdBfw41x6bv7Or7KJVKllXAYwZMwbR0dHS69jYWLz99tu4evWqRr7Lly9j5syZ2LNnDw4cOACVSoWzZ8+id+/eGrVtsbGxiI2NxYULFzBz5kxYWlpCCIEhQ4Zg48aNGtuMiIjAihUrsH37dhw/fhwuLi4Vum+tW7fGw4cPpdfbt2/H6dOnERYWBmvr3A7nU6dOxdKlSzXWe/z4MTZv3ozt27dj8+bN6NWrV5nf+9NPP9UIOs+dO4du3brhxo0bRe5nRkYGMjIypNeJiYllft9XjVIvB5P/e19q0jx/1ASrF9Wo5FJReRTs5pn/NYdEvXp4Pl9fSj1g/LxHUOoJbFlRrbKLU2lkV3OWX3Z2Nnbu3Ink5GRcv34dP/zwgxSYfffdd0hMTER8fDwmTpwIADhy5AhWrVoFILf2KC8w27RpEzIzMxEVFYXjx49jzpw5MDHJHXL9+++/S4HZiBEj8PjxY6Snp2P9+vUAcoOhadOmAQDCw8OxevVqqXxz586FEAJCiDLXnDVu3BiRkZEIDw9HixYtAAAPHjzAkiVLAACnT5+WAjMLCwscOnQIiYmJWL58OYDcZt4xY8YgLS0NQUFBuHv3rrRtX19fqVzaBi2kpKRg//79iI+Px5AhQwAAmZmZhQLU/BYtWgRzc3Pp4eTkVKb9fdUYmajx2W934d83HgBwYp8Z5gTUQlamrP9lKJ+4p8++exqbqTWWGZuqteYj+eL5fD193N0NXR29MKRJA6xZ/Ky2LGB6JMytqm4tqKw/aSZPnozu3bvD2NgYdevWxZ49e6RlEyZMgJmZGSwsLPDtt99K6Xv37gUAuLq6Smnff/89vvzySwQHB8PGxgbz5s1D9erVAeTWWOUJCgpCjRo1YGBgIAUt+bdZkZYuXQo7Ozs4OztrDDDYv38/AGDHjh1S2siRI9GuXTuYmppiwoQJaNSoEQAgOjoax48fL/N7jx49Gh07doS5uTkGDx4spWtrVs0zc+ZMJCQkSI/79++X+X1fFdbVs7B0+200bpMMANi52hrzR7kgM13W/y5UQOQ9femD2sgkB9UcnnV7cKmXLj2/cd7opZeNyo7n8/WVk6NATKQe1n1dHckJufdZPZVAdeeMEtZ8fcn608bHx0fj9ZMnT0pcJ68ZtGfPnpgyZQqMjIxw5MgRzJ49GwMGDIC7uzt8fHzw6NGjUm8zOTlZo0mvKMHBwVJfr7xHUc2Ezs7OWp9HRUUVKlf+5QA0tlma8hdUv3596bmx8bPRMunp6dqyAwD09fVhZmam8XgdOddNw7Ldt1DbIx05OcCqBTXw/SxH5OSwn9mraN9GK+n56NmPYGaVDe83k6Sm6pREHYTstKik0lFZ8Xy+PsbPe4g334qHnWMmlHo5sLTNwuCJT2BinjsAS50NRN5TlbCV15es63+NjDS/AdnZ2eHhw4dQKBR4+PAhatQo3P8n/5y6S5YswcKFC3Hp0iXcvXsXISEh+P7773Hu3DnMnz8fP/74I+zsnlWjbtiwAYMGDdK6zby5ySpqjrKIiAi4u7tLz/NUq1ZN2tf8efPLX8OVl68s5dLTezYNxIuYc+1V1m/8U9jaZwEAdHSAMbMfY8zsxxp5pvV1xcUTnIn8VbD+m2rw8UuCm2cafHskwLdHgrRMrQaWzXBEUrysb4OUD8/n66NllwT0HhNd5PKNy6shIabqTlkk65qzgnr37g0gN1gKCAjAtWvXkJWVhcjISGzduhVdunTB2rVrAeT2Ofv8889x5coVuLi4oFevXhqd5+/du6exTSC36S4kJATp6elISEhAcHAw3n33XXzwwQdSnrzO+gBw7do1jRGifn5+Ul+vvEdRTYXTp0/HkydPcO/ePcydO1dK79SpEwCgR48eUlpQUBBCQkKQnJyMFStW4MKFCwAAGxsbtGrVqlC5IiIiNEajElVVGWm6mNbHFeu/qYYHd1TIzFAgKV4Xpw+ZYno/V84m/4rh+Xx9/LnWGhdCjRETqURmhgKZGQpEPdTDsb/MMfudWlizuGoPvnqlvmLMmzcPhw8fxpUrV3DgwAE0aNCgUJ68mq/79+9j1qxZmDVrltZtvfXWWwCAPn36YMiQIVi/fj3Cw8O1Tj0REBAgPW/SpAn09fWRkZGBzZs3Y/PmzQCAw4cPl2lC2jNnzkj93vI4Ojpi6tSpAIA33ngDH3/8Mb755hvExcUV2rZSqcSPP/4IQ8Pc+X1MTEzg6emJS5cuITw8HFZWudX/c+fOLfNghaps6aSaWDqpZmUXgypQWooufv2qBn79qmrf7F8XPJ+vh83f2VX56TKK80rVnFlZWeGff/7BggUL0LhxYxgbG0NfXx/Ozs7o2LEjli5diq5duwLI7a82evRoeHp6wsrKCrq6ujA1NUWLFi2wcuVKTJgwAUBus95vv/2GtWvXol27dtKcaNWrV0fz5s0xa9YsKWACAHt7e6xbtw6enp5SYFQeoaGh6NOnD0xNTWFiYoJevXrh6NGjGjVgX3/9NTZt2oR27drBwsJCKle/fv1w/Phx9O3bV2Oba9euhZ+fH8zNzQu+HREREb0i+MPnVC6v6w+fExERvQj84XMiIiKiVxSDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyQiDMyIiIiIZYXBGREREJCMMzoiIiIhkhMEZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMiIiIiGWFwRkRERCQjDM6IiIiIZITBGREREZGMMDgjIiIikhEGZ0REREQywuCMiIiISEYYnBERERHJCIMzIiIiIhlhcEZEREQkIwzOiIiIiGSEwRkRERGRjDA4IyIiIpIRBmdEREREMsLgjIiIiEhGGJwRERERyYiysgtAryYhBAAgG1mAqOTCEBERyVw2sgA8+/wsDoMzKpekpCQAwDH8VcklISIienUkJSXB3Ny82DwKUZoQjqiAnJwcPHr0CKamplAoFJVdnBcmMTERTk5OuH//PszMzCq7OPQceC5fLzyfr4+qci6FEEhKSoK9vT10dIrvVcaaMyoXHR0dODo6VnYxXhozM7PX+qZRlfBcvl54Pl8fVeFcllRjlocDAoiIiIhkhMEZERERkYwwOCMqhr6+PubOnQt9ff3KLgo9J57L1wvP5+uD57IwDgggIiIikhHWnBERERHJCIMzIiIiIhlhcEZEREQkIwzOqEoJCgqCQqGAQqFAYGCglO7n5yelh4eHV1r5qGRFnUN6dfGcvv7CwsIQGBiIwMBABAcHl2ldFxcX6fqoaPHx8VK5goKCCi0PDAyU3lvb8heFk9ASERHRCxUWFoZ58+ZJr/38/CqvMPnEx8dL5fL19cWIESMqt0D/jzVnRET0ykhLS6vsIrxSsrKykJ2dXdnFoDJicEavhSNHjqBnz55wdXWFubk5lEolbGxs0LFjR/zxxx/l3q4QAj///DNat24Nc3NzqFQqODs7491338Xt27elfPv27ZOqvqdNmyalDxs2DAqFAkqlEomJiQCA27dvS3n79u1b7rJVBdu3b4e3tzcMDAzg4uKCzz77DGq1WmveCxcuYOjQoXB0dIRKpYKVlRW6dOmCgwcPauTL34Q2d+5cLFu2DHXr1oWhoSE8PDywbt06Ke+7774r5d2zZ4/GdiZOnCgt27FjR8Xv/GuqtOc0f1eD0NBQDB48GFZWVjAyMpLyREZG4uOPP4abmxsMDAxgYmKCJk2aYPHixcjMzNTYXt62XFxccObMGfj7+8PY2BhWVlYYOXIkYmJiXvi+v0gjRoyQ9vGPP/7A+PHjYWdnB319fTx48ADJycmYN28evLy8YGxsDENDQ3h6euKLL74odKx2794NX19fWFpaQqlUwtraGt7e3hg1ahTi4uKkfEIIBAUFoW3btrCwsIBKpYKLiws++OADREZGSvlcXFwwcuRI6fW8efPK3YwdERGBfv36wdzcHKampujTp4/Wrijbtm1Dhw4dYGVlBZVKBXt7ewwcOBDnzp3TOGa1atWSXoeEhEjl0lazp1ar8cUXX6B27dowMjKCj48PDhw4UKbyl5ogeg18/fXXAkCRj/Xr1wshhFi9erWUNnfuXGl9X19fKf3u3btCCCFycnJE//79i9ymiYmJOHnypBBCiNTUVKGvry8AiGbNmknbdXR0lPLv3r1bCCHEqlWrpLQVK1a8nAP0Ctq6datQKBSFjruDg0Ohc7hjxw6hp6en9TwpFArxww8/SNvNfw1YWlpqXSc0NFQIIcSVK1ekMnTt2lXaRlZWlqhWrZoAIBwdHUV2dvZLPTavqrKc0/z/kzY2Nhr5hRDi9u3bws7Orsj/zzfffFOkpaVJ752XbmxsLIyMjArl9/b2Funp6ZVxWCpEQEBAkcfr7NmzokGDBkUeq7Zt24qMjAwhhBBnzpwRSqWyyLy3bt0SQuTeHwcNGlRkvho1akj3Umdn5yLz5b8PFyX/+vmvlbyHo6OjiI6OlvJPmTKlyPfT09MT27dvL3TMCj58fX2FEELMnTtXSqtevXqhfCqVStrPisSaM3ot+Pn54eDBg4iMjERGRgZSUlKwa9cuafmSJUvKvM2tW7diy5YtAABnZ2ecPXsW8fHxmDFjBgAgOTkZo0aNAgAYGhqiZcuWAIBz584hKSkJ//77Lx48eAAdndx/s7xOsPk7w3bo0KHM5aoKhBCYMmUKxP/PkR0YGIiEhAQcP34cGRkZGnnT0tIwevRoZGVlwcXFBadPn0ZGRgZu3LiBunXrQgiByZMnIzo6utD7JCYmYsOGDUhISMD06dOl9DVr1gAAGjRogLfeegsAsHfvXty5cwcAcODAAURFRQEARo8eDV1d3Yo/CK+ZspzTggwNDRESEoLU1FScP38eQG7N5ZMnTwAAw4cPR3R0NG7evIlGjRoBAI4dO4bly5cX2lZKSgqGDRuGmJgYXL58GW5ubgBy+0S9zA7fL1J2djZ27tyJ5ORkXL9+HT/88AOuXr0KAPjuu++QmJiI+Ph4TJw4EUBuy8OqVasA5NYe5TWDbtq0CZmZmYiKisLx48cxZ84cmJiYAAB+//13bNy4EUBuDdTjx4+Rnp6O9evXAwAeP34stSKEh4dj9erVUvnmzp0LIQSEEGWuOWvcuDEiIyMRHh6OFi1aAAAePHgg3eNPnz6NpUuXAgAsLCxw6NAhJCYmStdCVlYWxowZg7S0NAQFBeHu3bvStn19faVyaRu0kJKSgv379yM+Ph5DhgwBAGRmZkrHoUJVeLhHVAmePn0qPv74Y1GvXj1haGhY6NuNgYGBEKJsNWdDhw6V0pYtWyblzcrKEtbW1tKy27dvCyGE+Oyzz6S0PXv2iP/9738CgBgwYIAAIJo2bSqEeFabVrNmzZdzcF5B169fl46lra2tUKvV0rKZM2dqnMMDBw4U+e03/2Pr1q1CCM1roHfv3tJ2L126JKV37txZSg8ODpbSJ02aJIQQYsiQIQKA0NXVFQ8ePHhJR+XVVpZzKoTm/+SaNWs0tpWamirV7igUChEXFyct2759u0btWZ68NKVSKZKTk6X0n376SVrWp0+fF7PzL0H+WqD58+drLNNW21Tw0a1bNyGEEH/88YdGjdqCBQvE5s2bxc2bNzW2mf/+WNTDxMREyl/Uvbc08tec3bhxQ0rft2+flN6kSRMhhBCzZs0q9P+ap1GjRtKyv//+WwghxN27dwvVluWXv+Ys//Z27dolpY8bN65M+1MarDmjV15OTg78/f3xzTff4Pr161o7DKenp5d5u3nfyoHcmrM8SqUSjo6OhfLlrwULCQlBSEgIAGDkyJGoWbMmzp8/j3PnzuHBgweF8pOm/LVcDg4OUu0joHkuAM3zVNpt5qlfv7703NjYWHqe/3rx9fVFs2bNAACrV69GVFSU1Mese/fucHBwKNX7V3VlOacF+fj4aLyOjY2VanfMzc1hYWEhLXNxcZGea7s2bGxsNM51/vfOqw191RU8XqX5H8k7Pz179sSUKVNgZGSEI0eOYPbs2RgwYADc3d3h4+ODR48elXqbycnJJdaKArmtCXl9vfL3DdQm//nSdu6Kum8DJV8bJSnN/aKiMDijV96lS5dw8eJFAICdnR0uXbqE7OxsqQN+ednZ2UnPIyIipOdqtVoKsPLna9q0qfQhERwcjODgYOjq6qJ169bw9fWFWq3GggULpPUYnBXNxsZGev7w4UPk5ORIr/OfC0DzPHXu3Flqlsj/yMnJwbhx4wq9j56envS8uDmUpk6dCiB32P2gQYOQkpICAHjvvffKuGdVV1nOaUH5BwEAgJWVFZTK3JmgEhISkJCQIC3L3zk8/7WRJzo6Wjp/Bd+7WrVqJezFq6Hg8co7DgqFAo8ePdL6P3L8+HEp/5IlSxAbG4vTp09j8+bN+OCDDwDkdtmYP3++xjYBYMOGDUX+3+X9mHlFzVGW/3xpO3dF3bcB7ddGWcpV2vtFRWBwRq+8vJs0AOjq6sLExAQJCQmYPHnyc223R48e0vOvv/4aYWFhSExMxOzZs6WRXQ0aNICrq6v03nkjfE6dOoWIiAg0adIEpqam8PX1BQCpxkWhUMDf3/+5yvc6c3d3l771Pn36FAsWLEBiYiJOnjwp9Y3J07p1a9ja2gIA9u/fjyVLliAmJgYZGRm4fv06vvzyS9SpU+e5ytO3b19pVNfhw4cBAK6urujYseNzbbcqKcs5LYmhoaF07IUQmDRpEmJiYnDnzh0peAA0/4fzZGdnY+rUqYiLi8PVq1c1+qN26tSpPLsme7179waQe6wCAgJw7do1ZGVlITIyElu3bkWXLl2wdu1aALm1/p9//jmuXLkCFxcX9OrVC7169ZK2de/ePY1tAsDMmTMREhKC9PR0JCQkIDg4GO+++64U1AGAtbW19PzatWsaI0T9/PwKBXZFTQY+ffp0PHnyBPfu3cPcuXOl9Lxzl/+cBwUFISQkBMnJyVixYgUuXLgAIPeLQqtWrQqVKyIiQmM0aqWq8IZSopcsOztbNGzYsFB/B3d3d43XQpR9tGafPn2K7E9hZGQkjerL891332nkmTZtmhAid2RZ/nQvL6+XcmxeZVu2bNE6ss/W1rbQOdy5c6dQqVTF9n/JU9Q1UFLfk+XLl2ts78svv3zBR+D1U5Zzqu1/Mr+bN29qrFfw0bJlS62jNU1MTISZmVmh/K/TaM3Dhw9rLIuJiREeHh7F/n+sXr1aCCHE2rVri823fPlyIUTu/TGv72VRj4CAAKkMDx8+lEa0538ULKs2ZR2t+fHHHxdZJqVSKfU/zePp6VkoX951mL/PWd4xEkKIw4cPa93PisKaM3rl6erqYteuXejVqxcsLS1hZmaGvn374tChQ8+1XYVCgS1btuDHH39EixYtYGpqCqVSCScnJwQEBOD8+fPSt688BWtS8mrMXF1dNfqpsUmzZP369cO2bdvg5eUFlUoFJycnzJ49GwsXLiyUt3v37jh79iyGDx+OmjVrQk9PD+bm5qhfvz6GDx+OTZs2PXd53n33XVhZWQEAVCqVxrxNVDplOaclcXNzQ1hYGCZMmABXV1eoVCoYGRnB29sbixYtwuHDh2FgYFBoPWtraxw9ehQdOnSAkZERLCwsEBAQgAMHDkhNcK8bKysr/PPPP1iwYAEaN24MY2Nj6Ovrw9nZGR07dsTSpUvRtWtXALn91UaPHg1PT09YWVlBV1cXpqamaNGiBVauXIkJEyYAyL0//vbbb1i7di3atWsnzYlWvXp1NG/eHLNmzZK6AwCAvb091q1bB09PTxgaGpZ7X0JDQ9GnTx+YmprCxMQEvXr1wtGjRzVqwL7++mts2rQJ7dq1g4WFhVSufv364fjx44Xml1y7di38/Pxgbm5e7nJVNIUQ/z+umYiIivTgwQPUq1cPKSkpGD58OH799dfKLhKVQV4fIWdnZ/5+Lskea86IiIqxfft2uLu7w83NDSkpKTA0NMSnn35a2cUiotcYgzMiomIkJCTg1q1bEEKgSZMm2LVrlzRxKRHRi8BmTSIiIiIZYc0ZERERkYwwOCMiIiKSEQZnRERERDLC4IyIiIhIRhicEREREckIgzMioucUHh4OhUKBESNGaKT7+fm98B9IriguLi5wcXGp7GK88HIEBQVBoVAgKCjohb0H0fNicEZEr4y8ICj/I+9ngIYMGYKLFy9WdhEr1IgRI6BQKGQ3o31egPPFF19UdlGIXkvKyi4AEVFZubq6YtiwYQCA5ORknDx5Ehs2bMDvv/+OQ4cOFfrN08qyZs0apKamVnYxiOgVw+CMiF45derUQWBgoEbap59+ioULF2LWrFk4fPhw5RSsgJo1a1Z2EYjoFcRmTSJ6LXz44YcAgNOnT0tpCoUCfn5+ePjwIUaMGIHq1atDR0cHwcHBUp4jR46ge/fusLGxgb6+Ptzc3PDpp59qrfFSq9X48ssvUadOHRgYGKBOnTpYtGgRcnJytJapuD5nO3fuROfOnWFtbQ0DAwO4uLjgnXfeweXLlwHk9r3K+3H1WrVqSc24fn5+Gtu5e/cuRo8ejZo1/6+9+4+Juv4DOP5UOE5i3CE/jgGbKAXlBtMCCZeKKRsLZOhW05EGIuTSCv9gzNaPWQ1pRjYcNMvtKJu/SlcIlls6MixQbOnKIKdCCqUmcjAlOZDX9492V/e9A89vNs9vr8fGH/d+v96fz+vzvo299nm/P5+bhNFoJCoqioKCAn7++WeP562rq2PGjBkEBgYSGRlJcXExvb29nif1Nvj222959tlnSUxMxGw2ExgYSFJSEm+88QZDQ0Ojjuvt7aW4uJjIyEgCAwNJTU1l7969HmNFBKvVyiOPPILJZOKee+4hJSUFq9X6T12WUv8ovXOmlPq/MFoR1NPTw8yZMwkNDWXx4sXY7XZMJhMAmzdvZtWqVUycOJGcnBwiIiJobW2lvLycxsZGGhsbCQgIcB7r6aefxmq1MmXKFFavXs3169fZuHEj33zzzS3lWlZWxptvvkloaCgLFy7EYrFw/vx5Dhw4QHJyMomJiaxZs4b333+fEydOUFJSQkhICIDLZvkjR46QmZnJtWvXyMnJ4b777qOzs5Nt27bx+eef09zcTFxcnDN+69at5OfnYzKZWLZsGSEhITQ0NJCRkYHdbne51ttly5Yt1NfXM2fOHLKyshgYGODLL7/khRdeoLW1lT179riNsdvtZGRk8Pvvv5Ofn4/NZmPnzp0sXLiQDz/8kCeffNIZKyIsXbqU7du3k5CQQF5eHgEBAXzxxResWLGCH3/8kcrKytt+XUr9o0Qppe4SHR0dAkhmZqZb34svviiAzJ0719kGCCDLly+X4eFhl/iTJ0+Kv7+/PPjgg9LT0+PSV1FRIYBUVlY62xobGwWQadOmydWrV53tXV1dEh4eLoDk5+e7HCc9PV3++9/svn37BJCkpCS5fPmyS9/Q0JBcuHDB+Tk/P18A6ejocLteu90ukydPluDgYDl+/LhLX1NTk/j5+cmCBQucbX19fWIymSQoKEh++uknl+PMmTNHAImNjXU7jye1tbUCSEVFxU1jOzs73eZ+ZGRECgsLBZDDhw+79MXGxgog8+bNE7vd7mxva2uTwMBACQkJkf7+fmf7e++9J4CsWLFChoaGnO2Dg4OSk5MjgBw7dswt99raWq+uVak7QZc1lVJ3ndOnT7Nu3TrWrVtHaWkps2bNory8nAkTJrB+/XqX2ICAADZs2ICfn59L+7vvvsvw8DCbNm0iNDTUpa+srIyIiAh27NjhbNu6dSsAr7zyCkFBQc72mJgYSkpKvM69pqYGgKqqKsLCwlz6/P39iYyM9Oo4DQ0NdHZ2UlZWxrRp01z6Zs2aRW5uLp999hn9/f0AfPrpp/T391NYWEhCQoIz1mAwUF5e7nX+tyo2NtZt7seNG8fq1asBOHDggMdxr7/+OgaDwfn5gQceoLCwEJvNRl1dnbO9urqaoKAgqqur8ff/czEoICDAeV1//R6VuhvosqZS6q5z5swZXn31VeCP4iIyMpK8vDzWrl1LUlKSS+yUKVMIDw93O0ZLSwsA+/fv91ggGAwG2tvbnZ9PnDgBwOzZs91iPbWN5ujRoxiNRtLT070e44kj//b2dreHIwAuXLjAyMgIp06dIiUlZcz8Z86c6VLY3E52u53q6mp27txJe3s7V69eRUSc/b/88ovbGIPBQFpamlv77Nmzqamp4fjx4yxdupSBgQG+//57oqOjPb7Ww7Gn7a/fo1J3Ay3OlFJ3nczMTPbv3+9V7Gh3oq5cuQLg9V2jvr4+xo8f77HQ8/ZuF4DNZiMmJobx4//ewoUj/23bto0Zd+3aNeCP/AEsFotbjJ+fn9tdvNvl8ccfp76+noSEBBYvXozFYsFgMGCz2aiqqmJwcNBtTFhYmMf5ccyz41p6e3sREbq7u53FuieOOVDqbqHFmVLq/9poDwo4Hgro7+8nODj4pscxm82MjIxw+fJlIiIiXPouXrzodT4hISHOu1p/p0Bz5F9fX8+CBQtuGm82mwG4dOmSW9+NGzfo6ekhJibmf87Hk9bWVurr68nMzGTfvn0uy5stLS1UVVV5HNfT0+Nxfhzz7LgWxxwkJydz7Nix25q7UneS7jlTSv0rPfzww8Cfy4M349jX1dTU5NbnqW00qampDA4OcujQoZvGOoqZGzduuPU58m9ubvbqvGPl39zczPDwsFfHuRVnzpwBIDs7223f2VhzNjQ05PF7cYyZPn06AMHBwUydOpW2tjZsNtvtSVopH6DFmVLqX2nVqlX4+/vz3HPPcf78ebd+m83Gd9995/z81FNPAfDaa6+5LJN1d3ePegfIE8dG+JKSEufSpMPw8LDLXTjHgwpdXV1ux8nNzWXSpEls3LiRr776yq1/aGiIw4cPu8SbTCasViunTp1yiXvppZe8zv9WxMbGArjkAXDy5EkqKirGHPvyyy+7vAetvb0dq9WK2WwmNzfX2f78888zMDBAcXGxx+XLjo4On/v5K6VuRpc1lVL/SomJibzzzjs888wz3H///WRlZXHvvffS39/P2bNnOXToEAUFBWzevBn444Wyy5cvp7a2lqSkJBYtWsTg4CC7du0iLS2NhoYGr86blZVFaWkplZWVxMfHs2jRIiwWC93d3Rw8eJDS0lLWrFkDwLx586isrGTlypU88cQTBAUFMWnSJPLy8jAajezevZvHHnuM9PR05s+fT2JiIgDnzp2jqamJsLAw52Z4s9nMpk2bKCgoYMaMGSxZsgSz2UxDQwOBgYFERUXd8hx+/PHHo262z8vLY/78+aSmpvLRRx/x66+/kpaWxrlz59i7dy/Z2dns3r3b49ioqChsNhvTp08nOzubvr4+duzYwfXr19myZYvLMvTKlStpaWnhgw8+4OuvvyYjI4Po6GguXrxIe3s7R44cYfv27T7xo+5Kee1Ov8tDKaW8NdZ7zjwBJD09fcyYo0ePypIlSyQ6OloMBoOEh4fLQw89JGvXrpW2tjaX2OHhYamoqJC4uDgJCAiQuLg4Wb9+vZw+fdrr95w57NmzRx599FExm81iNBpl8uTJsmzZMvnhhx9c4jZs2CDx8fFiMBg8Xk9XV5eUlJRIfHy8GI1GMZlMMnXqVCkqKpKDBw+6nfeTTz6R5ORkMRqNYrFYpKioSK5cuSKxsbG3/J6zsf7efvttERG5dOmSFBYWSnR0tEyYMEGSkpKkpqZGzp4963HOHHn09PRIUVGRWCwWMRqNkpKSInV1daPmtGvXLsnIyJCJEyeKwWCQmJgYmTt3rrz11lvy22+/ueWu7zlTvmycyF+eaVZKKaWUUneU7jlTSimllPIhWpwppZRSSvkQLc6UUkoppXyIFmdKKaWUUj5EizOllFJKKR+ixZlSSimllA/R4kwppZRSyodocaaUUkop5UO0OFNKKaWU8iFanCmllFJK+RAtzpRSSimlfIgWZ0oppZRSPuQ//LdB+35a5WEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6), squeeze=True)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test, \n",
    "                                        y_pred=y_pred, \n",
    "                                        ax=ax, \n",
    "                                        colorbar=False)\n",
    "\n",
    "wf1 = f\"{test_weighted_f1:.4f}\"\n",
    "micro = f\"{test_micro_f1:.4f}\"\n",
    "macro = f\"{test_macro_f1:.4f}\"\n",
    "metrics = f\"Weighted F1: {wf1}, Micro F1: {micro}, Macro F1: {macro}\"\n",
    "\n",
    "plt.suptitle(\"Confusion Matrix Test Set Predictions\", \n",
    "             fontsize=14,               \n",
    "             y=0.95,\n",
    "             weight=\"bold\", \n",
    "             ha=\"center\", \n",
    "             va=\"center\")\n",
    "\n",
    "ax.set_title(metrics, fontsize=11)\n",
    "ax.set_xlabel(\"Predicted Label\", fontsize=14)\n",
    "ax.set_ylabel(\"True Label\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c90ac-7817-4b69-98bc-425cbbac5ce8",
   "metadata": {},
   "source": [
    "### Training Set Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45bd28-96d2-41ce-a169-9e7bce9d4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Action\"\n",
    "drop_cols = [\"NAT Destination Port\"]\n",
    "\n",
    "X_train = train_df.drop(columns=drop_cols+[target])\n",
    "y_train = train_df[target].to_numpy()\n",
    "\n",
    "y_pred_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a319d25-f4df-45f8-b43e-fbe2952a0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weighted_f1 = f1_score(y_true=y_train, y_pred=y_pred, average=\"weighted\")\n",
    "train_micro_f1 = f1_score(y_true=y_test, y_pred=y_pred, average=\"micro\")\n",
    "train_macro_f1 = f1_score(y_true=y_test, y_pred=y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Weighted F1: {test_weighted_f1}\")\n",
    "print(f\"Micro F1: {test_micro_f1}\")\n",
    "print(f\"Macro F1: {test_macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e53b9c-6d7c-46ab-8e8f-f3a4bb909ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52048a1-13c8-4ea2-8f80-16d50d452ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_base_env]",
   "language": "python",
   "name": "conda-env-ds_base_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
